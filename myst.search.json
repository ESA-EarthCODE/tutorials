{"version":"1","records":[{"hierarchy":{"lvl1":"Open Science Catalog Editor"},"type":"lvl1","url":"/git-clerk-example","position":0},{"hierarchy":{"lvl1":"Open Science Catalog Editor"},"content":"A tutorial on how to use the Open Science Catalog Editor, A visual interface to the Open Science Catalog, to upload data.\n\nFirst log in to  \n\nhttps://​workspace​.earthcode​.eox​.at/\n\nAccept the required permissions, so that git-clerk can open PR requests from your github account in the OSC repository.\n\nClick on the Open Science Catalog Editor.","type":"content","url":"/git-clerk-example","position":1},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"Starting with the OSC Editor"},"type":"lvl2","url":"/git-clerk-example#starting-with-the-osc-editor","position":2},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"Starting with the OSC Editor"},"content":"When you would like to add new entries, please start a new session (give it a meaningful name) and start with Adding New Project in the main page.\nThen you can proceed by providing the information directly in the form.\n\nPlease see the steps with an example below:\n\nYou can find here a description of required metadata for projects/products etc.: \n\nhttps://esa-earthcode.github.io/documentation/Technical Documentation/Open Science Catalog/Open Science Catalog Overview\n\nOnce you have all the fields filled in you can submit the project and products metadata for review which will automatically open pull request, which will be checked and merged by our team.","type":"content","url":"/git-clerk-example#starting-with-the-osc-editor","position":3},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"In case of erorrs"},"type":"lvl2","url":"/git-clerk-example#in-case-of-erorrs","position":4},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"In case of erorrs"},"content":"The tool is basically a user interface to create a github pull request on your behalf. You can view the results of your entry here: \n\nhttps://​github​.com​/ESA​-EarthCODE​/open​-science​-catalog​-metadata​/pulls. Whenever you create a pull request or branch we run an automatic validation to ensure your data conforms to the OSC schema.\n\nIf the validation has failed you can manually inspect the errors. If you click on the failed validation link (Validate / check (ubuntu-latest) ...) you can see the reasons for blocking the PR submissions.\n\n\nFor example, your errors might look like this:\n\n\nIn this case, the problems come from a missing:\n\nTemporal extent specification\n\nMissing contracts.\n\nMissing links to your website and the eo4society page for the project.\n\nYou can ignore the missing extensions/parent/root link/ errors those should be resolved automatically.","type":"content","url":"/git-clerk-example#in-case-of-erorrs","position":5},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"You can always ask the EarthCODE team for support if needed!"},"type":"lvl2","url":"/git-clerk-example#you-can-always-ask-the-earthcode-team-for-support-if-needed","position":6},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"You can always ask the EarthCODE team for support if needed!"},"content":"","type":"content","url":"/git-clerk-example#you-can-always-ask-the-earthcode-team-for-support-if-needed","position":7},{"hierarchy":{"lvl1":"Open Science Catalog"},"type":"lvl1","url":"/index-2","position":0},{"hierarchy":{"lvl1":"Open Science Catalog"},"content":"The \n\nOpen Science Catalog (OSC) is a key component of the ESA EO Open Science framework. It is a publicly available web-based application designed to provide easy access to scientific resources including geoscience products, workflows, experiments and documentation from activities and projects funded by ESA under the EO Programme.\n\nThere are three different ways and several tools to contribute to Open Science Catalog:","type":"content","url":"/index-2","position":1},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"1: Use a Visual GUI Interface (No coding required)"},"type":"lvl2","url":"/index-2#id-1-use-a-visual-gui-interface-no-coding-required","position":2},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"1: Use a Visual GUI Interface (No coding required)"},"content":"Git Clerk - A guide for using the Git Clerk tool which is a user interface for automatically creating product entries and creating a Pull Request in the OSC GitHub Repo.","type":"content","url":"/index-2#id-1-use-a-visual-gui-interface-no-coding-required","position":3},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"2: Manually contribution (For users familiar with Git)"},"type":"lvl2","url":"/index-2#id-2-manually-contribution-for-users-familiar-with-git","position":4},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"2: Manually contribution (For users familiar with Git)"},"content":"Direct editing metadata in JSON - A guide for manually creating Open Science Catalog entries by creating JSON files with metadata.\n\nGenerating OSC files with pystac - A guide for creating Open Science Catalog entries in more automated way by implementing pystac. Require familiarity with Python.","type":"content","url":"/index-2#id-2-manually-contribution-for-users-familiar-with-git","position":5},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"3: Using one of the platform tools (For EarthCODE integrated platforms users)"},"type":"lvl2","url":"/index-2#id-3-using-one-of-the-platform-tools-for-earthcode-integrated-platforms-users","position":6},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"3: Using one of the platform tools (For EarthCODE integrated platforms users)"},"content":"DeepCode - An example using DeepCode: a library for automatically generating product entries for DeepESDL datasets.\n\nopenEO Publishing Tool - A platform agnostic tool to publish workflows, experiments and products created on openEO-based platforms. Learn more in our dedicated \n\npublication guide.","type":"content","url":"/index-2#id-3-using-one-of-the-platform-tools-for-earthcode-integrated-platforms-users","position":7},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)"},"type":"lvl1","url":"/osc-pr-manual","position":0},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)"},"content":"The \n\nOpen Science Catalog (OSC) is a key component of the ESA EO Open Science framework. It is a publicly available web-based application designed to provide easy access to scientific resources including geoscience products, workflows, experiments and documentation from activities and projects funded by ESA under the EO Programme.\n\nThe Open Science Catalog is built on the Spatio Temporal Asset Catalog (STAC), which is a standardised format for describing geospatial data. Throught the open source STAC browser, the catalog allows users to browse and explore interlinked elements such as themes, variables, EO missions, projects, products, workflows, and experiments, all described using STAC-compliant JSON files. This schema ensures that these can be easily reused by other scientists and automated workflowss and correclty displayed in the web browser. Data, workflows, and experiments are documented in the catalogue primarily through enriched metadata and direct links to the corresponding research outcomes. The physical location of these resources is typically indicated via the Project Results Repository or other secure external repositories. Further details on the OSC format can be found \n\nhere.","type":"content","url":"/osc-pr-manual","position":1},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Adding information to the OSC"},"type":"lvl2","url":"/osc-pr-manual#adding-information-to-the-osc","position":2},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Adding information to the OSC"},"content":"There are three ways to add information to the OSC.\n\nManually opening a pull request (this tutorial)\n\nUsing the GUI editor.\n\nUsing one of the platform specific tools\n\nThis notebook describes how you can add information to the OSC by manually creating and editting json files that describe STAC Collections and Catalogs. The steps to add information in this way are:\n\nFork the repository\n\nAdd the information about project/product/workflow/variables in STAC json format.\n\nOpen a PR to merge the new information into the OSC.\n\nIn general most of the information that you need, is already in your data or project documentation, so you will NOT need to generate anything new. All information that you provide will be automatically validated and manually verified by an EarthCODE team member. Therefore, you can use the automatic validation from the CI to make the appropriate changes to the format or information you provide.","type":"content","url":"/osc-pr-manual#adding-information-to-the-osc","position":3},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"1. Forking the repository","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-1-forking-the-repository","position":4},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"1. Forking the repository","lvl2":"Adding information to the OSC"},"content":"Since the OSC metadata is fully hosted on GitHub. Use your personal GitHub account to cotnribute to the catalog. If you do not have an account, you need to setup a new GitHub account: \n\nhttps://​docs​.github​.com​/en​/get​-started​/start​-your​-journey​/creating​-an​-account​-on​-github.\n\nTo contribute your research outputs, you need to create valid STAC objects and commit them to the \n\nopen-science-catalog-metadata repository on GitHub.\nThe first step in this process is to fork the open-science-catalog-metadata repository, that will create your own copy of the Open Science Catalog. ( More information about how to do this in GitHub is available here: \n\nhttps://​docs​.github​.com​/en​/pull​-requests​/collaborating​-with​-pull​-requests​/working​-with​-forks​/fork​-a​-repo )\n\nOnce you have a OSC copy, you should have a look at the folder structure and information for an existing Item of the same type as the one you want to add - product, project, workflow, variable etc. This will give you an idea of the required information for a valid STAC Object. These STAC objects, stored as JSON files, are be automatically processed and rendered in the catalog viewer.\n\n","type":"content","url":"/osc-pr-manual#id-1-forking-the-repository","position":5},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-2-add-the-information-about-project-product-workflow-experiments-variables","position":6},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"After you forked the repository, you can start adding the required information. \n\nThis document explains the Open Science Catalog Extension to the SpatioTemporal Asset Catalog (STAC) specification. There are different requirements depending on the catalog entry you are trying to add.Sometimes its easier to copy the folder of existing project/product/workflow, rename it and start changing its information.\nFor example, copying the contents of this folder products/sentinel3-ampli-ice-sheet-elevation/, renaming it to products/New_Project_Name/ and editing its values.\n\n\n","type":"content","url":"/osc-pr-manual#id-2-add-the-information-about-project-product-workflow-experiments-variables","position":7},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.1 Add new Project","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-1-add-new-project","position":8},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.1 Add new Project","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Projects are the containers that have the top level information about your work. It is the first type of information you should provide. Typically an OSC project corresponds to a project financed by the European Space Agency - Earth Observation programme. Before creating new project, check if your project is not already on the \n\nlist of onboarded projects. In such case you can use your project entry and only update it where needed.\n\nField\n\nDescription\n\nSTAC representation\n\nProject_ID\n\nNumeric identifier\n\n\n\nStatus\n\n“ongoing” or “completed”\n\nosc:status property\n\nProject_Name\n\nName\n\ntitle property\n\nShort_Description\n\n\n\ndescription property\n\nWebsite\n\n\n\nlink\n\nEo4Society_link\n\n\n\nlink\n\nConsortium\n\n\n\ncontacts[].name property\n\nStart_Date_Project\n\n\n\nextent.temporal[] property\n\nEnd_Date_Project\n\n\n\nextent.temporal[] property\n\nTO\n\n\n\ncontacts[].name property\n\nTO_E-mail\n\n\n\ncontacts[].emails[].value property\n\nTheme1 - Theme6\n\nTheme identifiers\n\nosc:themes property\n\nMetadata of each project is stored in a folder named after their unique id (collectionid). Each folder has one file - collection.json that has all the project information (metadata). Have a look at the structure of the Project entry below (with example values filled in):{\n    'type': 'Collection', // This is the STAC type specification. You dont need to change this\n    'stac_version': '1.1.0',  // This is the STAC version specification. You dont need to change this\n    'id': 'worldcereal2', // This is your project id. Please make sure to use unique id name for your project! The parent folder of the collection.json should have the same name as this id (not displayed in the browser).\n    'title': 'WorldCereal2', // Title of your project. Official acronym of the project may be used as well (this will be displayed to public)\n    'description': 'WorldCereal is an ESA initiative that provides global '\n                'cropland and crop type maps at 10-meter resolution, offering '\n                'seasonally updated data on temporary crops, croptypes (maize, '\n                'winter cereals and spring cereals), and irrigation.', // A short, but meaningful description of your project.\n    'links': [  // links to related elements of the catalog. The first two links should always be present and are always the same.\n        {'href': '../../catalog.json',\n        'rel': 'root',\n        'title': 'Open Science Catalog',\n        'type': 'application/json'},\n        {'href': '../catalog.json',\n        'rel': 'parent',\n        'title': 'Projects',\n        'type': 'application/json'},\n        // The next two links are external links to project websites.  These are mandatory and you have to adapt them to your project.\n        {'href': 'https://esa-worldcereal.org/en', # your dedicated project page\n            'rel': 'via',\n            'title': 'Website'},\n           {'href': 'https://eo4society.esa.int/projects/worldcereal-global-crop-monitoring-at-field-scale/', #link to the project page on EO4Society website\n            'rel': 'via',\n            'title': 'EO4Society Link'},\n        // The next link is a link to the themes specified in the themes field below. It is mandatory to have a link to all themes specified in the themes array\n        {'href': '../../themes/land/catalog.json',  #related theme of the project\n            'rel': 'related',\n            'title': 'Theme: Land',\n            'type': 'application/json'}\n    ],\n\n    'themes': [ // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having at least one theme is mandatory. Check available themes here: https://opensciencedata.esa.int/themes/catalog\n        {'concepts': [{'id': 'land'}], \n        'scheme': 'https://github.com/stac-extensions/osc#theme'}\n    ],\n\n    'stac_extensions': [ // which schemas is the project information validated against. Typically you would not change these.\n        'https://stac-extensions.github.io/osc/v1.0.0/schema.json', \n        'https://stac-extensions.github.io/themes/v1.0.0/schema.json',\n        'https://stac-extensions.github.io/contacts/v0.1.1/schema.json'\n        ]\n    'osc:status': 'completed', // status of the project - Select from: completed, ongoing, scheduled\n    'osc:type': 'project', // Type of OSC STAC collection, for projects should always be project\n    'updated': '2025-07-14T17:03:29Z', // when was last update made\n    'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]}, // The study area of the project and its planned duration.\n                'temporal': {'interval': [['2021-01-01T00:00:00Z',\n                                        '2021-12-31T23:59:59Z']]}}\n    'license': 'proprietary' // Top level license of project outcomes. Should be one of https://github.com/ESA-EarthCODE/open-science-catalog-validation/blob/main/schemas/license.json\n\n    // list of consortium members working on the project and contact to ESA TO following the project. This field is required.\n    'contacts': [{'emails': [{'value': 'Zoltan.Szantoi@esa.int'}],\n               'name': 'Zoltan Szantoi',\n               'roles': ['technical_officer']},\n              {'name': 'VITO Remote Sensing', 'roles': ['consortium_member']}\n \n }\n\n\nIn addition to specifying the links within the project collection.json entry (created above), you should also add an entry in the parent catalog, listing all projects to be correclty rendered into STAC Browser. Once done it is required to add the following link (as provided below) to: projects/catalog.json . \nAdd this links array into the project/catalog.json just after the last project entry. Edit the catalog.json direclty by copy-and paste the followinf link (updated according to the data from your collection.json){\n    'rel':'child', \n    'target: './{project_id}/collection.json', // use the collectionid of the project\n    'media_type': 'application/json',\n    'title': '{project_title}'   // title of th project as described in the collection.json file created before. \n}\n\n","type":"content","url":"/osc-pr-manual#id-2-1-add-new-project","position":9},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.2 Add new Product","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-2-add-new-product","position":10},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.2 Add new Product","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Products represent the outputs of you projects and typically reference datasets. Similarly to Projects, they are STAC items and follow similar structure, with some additional fields, improving their findability.\n\nField\n\nDescription\n\nSTAC representation\n\nID\n\nNumeric identifier\n\n\n\nStatus\n\n“ongoing” or “completed”\n\nosc:status property\n\nProject\n\nThe project identifier\n\nosc:project property, collection link\n\nWebsite\n\n\n\nlink\n\nProduct\n\nName\n\nlink\n\nShort_Name\n\n\n\nidentifier\n\nDescription\n\n\n\ndescription property\n\nAccess\n\nURL\n\nlink\n\nDocumentation\n\nURL\n\nlink\n\nVersion\n\n\n\nversion property\n\nDOI\n\nDigital Object Identifier\n\nsci:doi property and cite-as link\n\nVariable\n\nVariable identifier\n\ncollection link\n\nStart\n\n\n\nextent.temporal[]\n\nEnd\n\n\n\nextent.temporal[]\n\nRegion\n\n\n\nosc:region property\n\nPolygon\n\n\n\ngeometry\n\nReleased\n\n\n\ncreated property\n\nTheme1 - Theme6\n\nTheme identifiers\n\nosc:themes property\n\nEO_Missions\n\nSemi-colon separated list of missions\n\nosc:missions property\n\nStandard_Name\n\n\n\ncf:parameter.name property{\n 'type': 'Collection', // This is the STAC type specification. You dont need to change this\n 'id': 'worldcereal-crop-extent-belgium2', // This is the unique id of the product. Typically contains the dataset title+project name (or acronym)\n 'stac_version': '1.0.0', // This is the STAC version specification. You dont need to change this\n 'stac_extensions': [  // which schemas is the product information validated against. Typically you would not change these.\n    'https://stac-extensions.github.io/osc/v1.0.0/schema.json',\n    'https://stac-extensions.github.io/themes/v1.0.0/schema.json',\n    'https://stac-extensions.github.io/cf/v0.2.0/schema.json'\n ],\n 'created': '2025-07-14T17:37:16Z', //initial creation date\n 'updated': '2025-07-14T17:37:16Z'  // date of the last update\n 'title': 'WorldCereal Crop Extent - Belgium2', // product title\n 'description': 'WorldCereal is an ESA initiative that provides global ' // Short, but meaningful product description. It should provide enough information to the external users on the specific product.\n                'cropland and crop type maps at 10-meter resolution, offering '\n                'seasonally updated data on temporary crops, croptypes (maize, '\n                'winter cereals and spring cereals), and irrigation. This '\n                'dataset provides the outputs for Belgium.',\n\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]}, // the temporal and spatial extent of the product\n            'temporal': {'interval': [['2021-01-01T00:00:00Z',\n                                       '2021-12-31T23:59:59Z']]}},\n 'keywords': ['Crops', 'Cereal'], // list of keywords associated with the product. These are expected to be inline with the description.\n\n 'osc:project': 'worldcereal2', //unique id of the OSC project, this product is associated with. It must be the id provided in the ./project/(collectionid)\n 'osc:region': 'Belgium', //text description of the study area\n 'osc:status': 'ongoing', //product status\n 'osc:type': 'product', // Type of OSC STAC collection, for products should always be product\n \n 'links': [ // links to different elements of the catalog. The first two links should always be present and are always the same.\n    \n    {'href': '../../catalog.json',\n    'rel': 'root',\n    'title': 'Open Science Catalog',\n    'type': 'application/json'},\n    {'href': '../catalog.json',\n    'rel': 'parent',\n    'title': 'Products',\n    'type': 'application/json'},\n    {'href': '../../projects/worldcereal2/collection.json', // link to parent project (associated project)\n    'rel': 'related',\n    'title': 'Project: WorldCereal2',\n    'type': 'application/json'},\n\n    {'href': '../../themes/land/catalog.json', // link to the theme (scientific domain) this product is associated with.\n    'rel': 'related',\n    'title': 'Theme: Land',\n    'type': 'application/json'},\n    {'href': '../../eo-missions/sentinel-2/catalog.json', // link to eo-missions used to produce the outcomes\n    'rel': 'related',\n    'title': 'EO Mission: Sentinel-2',\n    'type': 'application/json'},\n    {'href': '../../variables/crop-yield-forecast/catalog.json', // link to variables specified below.\n    'rel': 'related',\n    'title': 'Variable: Crop Yield Forecast',\n    'type': 'application/json'},\n\n    {'href': 'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/ESA_WORLDCEREAL_SPRINGCEREALS', // link to dataset hosted in ESA Project Results Repository (PRR). \n    'rel': 'child',\n    'title': 'ESA WorldCereal Spring Cereals'},\n\n    {'href': 'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/ESA_WORLDCEREAL_SPRINGCEREALS',\n    'rel': 'via',\n    'title': 'Access'}, // external link to the actual data\n    {'href': 'https://worldcereal.github.io/worldcereal-documentation/',\n    'rel': 'via',\n    'title': 'Documentation'} // external link to data documentation\n],\n 'osc:missions': ['sentinel-2'], // array of ESA missions related to the product. This array of values is mandatory and limited to missions already existing in the OSC. If you would like to associate your product to a mission that is not on the list, create eo-mission entry first. \n 'osc:variables': ['crop-yield-forecast'], // array of variables related to the product. This array of values is mandatory and limited to variables already existing in the OSC. If you would like to associate your product to a mission that is not on the list, create eo-mission entry first. \n 'cf:parameter': [{'name': 'crop-yield-forecast'}], // optional parameters following cf conventions\n \n 'sci:doi': 'https://doi.org/10.57780/s3d-83ad619', // DOI, if already assigned\n \n 'themes': [ // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having at least one theme is mandatory.\n    {'concepts': [{'id': 'land'}],\n    'scheme': 'https://github.com/stac-extensions/osc#theme'}],\n \n 'license': 'proprietary', //  License of the product. Should be one of https://github.com/ESA-EarthCODE/open-science-catalog-validation/blob/main/schemas/license.json\n\n}\n\n\nIn addition to specifying the links from the product to other parts of the catalog, it is required to add the reverse links, as in case of the Project to following elements:\n\nFrom the Product Collection.json to the Catalog.json (listing all products in the OSC)\n\nFrom the associated Project to the Product\n\nFrom the associated EO-Missions catalog to the Product\n\nFrom the associated Variables Catalog to the Product\n\nFrom the associated Themes Catalog to the Product\n\nAdd the Product link to products/catalog.json by pasting the following in the links array:{\n    'rel':'child', \n    'target: './worldcereal-crop-extent-belgium2/collection.json', // use the collectionid of the product\n    'media_type': 'application/json',\n    'title': 'WorldCereal Crop Extent - Belgium2'   // title of the product as described in the collection.json file created before. \n}\n\nAdd the links array to associated elements of the OSC. For example add following product to parent project:{\n      \"rel\": \"related\",\n      \"href\": \"../../products/worldcereal-crop-extent-belgium2/collection.json\",\n      \"type\": \"application/json\",\n      \"title\": \"Product: WorldCereal Crop Extent - Belgium2\"\n}\n\nSimilarly, add links to other OSC elements like eo-missions, variables, themes etc.\n\n","type":"content","url":"/osc-pr-manual#id-2-2-add-new-product","position":11},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.3 Add new Workflow","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-3-add-new-workflow","position":12},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.3 Add new Workflow","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Workflows are the code and workflows associated with a project, that have been used to generate a specific product. Workflows follow OGC record specifications in contrast to OSC Projects and Products entries. However, the metadata of a workflow is also expressed in JSON format.\n\nField Name\n\nDescription\n\nconformsTo\n\nAn array of URIs indicating which OGC API Records specifications this record conforms to.\n\ntype\n\nIndicates the GeoJSON object type. Required to be \"Feature\" for OGC compliance.\n\ngeometry\n\nSpatial representation of the item. Set to None here, as it may not be spatially explicit.\n\nlinkTemplates\n\nAn array of link templates as per the OGC API. Used for dynamic link generation.\n\nid\n\nUnique identifier for the workflow STAC item ('worldcereal-workflow2').\n\nlinks\n\nList of external and internal references including catalog navigation, project association, theme association, process graph, source code, and service endpoint.\n\nproperties.contacts\n\nList of individuals or organizations associated with the workflow. Each contact may include name, email, and roles such as technical_officer or consortium_member.\n\nproperties.created\n\nTimestamp representing when the workflow was first created (2025-07-14T18:02:13Z).\n\nproperties.updated\n\nTimestamp of the most recent update to the workflow (2025-07-14T18:02:13Z).\n\nproperties.version\n\nThe version number of the workflow (1).\n\nproperties.title\n\nA concise, descriptive title of the workflow: “ESA worldcereal global crop extent detector2”.\n\nproperties.description\n\nA summary of what the workflow does: “Detects crop land at 10m resolution, trained for global use...”.\n\nproperties.keywords\n\nArray of keywords to support discoverability (e.g., agriculture, crops).\n\nproperties.themes\n\nArray of themes the workflow relates to. Each entry includes a concepts array with IDs (e.g., 'land') and a scheme URL.\n\nproperties.formats\n\nOutput formats of the workflow (e.g., GeoTIFF).\n\nproperties.osc:project\n\nProject ID associated with the workflow (worldcereal2).\n\nproperties.osc:status\n\nCurrent status of the workflow (e.g., completed).\n\nproperties.osc:type\n\nType of OSC object, expected to be workflow.\n\nproperties.license\n\nLicense for the workflow (e.g., 'varuious' – likely a typo for various).\n\nAll data is stored in a record.json file, witin a folder that has the same name as the workflow id.{\n    'conformsTo': [ // OGC spec, does not need to change\n        'http://www.opengis.net/spec/ogcapi-records-1/1.0/req/record-core' \n    ],\n    'type': 'Feature'// OGC spec requirement, does not need to change\n    'geometry': None, // OGC spec requirement, does not need to change\n    'linkTemplates': [], // OGC spec, does not need to change\n    'id': 'worldcereal-workflow2',  // unique workflow id\n\n    'links': [  // links to different parts of the catalog. The first two links should always be present and are always the same.\n        \n        {'href': '../../catalog.json',\n            'rel': 'root',\n            'title': 'Open Science Catalog',\n            'type': 'application/json'},\n        {'href': '../catalog.json',\n            'rel': 'parent',\n            'title': 'Workflows',\n            'type': 'application/json'},\n        {'href': '../../projects/worldcereal2/collection.json', // link to associated project\n            'rel': 'related',\n            'title': 'Project: WorldCereal2',\n            'type': 'application/json'},\n        {'href': '../../themes/land/catalog.json', // link to associated themes in the themes array specified below\n        'rel': 'related',\n        'title': 'Theme: Land',\n        'type': 'application/json'},\n        { // link to the openeo-process process graph that describes the workflow\n            'href': 'https://raw.githubusercontent.com/WorldCereal/worldcereal-classification/refs/tags/worldcereal_crop_extent_v1.0.1/src/worldcereal/udp/worldcereal_crop_extent.json',\n            'rel': 'openeo-process',\n            'title': 'openEO Process Definition',\n            'type': 'application/json'},\n        { // external link to the full workflow codebase\n            'href': 'https://github.com/WorldCereal/worldcereal-classification.git',\n            'rel': 'git',\n            'title': 'Git source repository',\n            'type': 'application/json'},\n        { // external link to the service used to run the workflow\n            'href': 'https://openeofed.dataspace.copernicus.eu',\n            'rel': 'service',\n            'title': 'CDSE openEO federation',\n            'type': 'application/json'}\n        ],\n    // OGC spec requirement to have a properties field, that contains most of the workflow metadata\n\n    'properties': {\n        \n        'contacts': [{'emails': [{'value': 'marie-helene.rio@esa.int'}],\n                                'name': 'Marie-Helene Rio',\n                                'roles': ['technical_officer']},\n                                {'name': 'CNR-INSTITUTE OF MARINE SCIENCES-ISMAR '\n                                        '(IT)',\n                                'roles': ['consortium_member']},\n                                {'name': '+ATLANTIC – Association for an Atla '\n                                        '(PT)',\n                                'roles': ['consortium_member']}],\n        'created': '2025-07-14T18:02:13Z', // date of workflow creation\n        'updated': '2025-07-14T18:02:13Z', // date of workflow last update\n        'version': '1' //  workflow version\n        'title': 'ESA worldcereal global crop extent detector2', // Short and meaningful title of the workflow\n        'description': 'Detects crop land at 10m resolution, trained '\n                    'for global use. Based on Sentinel-1 and 2 '\n                    'data...', // Short and meaningful workflow description. Should provide specification on how the workflow can be executed and what it does.\n        'keywords': ['agriculture', 'crops'], // workflow keywords (to enhance the findability of the workflow)\n        'themes': [{'concepts': [{'id': 'land'}], // // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having atleast one theme is mandatory.\n                            'scheme': 'https://github.com/stac-extensions/osc#theme'\n                    }],\n        'formats': [{'name': 'GeoTIFF'}], //format of worfklow output\n        'osc:project': 'worldcereal2', // workflow related project\n        'osc:status': 'completed', // workflow status\n        'osc:type': 'workflow', // OSC type, for workflows should always be workflow\n        'license': 'varuious', // workflow license\n        \n    }\n}\n\nIn addition to specifying the links from the workflow to other parts of the catalog, it is required to add the reverse links:\n\nFrom the Workflow record.json to the workflows/catalog.json (listing all workflows in the OSC)\n\nFrom the associated Project to the Workflow\n\nFrom the associated Themes to the Workflow\n\n","type":"content","url":"/osc-pr-manual#id-2-3-add-new-workflow","position":13},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"3. Open a PR to merge the new information into the OSC.","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-3-open-a-pr-to-merge-the-new-information-into-the-osc","position":14},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"3. Open a PR to merge the new information into the OSC.","lvl2":"Adding information to the OSC"},"content":"After you have added all the information, commit and push your changes to the forked repository and open a pull request against the OSC - \n\nhttps://​docs​.github​.com​/en​/pull​-requests​/collaborating​-with​-pull​-requests​/proposing​-changes​-to​-your​-work​-with​-pull​-requests​/creating​-a​-pull​-request .\n\nOnce you open the PR, there will be an automatic validation run against the information you added . If it fails you will have to change some of the added information. You can see if the PR is successfull based on the specific CI run, in the screen shot below. If you click on the red X, the validator will give you the specific reason for the failure. \nPlease be advised that once a pull request (PR) is submitted to the open-science-catalog-metadata repository, it will undergo a review process conducted by members of the EarthCODE team. During this process, the content will be evaluated for completeness and accuracy. Should any additional information or modifications be required, you may be asked to update your PR accordingly. All communication related to the review will be provided through comments within the PR.\n\n","type":"content","url":"/osc-pr-manual#id-3-open-a-pr-to-merge-the-new-information-into-the-osc","position":15},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Alternatives"},"type":"lvl2","url":"/osc-pr-manual#alternatives","position":16},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Alternatives"},"content":"EarthCODE provides a \n\nGUI editor to automatically create links and open a PR for you.\n\nIf you are using one of the EarthCODE platforms, they provide specialised tools for automatic this work.\n\nYou can use libraries like pystac to automate some of the required work. \n\nThis tutorial shows how.","type":"content","url":"/osc-pr-manual#alternatives","position":17},{"hierarchy":{"lvl1":"Generating OSC information using pystac"},"type":"lvl1","url":"/osc-pr-pystac","position":0},{"hierarchy":{"lvl1":"Generating OSC information using pystac"},"content":"This notebook shows how to generate OSC Projects, Products and Workflows using pystac. EarthCODE provides a \n\nGUI editor that offers this and more functionality, including a user interface. However, if you decide to manually create items, using a library like pystac can save some time.\nThe code described here does not carry out all the required steps to pass the automated OSC validation. For example, you still have to generate all return links as described in the manual PR tutorial. You’ll also have to manually open the PR in the end.\n\nNOTE: Before you run the notebook you’ll need a fork of the open-science-catalog-metadata repository. See the Manual PR Tutorial about how to do it.","type":"content","url":"/osc-pr-pystac","position":1},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Import libraries"},"type":"lvl3","url":"/osc-pr-pystac#import-libraries","position":2},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Import libraries"},"content":"\n\nimport pystac\nfrom datetime import datetime\nfrom pystac.extensions.projection import ProjectionExtension\n\n","type":"content","url":"/osc-pr-pystac#import-libraries","position":3},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Get all entries from the Open Science Catalog"},"type":"lvl3","url":"/osc-pr-pystac#get-all-entries-from-the-open-science-catalog","position":4},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Get all entries from the Open Science Catalog"},"content":"\n\n# read the catalog root\ncatalog = pystac.Catalog.from_file('../../open-science-catalog-metadata/catalog.json')\n\n# access the list of the themes in open science catalog\nthemes = catalog.get_child('themes')\nallowed_themes = [child.id for child in themes.get_children()]\n\n\n# access the list of available ESA missions\nmissions = catalog.get_child('eo-missions')\nallowed_missions = [child.id for child in missions.get_children()]\n\n# access the list of avaiable variables\nvariables = catalog.get_child('variables')\nallowed_variables = [child.id for child in variables.get_children()]\n\n# access the list of existing projects, products and workflows\nproducts = catalog.get_child('products')\nprojects = catalog.get_child('projects')\nworkflows = catalog.get_child('workflows')\n\n","type":"content","url":"/osc-pr-pystac#get-all-entries-from-the-open-science-catalog","position":5},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Add new variables, theme and eo missions"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-add-new-variables-theme-and-eo-missions","position":6},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Add new variables, theme and eo missions"},"content":"\n\ndef add_product_variables(collection, variables_to_add):\n    '''Add variables to the collection custom fields and add links to the missions collection.'''\n    \n    for variable in variables_to_add:\n        \n        assert variable in allowed_variables\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=variables.get_child(variable).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"Variable: {variables.get_child(variable).title}\")\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n        \"osc:variables\": variables_to_add\n    })\n\ndef add_themes(collection, themes_to_add):\n    '''Add themes to the collection custom fields and add links to the themes collection.'''\n    \n    themes_list = []\n    for theme in themes_to_add:\n        \n        assert theme in allowed_themes\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=themes.get_child(theme).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"Theme: {themes.get_child(theme).title}\")\n        )\n        \n        themes_list.append(\n            {\n                \"scheme\": \"https://github.com/stac-extensions/osc#theme\",\n                \"concepts\": [{\"id\": theme}]\n            }\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n        \"themes\": themes_list\n    }\n    )\n\n\ndef add_links(collection, relations, targets, titles):\n\n    '''Add links from the collection to outside websites.'''\n    links = []\n    \n    for rel, target, title in zip(relations, targets, titles):\n        links.append(pystac.Link(rel=rel, target=target, title=title)),\n    \n    collection.add_links(links)\n\n\ndef create_contract(name, roles, emails):\n    '''Create a contact template'''\n    contact =  {\n        \"name\": name,\n        \"roles\": [r for r in roles]\n    }\n    if emails:\n        contact['emails'] = [{\"value\":email} for email in emails]\n    return contact\n\ndef add_product_missions(collection, missions_to_add):\n    '''Add missions to the collection custom fields and add links to the missions collection.'''\n    \n    for mission in missions_to_add:\n        \n        assert mission in allowed_missions\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=missions.get_child(mission).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"EO Mission: {missions.get_child(mission).title}\"\n            )\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n         \"osc:missions\": missions_to_add\n    }\n    )\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-add-new-variables-theme-and-eo-missions","position":7},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new project collection"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-project-collection","position":8},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new project collection"},"content":"\n\n\ndef create_project_collection(project_id, project_title, project_description, \n                      project_status, extent, project_license):\n\n    '''Create project collection template from the provided information.'''\n\n    # Create the collection\n    collection = pystac.Collection(\n        id=project_id,\n        description=project_description,\n        extent=extent,\n        license=project_license,\n        title=project_title,\n        extra_fields = {\n            \"osc:status\": project_status,\n            \"osc:type\": \"project\",\n            \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        },\n        stac_extensions=[\n            \"https://stac-extensions.github.io/osc/v1.0.0/schema.json\",\n            \"https://stac-extensions.github.io/themes/v1.0.0/schema.json\",\n            \"https://stac-extensions.github.io/contacts/v0.1.1/schema.json\"\n        ]\n    \n    )\n\n    # Add pre-determined links \n    collection.add_links([\n        pystac.Link(rel=\"root\", target=\"../../catalog.json\", media_type=\"application/json\", title=\"Open Science Catalog\"),\n        pystac.Link(rel=\"parent\", target=\"../catalog.json\", media_type=\"application/json\", title=\"Projects\"),\n        # pystac.Link(rel=\"self\", target=f\"https://esa-earthcode.github.io/open-science-catalog-metadata/projects/{project_id}/collection.json\", media_type=\"application/json\"),\n    ])\n\n    return collection\n\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-project-collection","position":9},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new product collection"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-product-collection","position":10},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new product collection"},"content":"\n\ndef create_product_collection(product_id, product_title, product_description, product_extent, product_license,\n                              product_keywords, product_status, product_region, product_project_id, product_project_title,\n                              product_parameters=None, product_doi=None):\n\n    collection = pystac.Collection(\n            id=product_id,\n            title=product_title,\n            description=product_description,\n            extent=product_extent,\n            license=product_license,\n            keywords=product_keywords,\n            stac_extensions=[\n                \"https://stac-extensions.github.io/osc/v1.0.0/schema.json\",\n                \"https://stac-extensions.github.io/themes/v1.0.0/schema.json\",\n                \"https://stac-extensions.github.io/cf/v0.2.0/schema.json\"\n            ],\n        )\n    \n    # Add pre-determined links \n    collection.add_links([\n        pystac.Link(rel=\"root\", target=\"../../catalog.json\", media_type=\"application/json\", title=\"Open Science Catalog\"),\n        pystac.Link(rel=\"parent\", target=\"../catalog.json\", media_type=\"application/json\", title=\"Products\"),\n        # pystac.Link(rel=\"self\", target=f\"https://esa-earthcode.github.io/open-science-catalog-metadata/products/{project_id}/collection.json\", media_type=\"application/json\"),\n        pystac.Link(rel=\"related\", target=f\"../../projects/{product_project_id}/collection.json\", media_type=\"application/json\", title=f\"Project: {product_project_title}\"),\n\n    ])\n\n    # Add extra properties\n    collection.extra_fields.update({\n        \"osc:project\": product_project_id,\n        \"osc:status\": product_status,\n        \"osc:region\": product_region,\n        \"osc:type\": \"product\",\n        \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n    })\n\n    if product_doi is not None:\n        collection.extra_fields[\"sci:doi\"] = product_doi\n\n\n    if product_parameters:\n        collection.extra_fields[\"cf:parameter\"] = [{\"name\": p} for p in product_parameters]\n    \n    return collection\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-product-collection","position":11},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new workflow record"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-workflow-record","position":12},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new workflow record"},"content":"\n\ndef create_workflow_collection(workflow_id, workflow_title, \n                               workflow_description, workflow_license, workflow_extent,\n                               workflow_keywords, workflow_formats, workflow_project, workflow_project_title):\n\n    '''Create a workflow collection template from the provided information.'''\n\n    # Create the collection\n\n    collection = {\n        'id': workflow_id,\n        'type': 'Feature',\n        'geometry': None,\n        \"conformsTo\": [\"http://www.opengis.net/spec/ogcapi-records-1/1.0/req/record-core\"],\n        \"properties\": {\n            \"title\": workflow_title,\n            \"description\": workflow_description,\n            \"osc:type\": \"workflow\",\n            \"osc:project\": workflow_project,\n            \"osc:status\": \"completed\",\n            \"formats\": [{\"name\": f} for f in workflow_formats],\n            \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"keywords\": workflow_keywords,\n            \"license\": workflow_license,\n            \"version\": \"1\"\n        },\n        \"linkTemplates\": [],\n        \"links\": [\n            \n            {\n                \"rel\": \"root\",\n                \"href\": \"../../catalog.json\",\n                \"type\": \"application/json\",\n                \"title\": \"Open Science Catalog\"\n            },            \n            {\n                \"rel\": \"parent\",\n                \"href\": \"../catalog.json\",\n                \"type\": \"application/json\",\n                \"title\": \"Workflows\"\n            },            \n  \n            {\n                \"rel\": \"related\",\n                \"href\": f\"../../projects/{workflow_project}/collection.json\",\n                \"type\": \"application/json\",\n                \"title\": f\"Project: {workflow_project_title}\"\n            },\n            \n        ]\n\n    }\n    \n    return collection\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-workflow-record","position":13},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new project"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-project","position":14},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new project"},"content":"\n\n# Define id, title, description, project status, license\nproject_id = \"worldcereal2\"\nproject_title = \"WorldCereal2\"\nproject_description = \"WorldCereal is an ESA initiative that provides global cropland and crop type maps at 10-meter resolution, offering seasonally updated data on temporary crops, croptypes (maize, winter cereals and spring cereals), and irrigation.\"\nproject_status = \"completed\"\nproject_license = 'proprietary'\n\n# Define spatial and temporal extent\nspatial_extent = pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]])\ntemporal_extent = pystac.TemporalExtent([[datetime(2021, 1, 1), datetime(2021, 12, 31, 23, 59, 59)]])\nextent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n\n# Define links and link titles\nproject_link_targets = [\"https://esa-worldcereal.org/en\", \n                        \"https://eo4society.esa.int/projects/worldcereal-global-crop-monitoring-at-field-scale/\"]\nproject_link_relations = [\"via\", \"via\"]\nproject_link_titles = [\"Website\", \"EO4Society Link\"]\n\n# Define project themes\nproject_themes = [\"land\"]\n\n# contacts\nproject_contracts_info = [\n    (\"Zoltan Szantoi\", [\"technical_officer\"], [\"Zoltan.Szantoi@esa.int\"]),\n    (\"VITO Remote Sensing\", [\"consortium_member\"], None)\n]\n\ncollection = create_project_collection(project_id, project_title, project_description, \n                      project_status, extent, project_license)\n\n# add links\nadd_links(collection, project_link_relations, project_link_targets, project_link_titles)\n\n## add themes\nadd_themes(collection, project_themes)\n\n\n# Add contacts\ncollection.extra_fields.update({\n\n    \"contacts\": [create_contract(*info) for info in project_contracts_info]\n    \n})\n\ncollection.validate()\n\ncollection\n\n# save this file and copy it to the catalog/projects/{project}/collection.json\ncollection.save_object(dest_href='project_collection.json')\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/projects/worldcereal2/\n!cp project_collection.json ../open-science-catalog-metadata-staging/projects/worldcereal2/collection.json\n\n","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-project","position":15},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new product"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-product","position":16},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new product"},"content":"\n\nproduct_id = \"worldcereal-crop-extent-belgium2\"\nproduct_title = \"WorldCereal Crop Extent - Belgium2\"\nproduct_description = \"WorldCereal is an ESA initiative that provides global cropland and crop type maps at 10-meter resolution, offering seasonally updated data on temporary crops, croptypes (maize, winter cereals and spring cereals), and irrigation. This dataset provides the outputs for Belgium.\"\nproduct_keywords = [\n    \"Crops\",\n    \"Cereal\"\n]\nproduct_status = \"ongoing\"\nproduct_license = \"proprietary\"\n\n# Define spatial and temporal extent\nproduct_spatial_extent = pystac.SpatialExtent([[2.5135, 49.529, 6.156, 51.475]])\nproduct_temporal_extent = pystac.TemporalExtent([[datetime(2021, 1, 1), datetime(2021, 12, 31, 23, 59, 59)]])\nproduct_extent = pystac.Extent(spatial=product_spatial_extent, temporal=product_temporal_extent)\nproduct_region = \"Belgium\"\nproduct_themes = [\"land\"]\nproduct_missions = [ \"sentinel-2\"]\nproduct_variables = [  \"crop-yield-forecast\" ]\nproduct_parameters = [  \"crop-yield-forecast\" ]\n\nproduct_project_id = \"worldcereal2\"\nproduct_project_title = \"WorldCereal2\"\n\nproduct_doi = \"https://doi.org/10.57780/s3d-83ad619\"\n\n\n# define links to add\n\nproduct_target_relations = ['child', 'via', 'via']\nproduct_target_links = ['https://eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation',\n                        'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation',\n                        'https://eoresults.esa.int/d/sentinel3-ampli-ice-sheet-elevation/2025/05/07/sentinel-3-ampli-user-handbook/S3_AMPLI_User_Handbook.pdf']\nproduct_target_titles = ['PRR link', 'Access', 'Documentation']\n\n\nproduct_collection = create_product_collection(\n    product_id, product_title, product_description, product_extent, \n    product_license, product_keywords, product_status, product_region, \n    product_project_id, product_project_title, product_parameters, product_doi)\n\n# add themes\nadd_themes(product_collection, product_themes)\n\n\n\nadd_product_missions(product_collection, product_missions)\n\nadd_product_variables(product_collection, product_variables)\n\n# add links\nadd_links(product_collection,\n          product_target_relations,\n          product_target_links,\n          product_target_titles\n)\n\nproduct_collection.validate()\n\nproduct_collection\n\n# save this file and copy it to the catalog/products/{product_id}/collection.json\nproduct_collection.save_object(dest_href='product_collection.json')\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/products/worldcereal-crop-extent-belgium2/\n!cp product_collection.json ../open-science-catalog-metadata-staging/products/worldcereal-crop-extent-belgium2/collection.json\n\n","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-product","position":17},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new workflow"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-workflow","position":18},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new workflow"},"content":"\n\nworkflow_id = \"worldcereal-workflow2\"\nworkflow_title=\"ESA worldcereal global crop extent detector2\"\nworkflow_description=\"Detects crop land at 10m resolution, trained for global use. Based on Sentinel-1 and 2 data...\"\nworkflow_license = \"proprietary\"\nworkflow_keywords= [\"agriculture\", \"crops\"]\nworkflow_formats = [\"GeoTIFF\"]\nworkflow_project = \"worldcereal2\"\nworkflow_project_title = \"WorldCereal2\"\n\nworkflow_themes = ['land']\n\n# Define spatial and temporal extent\nspatial_extent = pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]])\ntemporal_extent = pystac.TemporalExtent([[datetime(2022, 2, 1), datetime(2026, 1, 31, 23, 59, 59)]])\nworkflow_extent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n\n\n# add custom theme schemas\n\nworkflow_contracts_info = [\n    (\"Marie-Helene Rio\", [\"technical_officer\"], [\"marie-helene.rio@esa.int\"]),\n    (\"CNR-INSTITUTE OF MARINE SCIENCES-ISMAR (IT)\", [\"consortium_member\"], None),\n    (\"+ATLANTIC – Association for an Atla (PT)\", [\"consortium_member\"], None),\n]\n\nworkflow_collection = create_workflow_collection(workflow_id, workflow_title, \n                               workflow_description, workflow_license, workflow_extent,\n                               workflow_keywords, workflow_formats, workflow_project, workflow_project_title)\n\n# add contacts\nworkflow_collection['properties'].update({\n\n    \"contacts\": [create_contract(*info) for info in workflow_contracts_info]\n    \n})\n\n\nworkflow_collection['properties']['themes'] = [\n    {\n        \"scheme\": \"https://github.com/stac-extensions/osc#theme\",\n        \"concepts\": [{\"id\": t} for t in workflow_themes]\n    }\n]\n\nfor t in workflow_themes:\n    workflow_collection['links'].append(\n            {\n                    \"rel\": 'related',\n                    \"href\": f\"../../{t}/land/catalog.json\",\n                    \"type\": \"application/json\",\n                    \"title\": f'Theme: {t.capitalize()}'\n                }\n)\n\nworkflow_target_relations = ['openeo-process', 'git', 'service']\nworkflow_target_links = ['https://raw.githubusercontent.com/WorldCereal/worldcereal-classification/refs/tags/worldcereal_crop_extent_v1.0.1/src/worldcereal/udp/worldcereal_crop_extent.json',\n                        'https://github.com/WorldCereal/worldcereal-classification.git',\n                        'https://openeofed.dataspace.copernicus.eu']\nworkflow_target_titles = ['openEO Process Definition', 'Git source repository', 'CDSE openEO federation']\n\nfor rel, link, title in zip(workflow_target_relations, workflow_target_links, workflow_target_titles):\n    workflow_collection['links'].append(\n        {\n                \"rel\": rel,\n                \"href\": link,\n                \"type\": \"application/json\",\n                \"title\": title\n            }\n    )\n\nimport json\nwith open('record.json', 'w') as f:\n    json.dump(workflow_collection, f)\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/workflows/worldcereal-workflow2/\n!cp record.json ../open-science-catalog-metadata-staging/workflows/worldcereal-workflow2/record.json","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-workflow","position":19},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project"},"type":"lvl1","url":"/careheat-v2","position":0},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). The code below demonstrates how to perform the necessary steps using real data from the ESA project deteCtion and threAts of maRinE HEAT waves (CAREHeat). The focus of CAREHeat is to improve existing extreme marine heatwave(MHW) detection algorithms, contributing to a better understanding of their impacts.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\n🔗 Check the project website: \n\ndeteCtion and threAts of maRinE HEAT waves (CAREHeat) – Website\n\n🔗 Check the eo4society page: \n\ndeteCtion and threAts of maRinE HEAT waves (CAREHeat) – eo4society\n\nCareHeat Dataset source: \n\nCheck out the Dataset of Marine heatwaves and cold spells events based on ESA-CCI SSTs","type":"content","url":"/careheat-v2","position":1},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl3":"Acknowledgment"},"type":"lvl3","url":"/careheat-v2#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl3":"Acknowledgment"},"content":"We gratefully acknowledge the deteCtion and threAts of maRinE HEAT waves (CAREHeat) for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\nimport os\n\n","type":"content","url":"/careheat-v2#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"1. Generate the parent collection"},"type":"lvl2","url":"/careheat-v2#id-1-generate-the-parent-collection","position":4},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"1. Generate the parent collection"},"content":"The root STAC Collection provides a general description of all project outputs which will be stored on the PRR.\nThe PRR STAC Collection template enforces some required fields that you need to provide in order to build its valid description. Most of these metadata fields should already be available and can be extracted from your data.\n\n# create the parent collection\ncollectionid = \"careheat-marine-heatwaves-cold-spells\"\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Marine heatwaves and cold spells events based on ESA-CCI SSTs\",\n  \"description\": \"Marine heatwaves (MHWs) and cold spells (MCSs) prepared by the National Research Council - Institute of Marine Sciences (CNR-ISMAR, Italy) within the ESA-funded CAREHeat project. The catalogues are based on the ESA-CCI sea surface temperature (SST) dataset (available from https://doi.org/10.24381/cds.cf608234) for the period 1982-2022, on a regular 1°x1° longitude-latitude grid.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n         [-180, -90, 180, 90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"1982-01-01T00:00:00Z\",\n          \"2022-12-31T23:59:59Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"CC-BY-4.0\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\n","type":"content","url":"/careheat-v2#id-1-generate-the-parent-collection","position":5},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl2","url":"/careheat-v2#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":6},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"The second step is to describe the different files as STAC Items and Assets. Take your time to decide how your data should be categorised to improve usability of the data, and ensure intuitive navigation through different items in the collections. There are multiple strategies for doing this and this tutorial demonstrate one of the possible ways of doing that. Examples of how other ESA projects are doing this are available in the \n\nEarthCODE documentation .\n\nbaseurl = './data/careheat-marine-heatwaves-cold-spells/'\n\nbbox = [-180, -90, 180, 90]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\nfrom pathlib import Path\nbaseurl = Path('../../data/careheat-marine-heatwaves-cold-spells/')\nfiles = list(baseurl.glob(\"*.nc\"))\n\n# Convert to POSIX-style strings\nfile_paths = [f.as_posix() for f in files]\n\nfor file in file_paths:\n    print(file)\n\nfor file in file_paths:\n\n    # open the dataset and read metadata + convert to STAC\n    ds = xr.open_dataset(file)\n    detrended = 'Detrended (SSA) ' if '_ssa_' in file else ''\n    \n    template = {\n\n        \"id\": f\"{collectionid}-{file.split('/')[-1][:-3].lower()}\",\n        \"type\": \"Feature\",\n        \"stac_version\": \"1.0.0\",\n        \"properties\": {\n            \"title\": detrended + ds.attrs['product'],\n            \"description\": detrended + ds.attrs['description'],\n            \"start_datetime\": pd.to_datetime(ds.attrs['climatologyPeriod'][0], format='%Y').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"end_datetime\": pd.to_datetime(ds.attrs['climatologyPeriod'][-1], format='%Y').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"license\": \"CC-BY-4.0\",\n            \"created\": pd.to_datetime(ds.attrs['date'], format='%Y-%m').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"git_information\": ds.attrs['git_information'],\n            \"website\": ds.attrs['website'],\n            \"version\": ds.attrs['version'],\n            \"changelog\": ds.attrs['changelog'],\n            \"institution\": ds.attrs['institution'],\n            \"author\": ds.attrs['author'],\n            \"contact\": ds.attrs['contact'],\n        },\n        \"geometry\": geometry,\n        \"bbox\": bbox,\n        \"assets\": {\n            \"data\": {\n                \"href\": f\"./{collectionid}/{file.split('/')[-1]}\",  # or local path\n                \"type\": \"application/x-netcdf\",\n                \"roles\": [\"data\"],\n                \"title\": detrended + ds.attrs['product']\n            }\n        }\n    }\n    # 3. Generate the STAC Item\n    item = xarray_to_stac(\n        ds,\n        template,\n        temporal_dimension=\"time\" if 'time' in ds.coords else False,\n        x_dimension='lon',\n        y_dimension='lat',\n        reference_system=False\n    )\n\n    # validate and add the STAC Item to the collection\n    item.validate()\n    collection.add_item(item)\n\ncollection\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/careheat-v2#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":7},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset"},"type":"lvl1","url":"/creating-stac-catalog-from-prr-example","position":0},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset"},"content":"This is an example notebook for creating the STAC Items uploaded to ESA Project Results Repository and made available at: \n\nhttps://​eoresults​.esa​.int​/browser​/​#​/external​/eoresults​.esa​.int​/stac​/collections​/sentinel3​-ampli​-ice​-sheet​-elevation\n\nDataset is also discoverable via Open Science Catalogue, providing access to created in this tutorial collection stored in ESA Project Results Repository (PRR).\n\n\nhttps://​opensciencedata​.esa​.int​/products​/sentinel3​-ampli​-ice​-sheet​-elevation​/collection\n\nIt focuses on generating metadata for a project with a hundreads of items, each of which has hundreads of netcdf assets.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project **SRAL Processing over Land Ice\n**. With the focus of the project on improving Sentinel-3 altimetry performances over land ice.\n\n🔗 Check the : \n\nUser handbook\n\n🔗 Check the : \n\nScientifc publication","type":"content","url":"/creating-stac-catalog-from-prr-example","position":1},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl4":"Acknowledgment"},"type":"lvl4","url":"/creating-stac-catalog-from-prr-example#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl4":"Acknowledgment"},"content":"We gratefully acknowledge the SRAL Processing over Land Ice team for providing access to the data used in this example, as well as support in creating it.","type":"content","url":"/creating-stac-catalog-from-prr-example#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"Steps described in this notebook"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#steps-described-in-this-notebook","position":4},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"Steps described in this notebook"},"content":"This notebook presents the workflow for generating a PRR Collection for the entire dataset coming from the project. To create a valid STAC Items and Collection you should follow steps described below:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the collection\n\nSave the normalised collection\n\nDue to the complexity of the project and the time it takes to process the data, the STAC Items are generated first and stored locally. They are added to the collection afterwards.\n\nThis notebook can be used as an example for following scenario(s):\n\nCreating the STAC Items from the files stored locally\n\nCreating the STAC Items from files stored in the s3bucket or other cloud repository\n\nCreating the STAC Items from files already ingested into PRR\n\nOf course if your files are locally stored, or stored in a different S3 Bucket the access to them (roor_url and items paths) should be adapted according to your dataset location.\n\nNote: Due to the original size of the dataset ~ 100GB, running this notebook end to end may take hours. We do advise therefore to trying it on your own datasets by changing file paths to be able to produe valid STAC Collaction and STAC Items.\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#steps-described-in-this-notebook","position":5},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"Loading Libraries"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#loading-libraries","position":6},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"Loading Libraries"},"content":"\n\nimport json\nimport time\nimport pystac\nimport rasterio\nfrom shapely import box\nimport pandas as pd\nimport xarray as xr\nfrom datetime import datetime\nfrom dateutil.parser import isoparse\nfrom dateutil import parser\nfrom dateutil.parser import parse\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#loading-libraries","position":7},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"2. Get Product file links"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-2-get-product-file-links","position":8},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"2. Get Product file links"},"content":"Note: We are using the links from the PRR directly, however when the notebook was created originally all the files were available locally. Furthermore, due to the original size of the dataset ~ 100GB, running this notebook end to end may take hours. We do advise therefore to trying it on your own datasets by changing file paths to be able to produe valid STAC Collaction and STAC Items.\n\nroot_url = 'https://eoresults.esa.int' # provide a root url for the datasets items \n\n# get all items for the S3 AMPLI collection from the PRR STAC API\nitems = pystac.ItemCollection.from_file('https://eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation/items?limit=10_000')\n\n# get the paths to all the data\n\n# using a dictionary is faster than using pystac\nitems_dict = items.to_dict()\nall_item_paths = []\nfor item in items_dict['features']:\n    assets = item['assets']\n    for asset_name, asset_dict in assets.items():\n        if asset_dict['roles'] == ['data']:\n            all_item_paths.append(asset_dict['href'])\n\n# Create a list of EO Missions and instruments as well as region of the dataset and cycles\ninstruments = ['sentinel-3a', 'sentinel-3b']\nregions = ['antarctica', 'greenland']\ncycles = [f\"cycle{str(i).zfill(3)}\" for i in range(5, 112)]  # Cycle005 to Cycle111\n\n# Assign the instrument name based on the acronym used in the file name\nrenaming = {\n    'S3A': 'sentinel-3a',\n    'S3B': 'sentinel-3b',\n    'ANT': 'antarctica',\n    'GRE': 'greenland'\n}\n\nDefine geometries, which are the same for all items within the same region. If they are not, these have to be extracted from the assets inside the item.\n\n# Define the spatial extent (bbox) for each region of interest\ngreenland_bbox = [-74.0, 59.0, -10.0, 84.0]\ngreenland_geometry = json.loads(json.dumps(box(*greenland_bbox).__geo_interface__))\n\nantarctica_bbox = [-180.0, -90.0, 180.0, -60.0]\nantarctica_geometry = json.loads(json.dumps(box(*antarctica_bbox).__geo_interface__))\n\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-2-get-product-file-links","position":9},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"2.1 Group the files by the instruments, region and cycle of the dataset","lvl2":"2. Get Product file links"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-2-1-group-the-files-by-the-instruments-region-and-cycle-of-the-dataset","position":10},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"2.1 Group the files by the instruments, region and cycle of the dataset","lvl2":"2. Get Product file links"},"content":"\n\ndata = []\n\nfor ipath in all_item_paths:\n    splitname = ipath.split('/')[-1].split('_')\n    instrument = splitname[0]\n    cycle = splitname[9]\n    region = splitname[-2]\n\n    data.append((renaming[instrument], renaming[region], cycle, ipath))\n\n\nfiledata = pd.DataFrame(data, columns=['instrument', 'region', 'cycle', 'path'])\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-2-1-group-the-files-by-the-instruments-region-and-cycle-of-the-dataset","position":11},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-3-create-the-stac-items-with-the-metadata-from-the-original-files-loaded-from-the-prr","position":12},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\n# group all files into items from the same instrument, region and cycle\nfor (instrument, region, cycle), links in filedata.groupby(['instrument', 'region', 'cycle']):\n    \n    # open the metadata attributes for each file in the group\n    datasets = [xr.open_dataset(root_url + link + '#mode=bytes') for link in links['path']]\n\n\n    # Define the Temporal extent\n    first_item = datasets[0]\n    last_item = datasets[-1]\n    props = first_item.attrs\n    props2 = last_item.attrs\n\n    start_datetime = props.get(\"first_meas_time\")\n    end_datetime = props2.get(\"last_meas_time\")\n\n    # Define the geometry\n    if props['zone'] == 'Antarctica':\n        bbox = antarctica_bbox\n        geometry = antarctica_geometry\n    elif props['zone'] == 'Greenland':\n        bbox = greenland_bbox\n        geometry = greenland_geometry\n\n\n    # Shared properties\n    properties = {\n        \"start_datetime\": start_datetime,\n        \"end_datetime\": end_datetime,\n        \"created\": props.get(\"processing_date\"),\n        \"description\": f\"Sentinel-3 AMPLI Land Ice Level-2 product acquired by {instrument.capitalize()} platform derived from the SRAL altimeter in Earth Observation mode over {region} region.\",\n        \"conventions\": props.get(\"Conventions\"),\n        \"platform_name\": props.get(\"platform_name\"),\n        \"platform_serial_identifier\": props.get(\"platform_serial_identifier\"),\n        \"altimeter_sensor_name\": props.get(\"altimeter_sensor_name\"),\n        \"operational_mode\": props.get(\"operational_mode\"),\n        \"cycle_number\": props.get(\"cycle_number\"),\n        \"netcdf_version\": props.get(\"netcdf_version\"),\n        \"product_type\": props.get(\"product_type\"),\n        \"timeliness\": props.get(\"timeliness\"),\n        \"institution\": props.get(\"institution\"),\n        \"processing_level\": props.get(\"processing_level\"),\n        \"processor_name\": props.get(\"processor_name\"),\n        \"processor_version\": props.get(\"processor_version\"),\n        \"references\": props.get(\"references\"),\n        \"zone\": props.get(\"zone\"),\n    }\n\n\n    # Create STAC item for the cycle\n    item = pystac.Item(\n        id=f\"sentinel-3{props.get(\"platform_serial_identifier\").lower()}-{props.get(\"zone\").lower()}-{cycle.lower()}\",\n        geometry=geometry,\n        bbox=bbox,\n        datetime=isoparse(start_datetime),\n        properties=properties\n    )\n\n    item.stac_version = \"1.1.0\"\n    item.stac_extensions = [\n        \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\",\n        \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\",\n        \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\"\n    ]\n\n    item.assets = {}\n\n    # Add assets from that cycle\n    for nc_href, ds in zip(links['path'], datasets):\n\n        asset_title = ds.attrs['product_name']\n        extra_fields = {\n            \"cycle_number\": str(ds.attrs.get(\"cycle_number\")),\n            \"orbit_number\": str(ds.attrs.get(\"orbit_number\")),\n            \"relative_orbit_number\": str(ds.attrs.get(\"relative_orbit_number\")),\n            \"orbit_direction\": ds.attrs.get(\"orbit_direction\"),\n        }\n\n        item.add_asset(\n            key=asset_title,\n            asset=pystac.Asset(\n                href=nc_href,\n                media_type=\"application/x-netcdf\",\n                roles=[\"data\"],\n                extra_fields=extra_fields\n            )\n        )\n\n    # Save STAC item per cycle\n    json_filename = f\"sentinel-3{props.get(\"platform_serial_identifier\").lower()}-{props.get(\"zone\").lower()}-{cycle.lower()}.json\"\n    item.save_object(dest_href='examples/' + json_filename, include_self_link=False)\n    print(f\" Saved {json_filename}\")\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-create-the-stac-items-with-the-metadata-from-the-original-files-loaded-from-the-prr","position":13},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.1  Import documentation","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-3-1-import-documentation","position":14},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.1  Import documentation","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\nimport pystac\nfrom datetime import datetime\nimport os\nfrom datetime import datetime, timezone\n\ndate_str = \"07/05/2025\"\n\n# Convert to ISO format string (YYYY-MM-DD)\niso_like_str = datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n\n# Parse with isoparse and attach UTC timezone\ndt_utc = isoparse(iso_like_str).replace(tzinfo=timezone.utc)\n\nprint(dt_utc.isoformat())\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-1-import-documentation","position":15},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.2  Create STAC Item for the documentation associated to the dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-3-2-create-stac-item-for-the-documentation-associated-to-the-dataset","position":16},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.2  Create STAC Item for the documentation associated to the dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\n# Basic metadata\ndoc_href = \"/d/S3_AMPLI_User_Handbook.pdf\"  # Relative or absolute href\ndoc_title = \"Sentinel-3 Altimetry over Land Ice: AMPLI level-2 Products\"\ndoc_description = \"User Handbook for Sentinel-3 Altimetry over Land Ice: AMPLI level-2 Products\"\n\n# Create STAC item\nitem = pystac.Item(\n    id=\"sentinel-3-ampli-user-handbook\",\n    geometry=None,\n    bbox=None,\n    datetime=dt_utc,\n    properties={\n        \"title\": doc_title,\n        \"description\": doc_description,\n        \"reference\": \"CLS-ENV-MU-24-0389\",\n        \"issue_n\": dt_utc.isoformat()\n    }\n)\n\n# Add asset for the PDF\nitem.add_asset(\n    key=\"documentation\",\n    asset=pystac.Asset(\n        href=doc_href,\n        media_type=\"application/pdf\",\n        roles=[\"documentation\"],\n        title=doc_title\n    )\n)\n\n# Save to file\nitem.set_self_href(\"examples/sentinel-3-ampli-user-handbook.json\")\nitem.save_object(include_self_link=False)\n\nprint(\"📄 STAC Item for documentation created: sentinel-3-ampli-user-handbook.json\")\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-2-create-stac-item-for-the-documentation-associated-to-the-dataset","position":17},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"4. Generate valid STAC collection"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-4-generate-valid-stac-collection","position":18},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"4. Generate valid STAC collection"},"content":"Once all the assets are processed, create the parent collection for all Items created in the previous step.\n\ncollection = pystac.Collection.from_dict(\n\n{\n  \"id\": \"sentinel3-ampli-ice-sheet-elevation\",\n  \"type\": \"Collection\",\n  \"links\": [\n  ],\n  \"title\": \"Sentinel-3 AMPLI Ice Sheet Elevation\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-180, -90, 180, 90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2016-06-01T00:00:00Z\",\n          \"2024-05-09T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"CC-BY-4.0\",\n  \"summaries\": {\n    \"references\": [\n      \"https://doi.org/10.5194/egusphere-2024-1323\"\n    ],\n    \"institution\": [\n      \"CNES\"\n    ],\n    \"platform_name\": [\n      \"SENTINEL-3\"\n    ],\n    \"processor_name\": [\n      \"Altimeter data Modelling and Processing for Land Ice (AMPLI)\"\n    ],\n    \"operational_mode\": [\n      \"Earth Observation\"\n    ],\n    \"processing_level\": [\n      \"2\"\n    ],\n    \"processor_version\": [\n      \"v1.0\"\n    ],\n    \"altimeter_sensor_name\": [\n      \"SRAL\"\n    ]\n  },\n  \"description\": \"Ice sheet elevation estimated along the Sentinel-3 satellite track, as retrieved with the Altimeter data Modelling and Processing for Land Ice (AMPLI). The products cover Antarctica and Greenland.\",\n  \"stac_version\": \"1.1.0\"\n}\n)\ncollection\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-generate-valid-stac-collection","position":19},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.1. Add items to collection","lvl2":"4. Generate valid STAC collection"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-4-1-add-items-to-collection","position":20},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.1. Add items to collection","lvl2":"4. Generate valid STAC collection"},"content":"Once the collection is created read all the items from disk and add the necassary links.\n\nimport glob\nfor fpath in glob.glob('examples/*'):\n    collection.add_item(pystac.Item.from_file(fpath))\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-1-add-items-to-collection","position":21},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.2 Save the normalised collection","lvl2":"4. Generate valid STAC collection"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-4-2-save-the-normalised-collection","position":22},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.2 Save the normalised collection","lvl2":"4. Generate valid STAC collection"},"content":"\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href='../data/example_catalog_ampli/',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-2-save-the-normalised-collection","position":23},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview"},"type":"lvl1","url":"/prr-stac-download-example","position":0},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview"},"content":"This notebook has been created to support the access to the users of EarthCODE and APEX, who would like to exploit available products and project results stored in the \n\nESA Project Results Repository (PRR). PRR provides access to data, workflows, experiments and documentation from ESA EOP-S Projects organised across Collections, accessible via \n\nOGC Records e S\n\nTAC API.\n\nEach collection contains \n\nSTAC Items, with their related assets stored within the PRR storage.\n\nScientists/commercial companies can access the PRR via the \n\nEarthCODE and \n\nAPEx projects.\n\nUse following notebook cells to preview the content of the ESA PRR and request the download of selected products.\n\n","type":"content","url":"/prr-stac-download-example","position":1},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Loading Libraries and set up logging level"},"type":"lvl3","url":"/prr-stac-download-example#loading-libraries-and-set-up-logging-level","position":2},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Loading Libraries and set up logging level"},"content":"\n\nimport os\nimport logging\nimport pprint\nimport shutil\nfrom urllib.parse import urljoin\nfrom urllib.request import urlretrieve\n\n#Make sure you have installed pystac_client before running this\nfrom pystac_client import Client\n\n# set pystac_client logger to DEBUG to see API calls\nlogging.basicConfig()\nlogger = logging.getLogger(\"pystac_client\")\nlogger.setLevel(logging.DEBUG)\n\n\n","type":"content","url":"/prr-stac-download-example#loading-libraries-and-set-up-logging-level","position":3},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Connect to ESA PRR Catalog and display the list of collections available"},"type":"lvl3","url":"/prr-stac-download-example#connect-to-esa-prr-catalog-and-display-the-list-of-collections-available","position":4},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Connect to ESA PRR Catalog and display the list of collections available"},"content":"\n\n# URL of the STAC Catalog to query\ncatalog_url = \"https://eoresults.esa.int/stac\"\n\n# custom headers\nheaders = []\n\ncat = Client.open(catalog_url, headers=headers)\ncat # display the basic informaiton about PRR Catalog in STAC Format\n\n\n\nUse the cell below to access entire list of collections available in ESA PRR.\n\ncollection_search = cat.collection_search(limit=150)\nprint(f\"Total number of collections found in ESA PRR is {collection_search.matched()}\")\n\n# Display the name of the names of collection (collection-ids) to be used to filter the colleciton of interest\nfor collection in collection_search.collections_as_dicts():\n    print(collection.get(\"id\", \"Unnamed Collection\"))\n\n\n\nAlternatively, you can display the metadata of all STAC Collections available\n\n# Or they can be displayed with their full metadata\ncollection_search = cat.collection_search(\n    datetime='2023-04-02T00:00:00Z/2024-08-10T23:59:59Z',  #this is an additional filter to be added to filter the collections based on the date.\n    limit=10\n)\nprint(f\"{collection_search.matched()} collections found\")\nprint(\"PRR available Collections\\n\")\n\nfor results in collection_search.collections_as_dicts():  # maybe this part should not display entire dic\n    pp = pprint.PrettyPrinter(depth=4)\n    pp.pprint(results)\n\n","type":"content","url":"/prr-stac-download-example#connect-to-esa-prr-catalog-and-display-the-list-of-collections-available","position":5},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Open Sentinel-3 AMPLI Ice Sheet Elevation collection"},"type":"lvl3","url":"/prr-stac-download-example#open-sentinel-3-ampli-ice-sheet-elevation-collection","position":6},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Open Sentinel-3 AMPLI Ice Sheet Elevation collection"},"content":"\n\nTo access specific collection, we will use the collection id from the cell above. Type sentinel3-ampli-ice-sheet-elevation to connect to selected collection and display its metadata.\n\ncollection = cat.get_collection(\"sentinel3-ampli-ice-sheet-elevation\") # place here the id of the selected collection\n#collection # or use simply json metadata to display the information \nprint(\"PRR Sentinel-3 AMPLI Collection\\n\")\npp = pprint.PrettyPrinter(depth=4)\npp.pprint(collection.to_dict())\n\n#Or display it in the STAC file format to better visualise the attributes and properties \ncollection\n\n\n\nFrom the cell below, we will retrieve and explore queryable fields from a STAC API, which allows us to understand what parameters we can use for filtering our searches.\n\nqueryable = collection.get_queryables()\n\npp = pprint.PrettyPrinter(depth=4)\npp.pprint(queryable)\n\n","type":"content","url":"/prr-stac-download-example#open-sentinel-3-ampli-ice-sheet-elevation-collection","position":7},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Display STAC Items from Sentinel-3 AMPLI Ice Sheet Elevation collection"},"type":"lvl3","url":"/prr-stac-download-example#display-stac-items-from-sentinel-3-ampli-ice-sheet-elevation-collection","position":8},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Display STAC Items from Sentinel-3 AMPLI Ice Sheet Elevation collection"},"content":"\n\nBy executing the cell below you will get the ids of items that can be found in the specific collection (requested above).\nFirst five items from the list are printed out.\n\nitems = collection.get_items()\n\n# flush stdout so we can see the exact order that things happen\ndef get_five_items(items):\n    for i, item in enumerate(items):\n        print(f\"{i}: {item}\", flush=True)\n        if i == 4:\n            return\n        \nprint(\"First page\", flush=True)\nget_five_items(items)\n\nprint(\"Second page\", flush=True)\nget_five_items(items)\n\nNow execute a search with a set of parameters. In this case it returns just one item because we filter on one queryable parameter (id)\n\n#Search for items based on spatio-temporal properties\n\n# AOI entire world\ngeom = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-180, -90],\n            [-180, 90],\n            [180 , 90],\n            [180, -90],\n            [-180, -90],\n        ]\n    ],\n}\n\n# limit sets the # of items per page so we can see multiple pages getting fetched\n#In this search we apply also filtering on ID that is one of the searchable parameters for the colletion\nsearch = cat.search(\n    max_items=7,\n    limit=5,\n    collections=\"sentinel3-ampli-ice-sheet-elevation\",        # specify collection id\n    intersects=geom,\n    query={\"id\": {\"eq\": \"sentinel-3a-antarctica-cycle107\"}},  # search for the specific Item in the collection \n    datetime=\"2023-04-02T00:00:00Z/2024-08-10T23:59:59Z\",     # specify the start and end date of the time frame to perform the search \n)\n\nitems = list(search.items())\n\nprint(len(items))\n\npp = pprint.PrettyPrinter(depth=4)\npp.pprint([i.to_dict() for i in items])\n\n\n\nIf you do not know the item id, search through available satellite instrument name, region, number of the cycle and the datetime range of the products of interest. \nYou can specify them by filtering based on following possible values: \n\nmissions: 3a or 3b\n\nregions: anarctica or greenland\n\ncycle range: for sentinel-3a possible cycle range is from 005 to 112; while sentinel-3b has range from 011-093\n\ndatetime: specify the time frame of the products from the range between: 2016-06-01 00:00:00 UTC – 2024-05-09 00:00:00 UTC \n\n#Search for items from specific mission and type of the instrument (based on the id) and the region as well as cycle number \n# Define your cycle range and mission types\ncycle_range = [f\"{i:03d}\" for i in range(90, 111)] #005 to 111   # for sentinel-3a possible cycle range is from 005 to 111; while s3b has range from 011-092\nmissions = [\"3b\"]          # select the mission and sensor type from:\"sentinel-3a\" or \"sentinel-3b\"]  \nregions = [\"antarctica\"]              # specify the region from: \"antarctica\" or \"greenland\"\n\n# AOI entire world\ngeom = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-180, -90],\n            [-180, 90],\n            [180 , 90],\n            [180, -90],\n            [-180, -90],\n        ]\n    ],\n}\n\n# limit sets the # of items per page so we can see multiple pages getting fetched\n#In this search we apply also filtering on ID that is one of the searchable parameters for the colletion\nsearch = cat.search(\n    max_items=7,\n    limit=5,\n    collections=\"sentinel3-ampli-ice-sheet-elevation\",\n    intersects=geom,  # search for the specific Item in the collection \n    datetime=\"2021-04-02T00:00:00Z/2024-08-10T23:59:59Z\",     # specify the start and end date of the time frame to perform the search which are: 2016-06-01 00:00:00 UTC – 2024-05-09 00:00:00 UTC\n)\nitems = list(search.items())\nprint(f\"Number of items found: {len(items)}\")\nprint(items)\n\npp = pprint.PrettyPrinter(depth=4)\n\nfiltered = [\n    item for item in items\n    if any(m in item.id.lower()  for m in missions)\n    and any(r in item.id.lower()  for r in regions)\n    and any(f\"cycle{c}\" in item.id.lower() for c in cycle_range)\n]\n\n\n#for i, item in enumerate(filtered, 2):\n   # print(f\"{i}. {item.id} @ {item.datetime}\")\n\n## Print number of filtered items\nprint(f\"Number of filtered items: {len(filtered)}\")\nfor i, item in enumerate(filtered, 2):\n    print(f\"{i}. {item.id} @ {item.datetime}\")\n\n","type":"content","url":"/prr-stac-download-example#display-stac-items-from-sentinel-3-ampli-ice-sheet-elevation-collection","position":9},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download all assets from the selected item "},"type":"lvl2","url":"/prr-stac-download-example#download-all-assets-from-the-selected-item","position":10},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download all assets from the selected item "},"content":"Based on the selection done in the previous cell, download the products to the downloads folder in your workspace\n\nbase_url = \"https://eoresults.esa.int\"\n\nitem_to_be_downloaded = 3\ntarget = items[item_to_be_downloaded]\n\noutput_dir = f\"downloads/{target.id}\"\nos.makedirs(output_dir, exist_ok=True)\n\nassets_total=len(target.assets.items())\nassets_current=0\nfor asset_key, asset in target.assets.items():\n    filename = os.path.basename(asset.href)\n    full_href = urljoin(base_url, asset.href)\n    local_path = os.path.join(output_dir, filename)\n    assets_current+=1\n    print(f\"[{assets_current}/{assets_total}] Downloading {filename}...\")\n    try:\n        urlretrieve(full_href, local_path)\n    except Exception as e:\n        print(f\"Failed to download {full_href}. {e}\")\n\n\n","type":"content","url":"/prr-stac-download-example#download-all-assets-from-the-selected-item","position":11},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download filtered items "},"type":"lvl2","url":"/prr-stac-download-example#download-filtered-items","position":12},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download filtered items "},"content":"Based on the selection done in the previous cell, download the products to the downloads folder in your workspace. You will download here the items which result from further filtering options (by mission type, cycle number, region etc.)\n\ntarget = filtered[0] if len(filtered) > 0 else None\n\noutput_dir = f\"downloads/{target.id}\"\nos.makedirs(output_dir, exist_ok=True)\n\nassets_total=len(target.assets.items())\nassets_current=0\nfor asset_key, asset in target.assets.items():\n    filename = os.path.basename(asset.href)\n    full_href = urljoin(base_url, asset.href)\n    local_path = os.path.join(output_dir, filename)\n    assets_current+=1\n    print(f\"[{assets_current}/{assets_total}] Downloading {filename}...\")\n    try:\n        urlretrieve(full_href, local_path)\n    except Exception as e:\n        print(f\"Failed to download {full_href}. {e}\")     \n\nbase_url = \"https://eoresults.esa.int\"\nfor index, item in enumerate(filtered, 2):\n    output_dir = f\"filtered/{item.id}\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    assets_total = len(item.assets.items())\n    assets_current = 0\n\n    for asset_key, asset in item.assets.items():\n        filename = os.path.basename(asset.href)\n        full_href = urljoin(base_url, asset.href)\n        local_path = os.path.join(output_dir, filename)\n\n        assets_current += 1\n        print(f\"[{index}] [{assets_current}/{assets_total}] Downloading {filename} for item {item.id}...\")\n\n        try:\n            urlretrieve(full_href, local_path)\n        except Exception as e:\n            print(f\"Failed to download {full_href}. {e}\")\n\nprint(f\"Downloaded assets for {len(filtered)} items.\")\n\n","type":"content","url":"/prr-stac-download-example#download-filtered-items","position":13},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Read some data to ensure all items are downloaded properly"},"type":"lvl2","url":"/prr-stac-download-example#id-optional-read-some-data-to-ensure-all-items-are-downloaded-properly","position":14},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Read some data to ensure all items are downloaded properly"},"content":"\n\nimport xarray as xr\nimport numpy as np\n\n# change this to a downloaded file\nexample_filepath = f'./downloads/{target.id}/S3A_SR_2_TDP_LI_20240403T201315_20240403T201615_20250416T191921_0180_111_014______CNE_GRE_V001.nc'\n\n# Open selected product and check the values\n# Note: You can select another group of values to read : satellite_and_altimeter, or ESA_L2_processing\nds = xr.open_dataset(example_filepath, group='AMPLI_processing')\nvalues = ds['elevation_radar_ampli'].values\nvalues[~np.isnan(values)]\n\n","type":"content","url":"/prr-stac-download-example#id-optional-read-some-data-to-ensure-all-items-are-downloaded-properly","position":15},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Create an archive of products downloaded"},"type":"lvl2","url":"/prr-stac-download-example#id-optional-create-an-archive-of-products-downloaded","position":16},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Create an archive of products downloaded"},"content":"\n\nCreate an archive of the products downloaded to your workspace and save them in .zip format to make them compressed\n\n# Create an archive of downloaded products \nzip_path = shutil.make_archive(output_dir, 'zip', root_dir=output_dir)\nprint(f\"Created ZIP archive: {zip_path}\")","type":"content","url":"/prr-stac-download-example#id-optional-create-an-archive-of-products-downloaded","position":17},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset"},"type":"lvl1","url":"/prr-stac-introduction","position":0},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset"},"content":"","type":"content","url":"/prr-stac-introduction","position":1},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Introduction"},"type":"lvl3","url":"/prr-stac-introduction#introduction","position":2},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Introduction"},"content":"\n\nThis notebook has been created to show the core steps required of EarthCODE users to upload their research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with a single  netcdf file. Checkout the other projects on the webpage for more complex examples.\n\nPRR provides access to data, workflows, experiments and documentation from ESA Projects organised across Collections, accessible via the \n\nSTAC API. Each Collection contains \n\nSTAC Items, with their related Assets stored within the PRR storage. Scientists/commercial companies can access the PRR via the \n\nEarthCODE and \n\nAPEx projects.\n\nThe \n\nSTAC Specification, provides detailed explanation and more information on this metadata format.\n\nIn order to upload data to the ESA Project Results Repository (PRR) you have to generate a STAC Collection that is associated to your files. The STAC Collection provides metadata about your files and makes them searchable and machine readable. The metadata generation process is organised in four steps process:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the Collection\n\nSave the normalised Collection\n\nThe easiest way to generate all the required files is to use a STAC library, such as pystac or riostac. This library will take care of creating the links and formating the files in the correct way.  In the examples below we are using pystac.\n\nHave a look at the steps below and learn how to prepare your dataset to generate a valid STAC Collection. You will find all the steps descibed in the markdown cell, together with the example code (executable) to make this process easier. Please adjust the information in the fields required to describe your Collection and Items according to the comments, starting with : “#”\n\nNOTE: Depending on the information that you put in the Assets or Items the code, you may get an error about an object not being json-serialisable. If this happens, you have to transform the problem field into an object that can be described using standard JSON. For example, transforming a numpy array into a list.\n\n","type":"content","url":"/prr-stac-introduction#introduction","position":3},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"type":"lvl3","url":"/prr-stac-introduction#id-example-4datlantic-ohc-project","position":4},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"content":"The code below demonstrates how to perform the necessary steps using real data from the ESA Regional Initiative Project 4DATLANTIC-OHC. The project focuses on ocean heat content and provides monthly gridded Atlantic Ocean heat content change as well as OHC trends and their uncertainties.\n\n🔗 Learn more about the project here: \n\n4DATLANTIC-OHC – EO4Society \n🔗 Check the project website: \n\n4DATLANTIC-OHC – Website","type":"content","url":"/prr-stac-introduction#id-example-4datlantic-ohc-project","position":5},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl4":"Acknowledgment","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"type":"lvl4","url":"/prr-stac-introduction#acknowledgment","position":6},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl4":"Acknowledgment","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"content":"We gratefully acknowledge the 4DATLANTIC-OHC project team for providing access to the data used in this example.\n\nThis example is intended to help you understand the workflow and apply similar steps to your own Earth observation data analysis. \n\n","type":"content","url":"/prr-stac-introduction#acknowledgment","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Import necessary Python libraries"},"type":"lvl3","url":"/prr-stac-introduction#import-necessary-python-libraries","position":8},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Import necessary Python libraries"},"content":"You can create an example conda/miniconda enviroment to run the below code using:conda create -n prr_stack_example pystac xarray shapely\nconda activate prr_stack_example\n\n# import libraries\nfrom pystac import Collection\nimport pystac\nimport xarray as xr\nimport shapely\nimport json\nfrom datetime import datetime\n\n","type":"content","url":"/prr-stac-introduction#import-necessary-python-libraries","position":9},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"1. Generate a root STAC collection"},"type":"lvl3","url":"/prr-stac-introduction#id-1-generate-a-root-stac-collection","position":10},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"1. Generate a root STAC collection"},"content":"The root STAC Collection provides a general description of the enitre dataset, that you would like to store in ESA PRR. In the STAC Specification a Collection is defined as  an extension of the STAC Catalog with additional information such as the extents, license, keywords, providers, etc that describe STAC Items that fall within the Collection. \n\nIn short: it behaves as the container to store the various Items that build up your dataset. \n\nSTAC Collection has some required fields that you need to provide in order to build its valid description. Most of these metadata fields should be extracted from your data.\nPlease have a look at the example below.{\n  \"type\": \"Collection\", # Do not change\n  \"id\": \"\", # add a unique variation of project name + dataset name \n  \"stac_version\": \"1.1.0\", # Do not change\n  \"title\": \"\", # Meaningful title of your dataset\n  \"description\": \"\", # General description of your dataset\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          -180.0,\n          -90.0,\n          180.0,\n          90.0\n        ]\n      ]\n    }, # Spatial extent of your dataset. If you have multiple data files take the minimum bounding box that covers all.\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"1982-01-01T00:00:00Z\",\n          \"2022-12-31T23:59:59Z\"\n        ] # Temporal extent of your dataset. If you have multiple data files take the minimum temporal range that covers all.\n      ]\n    }\n  },\n\"license\": \"\", # the license that applies to entire dataset\n\"links\": [] # do not change\n\n}\n\n","type":"content","url":"/prr-stac-introduction#id-1-generate-a-root-stac-collection","position":11},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create Collection","lvl3":"1. Generate a root STAC collection"},"type":"lvl5","url":"/prr-stac-introduction#example-create-collection","position":12},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create Collection","lvl3":"1. Generate a root STAC collection"},"content":"\n\n# define collection id, since it will be reused\ncollectionid = \"4datlantic-ohc\"\n\n# create the root collection using pystac.Collection\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Atlantic Ocean heat content change\",\n  \"description\": \"Given the major role of the Atlantic Ocean in the climate system, it is essential to characterize the temporal and spatial variations of its heat content. The OHC product results from the space geodetic approach also called altimetry-gravimetry approach. This dataset contains variables as 3D grids of ocean heat content anomalies at 1x1 resolution and monthly time step. Error variance-covariance matrices of OHC at regional scale and annual resolution are also provided. See Experimental Dataset Description for details: https://www.aviso.altimetry.fr/fileadmin/documents/data/tools/OHC-EEI/OHCATL-DT-035-MAG_EDD_V2.0.pdf.Version. V2-0 of Dataset published 2022 in Centre National d’Etudes Spatiales. This dataset has been produced within the framework of the 4DAtlantic-Ocean heat content Project funded by ESA.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-100, \n         -90, \n         25,\n         90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2002-04-15T18:07:12Z\",\n          \"2023-09-01T18:59:59Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"Aviso License\",\n  \"links\": []\n\n}\n\n)\n\ncollection\n\n","type":"content","url":"/prr-stac-introduction#example-create-collection","position":13},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl3","url":"/prr-stac-introduction#id-2-group-your-dataset-files-into-stac-items-and-stac-assets","position":14},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"The second step is to describe the different files as Items and Assets. This is the most time-consuming step. There are multiple strategies for doing this and it is up to you to decide how to do it. The main consideration should be usability of the data.\n\nFor example:\n\nMicrosoft Planatery Computer groups its Sentinel-2 data into Items which represent individual regions, and each Item has 13 Assets each representing a band - \n\nhttps://​stacindex​.org​/catalogs​/microsoft​-pc​#​/43bjKKcJQfxYaT1ir3Ep6uENfjEoQrjkzhd2​?cp​=​1​&​t​=5 .\n\nThe California Forest Observatory (on Google Earth Engine) groups its data into Items, where each Item represents a specific year, data type and resolution for the whole study area. Each Item has only one Asset ( dataset ) associated with it - \n\nhttps://​stacindex​.org​/catalogs​/forest​-observatory​#​/4dGsSbK8F5jjmhRZYE6kjUMmgWCUKe6J2qqw​?t​=2.\n\nA More complex example from real-data from ESA-funded project: \n\nESA Projects Results Repository, gives the researchers flexibility in terms on how their datasets will be grouped into Items and Assets. You may need to consider that the more Items you have in your Collection, the slower the browsing would be if the user would like to browse through the publicly open STAC Browser. Please have a look at one example, that provides one Sentinel-3 AMPLI Ice Sheet Elevation Collection with around 400 Items complemented by around 360 Assets each.\n\n\nhttps://​eoresults​.esa​.int​/browser​/​#​/external​/eoresults​.esa​.int​/stac​/collections​/sentinel3​-ampli​-ice​-sheet​-elevation\n\nMore general examples about creating STAC catalogs are available here - \n\nhttps://​github​.com​/stac​-utils​/pystac​/tree​/main​/docs​/tutorials.\n\nThe easiest way to generate the required STAC Items is to copy over the metadata directly from your files.\n\n","type":"content","url":"/prr-stac-introduction#id-2-group-your-dataset-files-into-stac-items-and-stac-assets","position":15},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Open Dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl5","url":"/prr-stac-introduction#example-open-dataset","position":16},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Open Dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"\n\n# open dataset\n\n# define relative filepath within the folder structure you want to upload to the PRRs\nfilepath = 'https://data.aviso.altimetry.fr/aviso-gateway/data/indicators/OHC_EEI/4DAtlantic_OHC/OHC_4DATLANTIC_200204_202212_V2-0.nc'\n\nds = xr.open_dataset(filepath + '#mode=bytes')\nds\n\n# helper function to convert numpy arrays to lists\nimport numpy as np\ndef convert_to_json_serialisable(attrs):\n    attrs = attrs.copy()\n    for attr in attrs.keys():\n        if isinstance(attrs[attr], np.ndarray):\n            attrs[attr] = attrs[attr].tolist()\n    return attrs\n\n# sometimes attributes are not json serialisable, so we convert them to JSON serialisable formats\nfor var in ds.data_vars:\n    ds[var].attrs = convert_to_json_serialisable(ds[var].attrs)\n\nfrom xstac import xarray_to_stac\n\n","type":"content","url":"/prr-stac-introduction#example-open-dataset","position":17},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create valid STAC Item from your product (nc)","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl5","url":"/prr-stac-introduction#example-create-valid-stac-item-from-your-product-nc","position":18},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create valid STAC Item from your product (nc)","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"\n\n# Describe the first file following the datacube stac extension standards.\n# All data is extracted from the metadata / data already present in the file we only specify\n# the template and what information is extracted\n\nbbox = [ds['longitude'].values.min(), ds['latitude'].values.min(), ds['longitude'].values.max(), ds['latitude'].values.max(), ]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\ntemplate = {\n\n    \"id\": f\"{collectionid}-{'OHC_4DATLANTIC'.lower()}\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.1.0\",\n    \"description\": ds.attrs['summary'],\n    \"title\": 'OHC 4D Atlantic',\n    \"properties\": {\n            \"history\": ds.attrs['history'],\n            \"source\": ds.attrs['source'],\n            \"comment\": ds.attrs['comment'],\n            \"references\": ds.attrs['references'],\n            \"version\": ds.attrs['version'],\n            \"conventions\": ds.attrs['Conventions'],\n            \"contact\": ds.attrs['contact'],\n            \"start_datetime\": ds.attrs['start_date'] + 'T00:00:00Z',\n            \"end_datetime\": ds.attrs['end_date'] + 'T00:00:00Z',\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": f\"./{collectionid}/OHC_4DATLANTIC_200204_202212_V2-0.nc\",  # or local path\n            \"type\": \"application/x-netcdf\",\n            \"roles\": [\"data\"],\n            \"title\": 'OHC 4D Atlantic'\n        }\n    }\n}\n\n# 3. Generate the STAC Item\nitem = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\" if 'time' in ds.coords else False,\n    x_dimension='longitude',\n    y_dimension='latitude',\n    reference_system=False\n)\n\nitem # Preview created Item\n\n","type":"content","url":"/prr-stac-introduction#example-create-valid-stac-item-from-your-product-nc","position":19},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"3. Add the STAC Item to the STAC Collection"},"type":"lvl2","url":"/prr-stac-introduction#id-3-add-the-stac-item-to-the-stac-collection","position":20},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"3. Add the STAC Item to the STAC Collection"},"content":"Adding the Items to the Collection is a single function call when using a library such as pystac.\n\ncollection.add_item(item)\n\n","type":"content","url":"/prr-stac-introduction#id-3-add-the-stac-item-to-the-stac-collection","position":21},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"4. Save the Collection"},"type":"lvl2","url":"/prr-stac-introduction#id-4-save-the-collection","position":22},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"4. Save the Collection"},"content":"Again this step is a single function call.\n\ncollection.normalize_and_save(\n    root_href='example_4datlantic/', # path to the self-contained folder with STAC Collection\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/prr-stac-introduction#id-4-save-the-collection","position":23},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Congratulations, you have created your first STAC Collection. ","lvl2":"4. Save the Collection"},"type":"lvl5","url":"/prr-stac-introduction#congratulations-you-have-created-your-first-stac-collection","position":24},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Congratulations, you have created your first STAC Collection. ","lvl2":"4. Save the Collection"},"content":"Now, you have your results ready to be ingested into ESA PRR. To request data storage in ESA PRR, contact EarthCODE team at: \n\nearth-code@esa.int and provide following information:\n\nyour project name\n\ntotal size of your dataset\n\nlink to STAC Collection created together with associated Items (e.g. entire example_4datlantic folder) - can be provided as a .zip or link to online repository / GitHub public repository\n\nlink to the datasets (access link to final outcomes of the project or assets)\n\nspecify any restrictions related to the access of your dataset.\n\nin the email, do not forget to CC your ESA TO to acknowledge that the dataset will be imported into PRR.\n\nOnce the email is received, the EarthCODE team will make a request to publish your product into PRR on your behalf (in the future the self-ingestion system will be supported).\n\nOnce the collection is imported you will receive a dedicated URL to your products, which you can use to create the record on Open Science Data Catalogue to make your data discoverable or/and request a DOI for your dataset (at the moment this has to be done by external service of your choice).","type":"content","url":"/prr-stac-introduction#congratulations-you-have-created-your-first-stac-collection","position":25},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset"},"type":"lvl1","url":"/tccas-v2","position":0},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with multiple data files of different types.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project Terrestrial Carbon Community Assimilation System (TCCAS). The focus of TCCAS is the combination of a diverse array of observational data streams with the D&B terrestrial biosphere model into a consistent picture of the terrestrial carbon, water, and energy cycles.\n\n🔗 Check the project website: \n\nTerrestrial Carbon Community Assimilation System (TCCAS) – Website\n\n🛢️ TCCAS Dataset: \n\nTerrestrial Carbon Community Assimilation System (TCCAS) – Data base: Sodankylä and Lapland region","type":"content","url":"/tccas-v2","position":1},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"Acknowledgment"},"type":"lvl3","url":"/tccas-v2#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"Acknowledgment"},"content":"We gratefully acknowledge the Terrestrial Carbon Community Assimilation System (TCCAS) team for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport pandas as pd\n\n","type":"content","url":"/tccas-v2#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl2":"2. Define functions to be reused between regions"},"type":"lvl2","url":"/tccas-v2#id-2-define-functions-to-be-reused-between-regions","position":4},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl2":"2. Define functions to be reused between regions"},"content":"\n\ndef add_insitu_data(collectionid, geometry, bbox, start_time, collection, insitu_href):\n    print(start_time)\n    item = Item(\n        id=f\"{collectionid}-insitu_package\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": 'Insitu package with FloX, VOD and Miscellaneous field datasets related to the TCCAS project. ',\n        }\n    )\n\n    # 3. add an asset (the actual link to the file)\n    item.add_asset(\n                key=f'Insitu package', # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/{insitu_href}',\n                    media_type=\"application/tar+gzip\",\n                    roles=[\"data\"],\n                )\n    )\n\n    item.validate()\n    collection.add_item(item)\n\ndef add_documentation_item(collectionid, geometry, bbox, start_time, collection, ):\n    # add all the documentation under a single item\n    item = Item(\n        id=f\"{collectionid}-documentation\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": 'Documentation for the TCCAS project datasets.',\n        }\n    )\n\n    item.add_asset(\n                key=f'TCCAS user manual.', # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/TCCAS_manual.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    item.add_asset(\n                key=\"Satellite Data Uncertainty analysis Scientific Report\", # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/D7.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    item.add_asset(\n                key=\"Campaign Data User Manual\", # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/D11_CDUM-all_sites.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    collection.add_item(item)\n\n# add an item with multiple model forcing assets\n\ndef create_model_forcing_item(collectionid, geometry, bbox, start_time, collection, model_forcing):\n\n    item = Item(\n        id=f\"{collectionid}-model_forcing\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": ' Regional and Site-level model forcing Data Sets for Sodankylä and Lapland region, part of the TCCAS project.',\n        }\n    )\n\n    for k,v in model_forcing.items():\n        item.add_asset(\n                    key=k, # title can be arbitrary\n                    asset=pystac.Asset(\n                        href=f'./{collectionid}/{v}',\n                        media_type=\"application/x-netcdf\",\n                        roles=[\"data\"],\n                    )\n        )\n\n    item.validate()\n    collection.add_item(item)\n\n\n# some attributes extracted from xarray are not json serialisable and have to be cast to other types.\ndef convert_to_json_serialisable(attrs):\n    attrs = attrs.copy()\n    for attr in attrs.keys():\n        if isinstance(attrs[attr], np.ndarray):\n            attrs[attr] = attrs[attr].tolist()\n        elif str(type(attrs[attr])).__contains__('numpy.int'):\n            attrs[attr] = int(attrs[attr])\n    return attrs\n\ndef create_items_from_data(collectionid, geometry, bbox, collection, root_url, data_files, region):\n\n    for dataset_name, dataset_filepath in data_files.items():\n\n        # 1. open the netcdf file\n        ds = xr.open_dataset(root_url + dataset_filepath + '#mode=bytes')\n\n        if 'time' in ds.coords:\n            start_time = ds['time'][0].values\n            ts = (start_time - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n            start_time = datetime.fromtimestamp(ts)\n\n            end_time = ds['time'][-1].values\n            ts = (end_time - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n            end_time = datetime.fromtimestamp(ts)\n\n        elif 'yymmddHH' in ds.variables:\n            string_date = '-'.join(ds['yymmddHH'][0].values.astype(str)[:3])\n            start_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n            string_date = '-'.join(ds['yymmddHH'][-1].values.astype(str)[:3])\n            end_time = datetime.strptime(string_date, '%Y-%m-%d')\n        else:\n            string_date = '-'.join(ds['yymmddHHMMSS'][0].values.astype(int).astype(str)[:3])\n            start_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n            string_date = '-'.join(ds['yymmddHHMMSS'][0].values.astype(int).astype(str)[:3])\n            end_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n        if 'description' in ds.attrs:\n            description = ds.attrs['description']\n        else:\n            description = f'Dataset with variables related to {dataset_name}.'\n            \n        template = {\n\n            \"id\": f\"{collection.id}-{dataset_name.lower().replace(' ', '_')}\",\n            \"type\": \"Feature\",\n            \"stac_version\": \"1.0.0\",\n            \"properties\": {\n                \"title\": dataset_name,\n                \"description\": description,\n                \"start_datetime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                \"end_datetime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                \"region\": region,\n            },\n            \"geometry\": geometry,\n            \"bbox\": bbox,\n            \"assets\": {\n                \"data\": {\n                    \"href\": f\"./{collectionid}/{dataset_filepath.split('/')[-1]}\",  # or local path\n                    \"type\": \"application/x-netcdf\",\n                    \"roles\": [\"data\"],\n                    \"title\": dataset_name\n                }\n            }\n        }\n\n        # remove numpy values from attrs:\n        for var in ds.variables:\n            ds[var].attrs = convert_to_json_serialisable(ds[var].attrs)\n\n        if 'lon' in ds.coords:\n            x_dim, y_dim = 'lon', 'lat'\n        if 'longitude' in ds.coords:\n            x_dim, y_dim ='longitude', 'latitude'\n        elif 'x' in ds.coords:\n            x_dim, y_dim = 'x', 'y'\n        else:\n            x_dim, y_dim = False, False\n        \n        # 3. Generate the STAC Item\n        item = xarray_to_stac(\n            ds,\n            template,\n            temporal_dimension=\"time\" if 'time' in ds.variables else False,\n            x_dimension=x_dim,\n            y_dimension=y_dim,  \n            reference_system=False\n        )\n\n        # validate and add the STAC Item to the collection\n        item.validate()\n\n        # manually add license after validation since its new\n        item.properties['license'] = ds.attrs['license']\n        collection.add_item(item)\n    \n\n","type":"content","url":"/tccas-v2#id-2-define-functions-to-be-reused-between-regions","position":5},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Sodankylae and Lapland’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-sodankylae-and-lapland-data-and-links","position":6},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Sodankylae and Lapland’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'sodankylae'\ncollectionid = \"tccas-sodankylae\"\nbbox = [18.00, 65.00, 32.00, 69.00]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Fraction of absorbed Photosynthetic Active Radiation Leaf Area Index (JRC-TIP)\": \"/jrc-tip/jrctip_fapar-lai_sodankyla_20110101-20220105.nc\",\n    \"Brightness temperature (SMOS TB)\": \"smos/smos_l3tb/SMOS_L3TB__sodankyla.nc\",\n    \"Soil moisture and Vegetation Optical Depth (SMOS SM and SMOS L-VOD)\": \"smos/smosL2/smosL2_1D_v700_sodankyla_trans.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (Sentinel 5P)\": \"sif/tropomi/Sodankyla_SIF_TROPOMI_final.nc4\",\n    \"Slope (ASCAT Slope)\": \"ascat/local_slope.final/ASCAT_slope_so.nc\",\n    \"Photochemical Reflectance Index (MODIS PRI)\": \"modis/final/PRI_ESTIMATE_SODANKYLA_SINUSOIDAL.nc\",\n    \"Land Surface Temperature (MODIS LST)\": \"modis/final/LST_ESTIMATE_SODANKYLA_SINUSOIDAL.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO-2 SIF)\": \"sif/oco2/Sodankyla_SIF_OCO2_final.nc4\",\n    \"Vegetation Optical Depth (AMSR-2 VOD)\": \"amsr2/final/AMSR2_so.nc\"\n} \n\nmodel_forcing = {\n    \"static-site-level\": \"FI-Sod_staticforcing.nc\",\n    \"time-dependent (ERA5) - site level\": \"FI-Sod_dynforcing-era5_20090101-20211231_with-lwdown.nc\",\n    \"time-dependent (in-situ) - site level\": \"FI-Sod_dynforcing-insitu_20090101-20211231_with-insitu-lwdown.nc\",\n    \"static-regional\": \"sodankyla-region_cgls-pft-crops-redistributed_staticforcing.nc\",\n    \"time-dependent (ERA5) - regional\": \"sodankyla-region_dynforcing_era5_2009-2021.nc\"\n}\n\n\n\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for Lapland and Sodankyla region\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for Sodankylä and Lapland region.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          18.00,\n          65.00,\n          32.00,\n          69.00\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'sodankyla-insitu-package.tgz',\n)\n\ncreate_model_forcing_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    model_forcing)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/tccas-v2#id-2-2-define-sodankylae-and-lapland-data-and-links","position":7},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Netherlands region (including Reusel)’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-netherlands-region-including-reusel-data-and-links","position":8},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Netherlands region (including Reusel)’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'netherlands'\ncollectionid = \"tccas-netherlands\"\nbbox = [4.502, 47.502, 11.477, 51.999]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Vegetation Optical Depth (SMOS)\": \"smos/smosL2/smosL2_1D_v700_reusel_trans.nc\",\n    \"Slope\": \"ascat/local_slope.final/ASCAT_slope_re.nc\",\n    \"Solar Induced Chlorophyll Fluorescence\": \"sif/tropomi/Reusel_SIF_TROPOMI_final.nc4\",\n    \"Soil moisture\": \"smos/smosL2/smosL2_1D_v700_reusel_trans.nc\",\n    \"Brightness temperature\": \"smos/smos_l3tb/SMOS_L3TB__reusel.nc\",\n    \"Vegetation Optical Depth\": \"amsr2/final/AMSR2_re.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO2)\": \"sif/oco2/Reusel_SIF_OCO2_final.nc4\",\n    \"Land Surface Temperature\": \"modis/final/LST_ESTIMATE_REUSEL_SINUSOIDAL.nc\",\n    \"Photochemical Reflectance Index\": \"modis/final/PRI_ESTIMATE_REUSEL_SINUSOIDAL.nc\"\n}\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for the Netherlands region (including Reusel)\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for the Netherlands region, including Reusel.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n         [4.502, 47.502, 11.477, 51.999]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'reusel-insitu-package.tgz',\n)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/tccas-v2#id-2-2-define-netherlands-region-including-reusel-data-and-links","position":9},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Majadas del Tietar and Iberian region’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-majadas-del-tietar-and-iberian-region-data-and-links","position":10},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Majadas del Tietar and Iberian region’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'iberia'\ncollectionid = \"tccas-iberia\"\nbbox = [-8.496, 38.501, -2.505, 42.997]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Vegetation Optical Depth (SMOS)\": \"smos/smosL2/smosL2_1D_v700_lasmajadas_trans.nc\",\n    \"Slope\": \"ascat/local_slope.final/ASCAT_slope_lm.nc\",\n    \"Solar Induced Chlorophyll Fluorescence\": \"sif/tropomi/Majadas_SIF_TROPOMI_final.nc4\",\n    \"Fraction of absorbed Photosynthetic Active Radiation, Leaf Area Index\": \"jrc-tip/jrctip_fapar-lai_majadas_20110101-20220105.nc\",\n    \"Soil moisture\": \"smos/smosL2/smosL2_1D_v700_lasmajadas_trans.nc\",\n    \"Brightness temperature\": \"smos/smos_l3tb/SMOS_L3TB__lasmajadas.nc\",\n    \"Vegetation Optical Depth\": \"amsr2/final/AMSR2_lm.nc\",\n    \"Land Surface Temperature\": \"modis/final/LST_ESTIMATE_MAJADAS_SINUSOIDAL.nc\",\n    \"Photochemical Reflectance Index\": \"modis/final/PRI_ESTIMATE_MAJADAS_SINUSOIDAL.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO2)\": \"sif/oco2/Majadas_SIF_OCO2_final.nc4\"\n}\n\nmodel_forcing = {\n    \"static\": \"ES-LM1_staticforcing.nc\",\n    \"time-dependent (ERA5)\": \"ES-LM1_dynforcing-era5_20090101-20211231_with-lwdown.nc\",\n    \"time-dependent (in-situ)\": \"ES-LM1_dynforcing-insitu_20140401-20220930_with-insitu-lwdown.nc\",\n    \"static-regional\": \"majadas-region_cgls-pft-moli2bare_staticforcing.nc\",\n    \"time-dependent (ERA5)-regional\": \"majadas-region_dynforcing_era5_2009-2021.nc\"\n}\n\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for Iberian region (including Majadas del Tietar)\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for the Iberian region, including Majadas del Tietar.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-8.496, 38.501, -2.505, 42.997]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'lasmajadas-insitu-package.tgz',\n)\n\ncreate_model_forcing_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    model_forcing)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/tccas-v2#id-2-2-define-majadas-del-tietar-and-iberian-region-data-and-links","position":11},{"hierarchy":{"lvl1":"ESA Project Results Repository"},"type":"lvl1","url":"/index-1","position":0},{"hierarchy":{"lvl1":"ESA Project Results Repository"},"content":"The \n\nESA Project Results Repository (PRR) provides long term storage for research outcomes. It provides access to data, workflows, experiments and documentation from ESA Projects organised across Collections, accessible via the \n\nSTAC API. Each Collection contains \n\nSTAC Items, with their related Assets stored within the PRR storage. Scientists/commercial companies can upload data to the PRR via the \n\nEarthCODE and \n\nAPEx projects. Most data in the PRR is open access and anyone can download and use it, subject to the dataset’s particular license.","type":"content","url":"/index-1","position":1},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Uploading data to the PRR"},"type":"lvl2","url":"/index-1#uploading-data-to-the-prr","position":2},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Uploading data to the PRR"},"content":"In order to upload data to the ESA Project Results Repository (PRR) you have to generate a STAC Collection that is associated to your files. The STAC Collection provides metadata about your files and makes them searchable and machine readable. The metadata generation process is organised in four steps process:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the Collection\n\nSave the normalised Collection\n\nSend the data, metadata and some extra information to the EarthCODE team.","type":"content","url":"/index-1#uploading-data-to-the-prr","position":3},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Getting Started with PRR Metadata Creation"},"type":"lvl2","url":"/index-1#getting-started-with-prr-metadata-creation","position":4},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Getting Started with PRR Metadata Creation"},"content":"These notebooks are designed for users who are new to the process of publishing data to the ESA Project Results Repository (PRR). They provide step-by-step guidance on how to generate STAC Collections that meet PRR ingestion requirements.\nWhether you’re working with a single raster file or a large, multi-format dataset, notebooks below will assist you in this process.\n\nGenerating a STAC Collection for the PRR (Introduction) - Detailed explanation on how to create valid metadata to ingest simple raster data file (.nc) into PRR.\n\nGenerating a STAC Collection for the PRR (Multiple file types) - Example how to generate metadata for a more complex dataset with varied data types and formats.\n\nGenerating a STAC Collection for the PRR (Large dataset for multiple regions) - Example focuses on handling large dataset across multiple disjoint regions.\n\nGenerating STAC Collection for the PRR (zarr files) - A guide for generating a collection from zarr files.","type":"content","url":"/index-1#getting-started-with-prr-metadata-creation","position":5},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Accessing and Exploring data products on ESA PRR"},"type":"lvl2","url":"/index-1#accessing-and-exploring-data-products-on-esa-prr","position":6},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Accessing and Exploring data products on ESA PRR"},"content":"This notebook is generated for users willing to explore the ESA PRR repository, by browsing, previewing and/or downloading data published on the PRR.\n\nESA Project Results Repository (PRR) Data Access and Collections Preview - Use this notebook to access, explore, query, and download data from the ESA Project Results Repository (PRR).","type":"content","url":"/index-1#accessing-and-exploring-data-products-on-esa-prr","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project"},"type":"lvl1","url":"/prr-zarr","position":0},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with zarr data. The product has two zarr files, covering different regions, created using Sentinel 1 and Sentinel 2 data respectively.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project Yield Prediction and Estimation from Earth Observation (YIPEEO). The focus of YIPEEO is to improve field-scale crop yield forecasts through the usage of high-resolution remote sensing data and cutting edge scientific methods.\n\n🔗 Check the project website: \n\nYield Prediction and Estimation from Earth Observation (YIPEEO) – Website","type":"content","url":"/prr-zarr","position":1},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl4":"Acknowledgment"},"type":"lvl4","url":"/prr-zarr#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl4":"Acknowledgment"},"content":"We gratefully acknowledge the Yield Prediction and Estimation from Earth Observation (YIPEEO) team for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\n\n","type":"content","url":"/prr-zarr#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"1. Generate the parent collection"},"type":"lvl2","url":"/prr-zarr#id-1-generate-the-parent-collection","position":4},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"1. Generate the parent collection"},"content":"The root STAC Collection provides a general description of all project outputs which will be stored on the PRR.\nThe PRR STAC Collection template enforces some required fields that you need to provide in order to build its valid description. Most of these metadata fields should already be available and can be extracted from your data.\n\n# create the parent collection\ncollectionid = \"yipeeo-cropyields\"\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Yield Prediction and Estimation features from Sentinel1 and Sentinel2 data\",\n  \"description\": \"This dataset contains the processed Sentinel 1 and Sentinel 2 features used for yield rediction  in the Yield Prediction and Estimation from Earth Observation (YIPEEO) project. Sentinel-2 L2A collection is used to compute a set of features based on the provided bands as well as various vegetation indices. Sentinel-1 data for the years 2016-2023 was pre-processed by TUW RS on the Vienna Scientific Cluster using the software SNAP8 and software packages developed by the TUW RS group.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.844270319251073,\n          49.040729923617775,\n          31.01967739451807,\n          52.869947524440924\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2016-01-01T00:00:00Z\",\n          \"2022-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\n","type":"content","url":"/prr-zarr#id-1-generate-the-parent-collection","position":5},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl2","url":"/prr-zarr#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":6},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"The second step is to describe the different files as STAC Items and Assets. Take your time to decide how your data should be categorised to improve usability of the data, and ensure intuitive navigation through different items in the collections. There are multiple strategies for doing this and this tutorial demonstrate one of the possible ways of doing that. Examples of how other ESA projects are doing this are available in the \n\nEarthCODE documentation .","type":"content","url":"/prr-zarr#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.1 Add the Sentinel 1 features to a STAC Item","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl3","url":"/prr-zarr#id-2-1-add-the-sentinel-1-features-to-a-stac-item","position":8},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.1 Add the Sentinel 1 features to a STAC Item","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"\n\nsentinel1_url = 'https://objectstore.eodc.eu:2222/68e13833a1624f43ba2cac01376a18af:ASP_ZARR/S1_out.zarr'\nds = xr.open_zarr(sentinel1_url)\nds\n\nbbox = (\n    float(ds.min_lon.min().values), \n    float(ds.min_lat.min().values), \n    float(ds.max_lon.max().values), \n    float(ds.max_lat.max().values)\n)\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\n\ntemplate = {\n    \"id\": f\"{collectionid}-sentinel1-features\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.0.0\",\n    \"properties\": {\n        \"title\": \"Sentinel-1 Features\",\n        \"description\": 'Sentinel 1 features for crop yield prediction and estimatation from 2015 to 2022. The processing workflow consists of the following steps:\\n1. Apply precise orbit data\\n2. Border-noise removal\\n3. Radiometric calibration\\n4. Radiometric terrain-flattening\\n5. Range-Doppler terrain correction\\nFor steps 4. and 5. the 30 m Copernicus Digital Elevation Model (DEM) was used. To extract time series on field level from the pre-processed Sentinel-1 data, several further processing steps were performed to mitigate the impact of the viewing geometry and undesired objects within or near the fields. In a first step, an incidence angle normalization to 40\\u00b0 was performed. Afterwards, all pixels below a standard deviation of 5dB within one year were filtered out as they are typically stemming from radar shadow pixels or are no crop pixels. Finally, the cross-ratio was calculated by subtracting VV and VH polarized backscatter. ',\n        \"start_datetime\": pd.to_datetime(ds.time.min().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"end_datetime\": pd.to_datetime(ds.time.max().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"license\": \"CC-BY-4.0\",\n        \"platform\": \"sentinel-1\",\n        \"instruments\": [\"c-sar\"],\n        \"created\": datetime.utcnow().isoformat() + \"Z\"\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": \"f'/d/{collectionid}/S1_out.zarr\",  # or local path\n            \"type\": \"application/vnd+zarr\",\n            \"roles\": [\"data\"],\n            \"title\": \"Zarr Store of Sentinel1 Field Stats\"\n        }\n    }\n}\n# 3. Generate the STAC Item\nsentinel1_item = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\",\n    x_dimension=False,\n    y_dimension=False\n)\n\nsentinel1_item.validate()\nsentinel1_item\n\n","type":"content","url":"/prr-zarr#id-2-1-add-the-sentinel-1-features-to-a-stac-item","position":9},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.2 Add the sentinel 2 features","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl3","url":"/prr-zarr#id-2-2-add-the-sentinel-2-features","position":10},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.2 Add the sentinel 2 features","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"\n\nsentinel2_url = 'https://objectstore.eodc.eu:2222/68e13833a1624f43ba2cac01376a18af:ASP_ZARR/S2_out.zarr'\nds = xr.open_dataset(sentinel2_url, engine='zarr')\nds\n\nbbox = (\n    float(ds.min_lon.min().values), \n    float(ds.min_lat.min().values), \n    float(ds.max_lon.max().values), \n    float(ds.max_lat.max().values)\n)\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\n\ntemplate = {\n    \"id\": f\"{collectionid}-sentinel2-features\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.0.0\",\n    \"properties\": {\n        \"title\": \"Sentinel-2 Features\",\n        \"description\": 'Sentinel 2 features based on the provided bands as well as various vegetation indices. The Sentinel-2 L2A data cube is dynamically created by utilising the STAC API. The datacube is pre-filter with scenes of a cloud cover less than 80%. The following features are extracted per field and timestamp: Band Medians and Standard Deviations: B02, B03, B04, B05, B06, B07, B08, B8A, B11, B12; Vegetation indices based on median bands of NDVI, EVI, NDWI, NMDI. An outlier removal was added on a field scale level utilising the SCL band and outlier removal based on 2 x inter quartile range (IQR).',\n        \"start_datetime\": pd.to_datetime(ds.time.min().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"end_datetime\": pd.to_datetime(ds.time.max().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"license\": \"CC-BY-4.0\",\n        \"platform\": \"sentinel-2\",\n        \"instruments\": [\"msi\"],\n        \"created\": datetime.utcnow().isoformat() + \"Z\"\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": \"f'/d/{collectionid}/S2_out.zarr\",  # or local path\n            \"type\": \"application/vnd+zarr\",\n            \"roles\": [\"data\"],\n            \"title\": \"Zarr Store of Sentinel2 Field Stats\"\n        }\n    }\n}\n# 3. Generate the STAC Item\nsentinel2_item = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\",\n    x_dimension=False,\n    y_dimension=False\n)\nsentinel2_item.validate()\nsentinel2_item\n\n","type":"content","url":"/prr-zarr#id-2-2-add-the-sentinel-2-features","position":11},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"3. Add the Items to the collection and Save the metadata as a self-contained collection"},"type":"lvl2","url":"/prr-zarr#id-3-add-the-items-to-the-collection-and-save-the-metadata-as-a-self-contained-collection","position":12},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"3. Add the Items to the collection and Save the metadata as a self-contained collection"},"content":"\n\ncollection.add_items([sentinel1_item, sentinel2_item])\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href='../../data/yippeo_collection/',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/prr-zarr#id-3-add-the-items-to-the-collection-and-save-the-metadata-as-a-self-contained-collection","position":13},{"hierarchy":{"lvl1":"EarthCODE Tutorials"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"EarthCODE Tutorials"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"EarthCODE Tutorials","lvl2":"Welcome to the EarthCODE Tutorials Book!"},"type":"lvl2","url":"/#welcome-to-the-earthcode-tutorials-book","position":2},{"hierarchy":{"lvl1":"EarthCODE Tutorials","lvl2":"Welcome to the EarthCODE Tutorials Book!"},"content":"Here you will find guides and practical tutorials on how to use the various EarthCODE resources including publication process, data access and exploitation and working with the platforms!\n\nLooking how to upload data to the ESA Project Results Repository (PRR)? Start with our \n\nPRR Tutorials to learn how to easily generate STAC Collections direclty from your files and share your datasets.\n\nLooking how to contribute to the Open Science Catalog, Start with our \n\nOpen Science Catalog Tutorials to learn how to add / change content in the metadata catalog, by enriching it with your research outcomes.\n\nLooking for how to use the openEO to create workflows and experiments? Check out our \n\nopenEO Tutorials.","type":"content","url":"/#welcome-to-the-earthcode-tutorials-book","position":3},{"hierarchy":{"lvl1":"Creating a workflow"},"type":"lvl1","url":"/workflow","position":0},{"hierarchy":{"lvl1":"Creating a workflow"},"content":"This notebook showcases how to create a workflow using the \n\nopenEO Python client within the Copernicus Data Space Ecosystem (CDSE). The workflow will be published as a \n\nUser Defined Process (UDP). UDPs allow you to encapsulate your processing workflow, which consists of multiple steps, into reusable openEO building blocks that can be called with a single command. This approach enables you to share and reuse your workflows across different projects and experiments.\n\nIf your workflow is part of a scientific or research experiment, you can publish it in the \n\nEarthCODE Open Science Data Catalog once finalized. This ensures that your workflow is findable and accessible to other users in the scientific community.\n\nIn this example, we will create a workflow to generate a variability map, using Sentinel-2 data products that are available on the Copernicus Data Space Ecosystem. . In a variability map, each image pixel is assigned a specific category, which represents the deviation of the pixel value from the mean pixel value. These maps can be used to implement precision agriculture practices by applying different fertilization strategies for each category. For instance, a farmer might choose to apply more product to areas of the field that exhibit a negative deviation and less to those with positive deviations (enhancing poorer areas), or concentrate the application on regions with positive deviations (focusing on the more productive areas of the field).\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/workflow","position":1},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/workflow#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before creating any processing workflow in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\nimport json\nfrom openeo.rest.udp import build_process_dict\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/workflow#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Defining the workflow parameters"},"type":"lvl2","url":"/workflow#defining-the-workflow-parameters","position":4},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Defining the workflow parameters"},"content":"The first step in creating an openEO workflow is specifying the \n\ninput parameters. These parameters enable users to execute the workflow with their own custom settings, making it adaptable to different datasets and use cases. openEO provides built-in \n\nhelper functions that assist in defining these parameters correctly.\n\nfrom openeo.api.process import Parameter\n\narea_of_interest = Parameter.geojson(name='spatial_extent', description=\"Spatial extent for which to generate the variability map\")\ntime_of_interest = Parameter.date(name='date', description=\"Date for which to generate the variability map\")\n\n","type":"content","url":"/workflow#defining-the-workflow-parameters","position":5},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Implementation of the workflow"},"type":"lvl2","url":"/workflow#implementation-of-the-workflow","position":6},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Implementation of the workflow"},"content":"Next, we will begin implementing the variability map workflow. This involves using the predefined functions in openEO to create a straightforward workflow consisting of the following steps:\n\nSelect the S2 data based on the area_of_interest and time_of_interest parameters.\n\nCalculate the NDVI for the S2 data.\n\nApply an openEO User Defined Function (UDF) to calculate the deviation of each pixel against the mean pixel value of the datacube.\n\n# Step 1. Select the S2 data based on the workflow parameters\ns2_cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent=area_of_interest,\n    temporal_extent=[time_of_interest,time_of_interest],\n)\n    \ns2_masked = s2_cube.mask_polygon(area_of_interest)\n\n# Step 2. Calculate the S2 NDVI\ns2_ndvi = s2_masked.ndvi()\n\n# Step 3. Apply the UDF to calculate the variability map\ncalculate_udf = openeo.UDF.from_file(\"./files/variability_map.py\")\nvarmap_dc = s2_ndvi.reduce_temporal(calculate_udf)\n\nfrom IPython.display import JSON\n\nJSON(varmap_dc.to_json())\n\n","type":"content","url":"/workflow#implementation-of-the-workflow","position":7},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Create an openEO-based workflow"},"type":"lvl2","url":"/workflow#create-an-openeo-based-workflow","position":8},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Create an openEO-based workflow"},"content":"In this next step, we will create our workflow by establishing our openEO User Defined Process (UDP). This action will create a public reference to the workflow we developed in the preceding steps. This can be achieved by using the \n\nsave_user_defined_process function.\n\nNote\n\nThe publication of the UDP contains a public reference to the workflow, which can be shared with others. This allows users to execute the workflow without needing to recreate it from scratch, promoting collaboration and reuse of processing workflows.\n\nworkflow = connection.save_user_defined_process(\n    \"variability_map\",\n    varmap_dc,\n    parameters=[area_of_interest, time_of_interest],\n    public=True\n)\nworkflow\n\nIn the previous step, we created a workflow as a UDP in openEO. We can now use the public URL to share the workflow with others or to execute it in different contexts. The UDP encapsulates the entire processing logic, making it easy to apply the same workflow to different datasets or parameters without needing to redefine the steps each time. In this example, the published UDP is available at the following URL: \n\nVariability Map UDP.\n\n","type":"content","url":"/workflow#create-an-openeo-based-workflow","position":9},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Testing the workflow"},"type":"lvl2","url":"/workflow#testing-the-workflow","position":10},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Testing the workflow"},"content":"After saving the workflow, we can test it by executing the UDP with specific parameters. This step allows us to verify that the workflow operates as expected and produces the desired results. We start by defining the parameters that we want to use for the test run. These parameters will be passed to the UDP when it is executed.\n\nspatial_extent_value = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              5.170043941798298,\n              51.25050990858725\n            ],\n            [\n              5.171035037521989,\n              51.24865722468999\n            ],\n            [\n              5.178521828188366,\n              51.24674578027137\n            ],\n            [\n              5.179084341977159,\n              51.24984764553983\n            ],\n            [\n              5.170043941798298,\n              51.25050990858725\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    }\n  ]\n}\n\ndate_value = \"2025-05-01\"\n\nNext we use our previously created datacube varmap_dc to execute our workflow as an openEO batch job. This step involves submitting the job to the openEO backend, which will process the data according to the defined workflow and parameters. The backend will handle the execution of the workflow and return the results, which can then be analyzed or visualized as needed.\n\npath =  \"./files/varmap_workflow_test.tiff\"\n\nvarmap_test = connection.datacube_from_process(\n    \"variability_map\",\n    spatial_extent=spatial_extent_value,\n    date=date_value,\n)\nvarmap_test.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Workflow Test\", \n    description=\"This is an example of a workflow test containing the calculation of a variability map in Belgium\",\n)\n\nFinally, we can visualize the results of our workflow. This step allows us to see the output of the variability map and assess its quality and relevance for our specific use case. Visualization is a crucial part of the workflow, as it helps in interpreting the results and making informed decisions based on the data processed by our openEO workflow.\n\nvisualise_tif(path)\n\n\n\n","type":"content","url":"/workflow#testing-the-workflow","position":11},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Sharing your workflow"},"type":"lvl2","url":"/workflow#sharing-your-workflow","position":12},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Sharing your workflow"},"content":"Now that our workflow has been created using save_user_defined_process and we’ve confirmed that it works, we can share it with others and the broader community. Using the openEO functions demonstrated before, the workflow is automatically stored on the openEO backend we connected to in the initial steps. The workflow, referred to as a \n\nUser Defined Process (UDP) in openEO terminology, is a JSON-based structure that contains the steps of the workflow, represented as an \n\nopenEO process graph, along with additional metadata such as a description of the workflow parameters.","type":"content","url":"/workflow#sharing-your-workflow","position":13},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Exporting your workflow","lvl2":"Sharing your workflow"},"type":"lvl3","url":"/workflow#exporting-your-workflow","position":14},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Exporting your workflow","lvl2":"Sharing your workflow"},"content":"There are several ways to make your workflow accessible for reuse among peers and within your communities:\n\nShare the public URL with your peersSince we used public=True in our save_user_defined_process, a public URL was automatically added to the workflow definition.\nIn this case, the public URL is:\n\nhttps://​openeo​.dataspace​.copernicus​.eu​/openeo​/1​.1​/processes​/u:6391851f​-9042​-4108​-8b2a​-3dd2e8a9dd0b​/variability​_map\n\nExport the workflow definition to your preferred storageAlternatively, you can also export the workflow and store it in a version-controlled environment like GitHub or your own preferred storage. This gives you full control over its content and version history. In this case, instead of using save_user_defined_process, you can use build_process_dict to create a dictionary representation of the workflow, which can then be written to a file. However, if you want others to reuse your workflow, make sure the file is accessible via a public URL. This is necessary for the openEO backend to retrieve and execute the workflow definition.\n\nspec = build_process_dict(\n    process_id=\"variability_map\",\n    process_graph=varmap_dc,\n    parameters=[area_of_interest, time_of_interest],\n)\n\nwith open(\"files/variability_map_workflow.json\", \"w\") as f:\n    json.dump(spec, f, indent=2)\n\nTip\n\nDuring the development phase, the public URL of the workflow can be used to quickly share early versions with your colleagues. However, once the workflow reaches a level of maturity suitable for broader community use, we recommend storing the workflow definition on GitHub. Since version control within openEO is limited, using open-source tools like GitHub allows for better management of release procedures and workflow updates.\n\n","type":"content","url":"/workflow#exporting-your-workflow","position":15},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Sharing your workflow","lvl2":"Sharing your workflow"},"type":"lvl3","url":"/workflow#sharing-your-workflow-1","position":16},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Sharing your workflow","lvl2":"Sharing your workflow"},"content":"\n\nOnce you have a public reference to your workflow, either through the openEO backend or a public URL pointing to a definition stored on GitHub or another platform, you can share it in various ways to enable others to execute it, as shown in our \n\nCreating an experiment example. There are many different ways to share your workflow:\n\nDirect sharing with peers and communities\n\nPublishing your workflow on the \n\nEarthCODE Open Science Catalogue, as demonstrated in our \n\npublication example\n\nPublishing on platform marketplaces, such as the \n\nCopernicus Data Space Ecosystem Algorithm Plaza.","type":"content","url":"/workflow#sharing-your-workflow-1","position":17},{"hierarchy":{"lvl1":"Creating an experiment"},"type":"lvl1","url":"/experiment","position":0},{"hierarchy":{"lvl1":"Creating an experiment"},"content":"With our workflow successfully created as an openEO User Defined Process (UDP) in our \n\nprevious tutorial, we can now set up an experiment. This involves executing the workflow with a predefined set of input parameters. By specifying parameters such as the area of interest and time range, we can tailor the experiment to generate meaningful results.\n\nRunning the experiment will produce output products, which will be displayed at the end of the notebook. These results can then be further analyzed, shared with the scientific community, or published in the \n\nEarthCODE Open Science Catalogue to promote reproducibility and collaboration.\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/experiment","position":1},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/experiment#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before creating the experiment in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/experiment#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Setting up the experiment"},"type":"lvl2","url":"/experiment#setting-up-the-experiment","position":4},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Setting up the experiment"},"content":"In this step, we will set up the experiment by defining the parameters for the variability map calculation. This includes specifying the area of interest, the time range, and any other relevant parameters that are required for the processing workflow. The experiment will be executed using the UDP from \n\nprevious tutorial which yielded a public URL that can be used to execute the workflow. We will use this URL to set up our experiment, ensuring that the processing steps are applied correctly to the specified input data.\n\nNote\n\nAs mentioned in our \n\nworkflow tutorial, the workflow URL can either refer to a definition stored on the openEO backend—created using save_user_defined_process with public=True, or to a public URL pointing to a JSON file hosted on GitHub or another platform.\n\nworkflow_url = \"https://openeo.dataspace.copernicus.eu/openeo/1.1/processes/u:6391851f-9042-4108-8b2a-3dd2e8a9dd0b/variability_map\" \n\nspatial_extent_value = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              5.170043941798298,\n              51.25050990858725\n            ],\n            [\n              5.171035037521989,\n              51.24865722468999\n            ],\n            [\n              5.178521828188366,\n              51.24674578027137\n            ],\n            [\n              5.179084341977159,\n              51.24984764553983\n            ],\n            [\n              5.170043941798298,\n              51.25050990858725\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    }\n  ]\n}\n\ndate_value = \"2025-05-01\"\n\nWe can now execute our experiment using the datacube_from_process function and the information from the workflow we created:\n\nprocess_id: the ID that we have assigned to our workflow\n\nnamespace: the public URL of the workflow, which is provided as output of the save_user_defined_process call\n\n**kwargs: the parameters of the workflow. If a parameter is not specified, its default value will be used\n\nvarmap_experiment = connection.datacube_from_process(\n    process_id=\"variability_map\",\n    namespace=workflow_url,\n    spatial_extent=spatial_extent_value,\n    date=date_value\n)\n\nNow that we have created our experiment, we can explore how it is defined within openEO.\n\nfrom IPython.display import JSON\n\nJSON(varmap_experiment.to_json())\n\nNow that we have created our experiment, we can execute it by submitting it as an openEO batch job to the backend. This will trigger the processing workflow defined in our UDP, applying it to the specified input data. The output will be saved to a specified path, which we can then visualize or analyze further.\n\npath =  \"./files/varmap_experiment.tiff\"\nvarmap_experiment.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Experiment\", \n    description=\"This is an example experiment from CDSE containing the calculation of a variability map in Belgium\",\n)\n\nvisualise_tif(path)","type":"content","url":"/experiment#setting-up-the-experiment","position":5},{"hierarchy":{"lvl1":"Publishing an experiment to EarthCODE"},"type":"lvl1","url":"/publication","position":0},{"hierarchy":{"lvl1":"Publishing an experiment to EarthCODE"},"content":"In this guide we will explore the step needed to publish our \n\npreviously created experiment to the EarthCODE Open Science Catalogue (OSC). To support this process, a dedicated tool, called the \n\nopenEO Publishing tool, has been created, which will guide you through the process of publishing the experiment. The publishing tool will create a GitHub pull request on the OSC with all the necessary information required to publish the experiment, including details about the product and the workflow. After approval of the pull request by the EarthCODE team, your experiment, its corresponding workflow and the resulting output products will become available in the catalogue for users to discover and reuse.\n\nTo start the publishing process, you can access the openEO Publishing tool at \n\npublish​.earthcode​.vito​.be. The tool will prompt you to log in using your GitHub account, which is necessary to create the pull request in the EarthCODE OSC repository.\n\nOnce logged in, you will need to select one of the supported openEO backends from which you want to publish your experiment. The tool currently supports the following backends:\n\nCopernicus Data Space Ecosystem (CDSE) openEO Federation\n\nClick the Next button to proceed after selecting the backend. The openEO Publishing tool will then connect to the selected backend. If you are not already authenticated, you will be prompted to log in to the backend using your credentials. Clicking the Authenticate button will redirect you to the selected backend’s authentication page, where you can log in with your credentials. Once authenticated, you will be redirected back to the openEO Publishing tool.\nThis step is necessary to retrieve the list of experiments that you have created on the selected backend.\n\nNow that you are authenticated, you can select the experiment you want to publish from the list of available jobs. The tool will display all the processing jobs associated with your account on the selected backend. You can select one or multiple experiments to publish. Click the Next button to proceed to the next step.\n\nHint\n\nThe list of jobs only includes those that have been executed successfully. If you do not see your experiment in the list, make sure it has been executed and completed without errors.\n\nAfter selecting one or more jobs, you will be prompted to provide additional information about the experiment. First you will need to select if you want to publish the full experiment, only the workflow, or only the output products. The full experiment includes both the workflow and the output products, while selecting only the workflow or output products will publish only those components.\n\nBased on your selection, the tool will show a dedicated form to fill in the necessary details for the publication. Some of the fields are automatically filled in based on the experiment metadata retrieved from the platform, while others require manual input. The following sections describe the fields you need to fill in for each publication type.\n\nFor experiments:\n\nProject: The project under which the experiment is published. This should be a valid \n\nEarthCODE project name.\n\nID: A unique identifier for the experiment.\n\nTitle: A descriptive title for your experiment.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nLicense (optional): The license under which the experiment is published. This should be a valid license identifier.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nExperiment Process Graph: Selection of the openEO process graph that represents your experiment. You can either choose to take the process graph from the selected job or refer to a public process graph URL that you have created previously.\n\nFor workflows:\n\nProject: The project under which the workflow is published. This should be a valid \n\nEarthCODE project name\n\nID: A unique identifier for the workflow.\n\nURL: The public URL of the openEO User Defined Process hat represents your workflow. This should be a valid URL pointing to a public process graph. See our tutorial on \n\ncreating a workflow for more information on how to create a User Defined Process and get its URL.\n\nTitle: A descriptive title for your workflow.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nNote\n\nWhenever you publish a workflow as part of an experiment, some of the above fields are covered by the experiment metadata. Therefore, you only need to fill in the fields that are not already provided by the experiment. Additionally in the case of an experiment, you can also choose to select an existing workflows from the list of workflows that are already available on the EarthCODE OSC.\n\nFor products:\n\nProject: The project under which the product is published. This should be a valid \n\nEarthCODE project name\n\nID: A unique identifier for the product.\n\nTitle: A descriptive title for your product.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nAssets: A list of assets that part of the product. By default the result of the openEO job is automatically added to the list of assets. You can add additional assets by providing their URLs and a corresponding name. The assets should be publicly accessible URLs pointing to the output files generated by the experiment.\n\nNote\n\nWhenever you publish a product as part of an experiment, some of the above fields are covered by the experiment metadata. Therefore, you only need to fill in the fields that are not already provided by the experiment.\n\nAfter filling in the necessary details, you can click the Next button to proceed to the final step. The openEO Publishing tool will then create a pull request on the EarthCODE OSC repository with all the information you provided. You will be able to access the pull request by clicking the link provided in the tool and track its status either through that link or on the \n\nGitHub pull request page. The pull request will contain all the necessary files and metadata required to publish your experiment, workflow, or product in the EarthCODE OSC.\n\nYour experiment, workflow, or product will be now reviewed by the EarthCODE team. Once approved, it will be published in the EarthCODE Open Science Catalogue, making it available for other users to discover and reuse.","type":"content","url":"/publication","position":1},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue"},"type":"lvl1","url":"/reproduce","position":0},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue"},"content":"In this guide, we will use your newly published experiment from \n\nprevious tutorial and reproduce it. This process will demonstrate how to leverage the openEO federation and EarthCODE tools to reproduce experiments, ensuring transparency and reproducibility in scientific research, reinforcing the principles of Open Science.\n\nNote\n\nIn this guide, we will use a link to a testing experiment from a GitHub repository. The links that we use point to the EarthCODE Open Science Catalogue GitHub repository. This link may not work in the future, as it is a test experiment. However, you can use the same steps to reproduce any experiments published to the EarthCODE Open Science Catalogue.\n\nexperiment_url = \"https://raw.githubusercontent.com/ESA-EarthCODE/open-science-catalog-metadata-testing/c9ad31fd63330818e7895faf10f5104d9a101c01/experiments/cdse_federation_-_variability_map_experiment/process_graph.json\"\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/reproduce","position":1},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/reproduce#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before executing our published experiment in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/reproduce#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Using the openEO client"},"type":"lvl2","url":"/reproduce#using-the-openeo-client","position":4},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Using the openEO client"},"content":"The first option to execute an existing experiment is through the openEO Python client. By using openEO’s datacube_from_json, you can import the published experiment from the OSC into openEO.\n\nexperiment = connection.datacube_from_json(experiment_url)\n\nfrom IPython.display import JSON\n\nJSON(experiment.to_json())\n\npath =  \"./files/varmap_experiment_reproduce.tiff\"\nexperiment.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Experiment (reproduce)\"\n)\n\nvisualise_tif(path)\n\nIn this next step we will verify if the output products of the original experiment, created in the \n\nprevious tutorial are the same as the output products of the reproduced experiment. This is done by comparing the output products of the original experiment with the output products of the reproduced experiment.\n\nimport numpy as np\n\n\ndef compare_geotiff(file1: str, file2: str) -> bool:\n    with rasterio.open(file1) as src1, rasterio.open(file2) as src2:\n        # Check if dimensions match\n        if src1.width != src2.width or src1.height != src2.height:\n            return False\n        \n        # Check if coordinate reference systems match\n        if src1.crs != src2.crs:\n            return False\n        \n        # Check if transform properties match\n        if src1.transform != src2.transform:\n            return False\n        \n        # Compare pixel values\n        data1 = src1.read()\n        data2 = src2.read()\n        if not np.array_equal(data1, data2):\n            return False\n\n    return True\n\n# Call the function with the specified files\nare_equal = compare_geotiff(\"./files/varmap_experiment.tiff\", \"./files/varmap_experiment_reproduce.tiff\")\nprint(f\"Are the GeoTIFF files equal? {are_equal}\")\n\n","type":"content","url":"/reproduce#using-the-openeo-client","position":5},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"type":"lvl3","url":"/reproduce#using-the-openeo-web-editor","position":6},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"content":"","type":"content","url":"/reproduce#using-the-openeo-web-editor","position":7},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl4":"Using the experiment URL","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"type":"lvl4","url":"/reproduce#using-the-experiment-url","position":8},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl4":"Using the experiment URL","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"content":"Alternatively, you can also open the experiment through the \n\nopenEO Web Editor using the experiment URL. This can be done by clicking the Import process from an external source button located in the top navigation bar.\n\nNext, you can past the experiment URL into the window to fetch the experiment and open it in the editor.\n\nFinally, click the Create Batch Job button to initiate the execution of the experiment.","type":"content","url":"/reproduce#using-the-experiment-url","position":9},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the execution link from the published experiment","lvl2":"Using the openEO client"},"type":"lvl3","url":"/reproduce#using-the-execution-link-from-the-published-experiment","position":10},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the execution link from the published experiment","lvl2":"Using the openEO client"},"content":"Warning\n\nThis feature is still in development and is not yet fully integrated in the EarthCODE Open Science Catalogue. Therefore, the retrieval of the execution link requires you to access the experiment through GitHub. In future release, this link will be available directly from the EarthCODE Open Science Catalogue.\n\nWhenever an experiment or workflow is published to the EarthCODE Open Science Catalogue, an execution link is generated. This link can be used to execute the experiment or workflow directly in the openEO Web Editor. As the integration of this execution link in the EarthCODE OSC is still in development, for now you can access the execution link through the GitHub repository of the EarthCODE Open Science Catalogue. The execution link is available in the links field of the experiment metadata. For example: \n\nhttps://​editor​.openeo​.org​/​?process​=​https://​raw​.githubusercontent​.com​/ESA​-EarthCODE​/open​-science​-catalog​-metadata​-testing​/c9ad31fd63330818e7895faf10f5104d9a101c01​/experiments​/cdse​_federation​_​-​_variability​_map​_experiment​/process​_graph​.json​&​server​=​https://​openeofed​.dataspace​.copernicus​.eu​/openeo.\n\nNavigating to this link will open the experiment in the openEO Web Editor, allowing you to execute it directly.","type":"content","url":"/reproduce#using-the-execution-link-from-the-published-experiment","position":11},{"hierarchy":{"lvl1":"openEO"},"type":"lvl1","url":"/index-3","position":0},{"hierarchy":{"lvl1":"openEO"},"content":"openEO is an open standard designed to simplify access to and processing of EO data. It provides a unified API that abstracts away differences between cloud platforms, enabling scientists, developers, and analysts to build workflows and experiments in a consistent and portable way. Instead of learning the specifics of each platform or provider, users can work with a common interface to query datasets, chain processing steps, and run analyses at scale.\n\nFrom a technical perspective, openEO defines a set of RESTful APIs and \n\nclient libraries in popular programming languages such as Python, R, and JavaScript. These libraries allow users to connect to different backends, discover available EO collections and processing capabilities, and construct workflows as \n\nprocess graphs. These process graphs describe the sequence of operations in a platform-independent format, making them reusable across environments. Whether the goal is atmospheric correction, time-series analysis, or machine learning integration, openEO provides the building blocks for scalable EO data science.\n\nFrom a user perspective, openEO lowers the barrier to experimentation and collaboration. Users can start with small exploratory analyses in notebooks, gradually expand workflows into more complex experiments, and finally scale them up on powerful cloud infrastructures, all without changing the underlying logic. This portability fosters reproducibility and ensures that workflows developed once can be reused, adapted, or shared easily across projects.\n\nThe notebook tutorials provided in this section illustrate how to use openEO for common scientific tasks: from creating workflow using user defined processes and open source input data, incorporating it into scientific experiment, to publishing experiment with the workflow in EarthCODE. At the end reproducibility of published experiment is also demonstrated.","type":"content","url":"/index-3","position":1},{"hierarchy":{"lvl1":"openEO","lvl2":"openEO Tutorials"},"type":"lvl2","url":"/index-3#openeo-tutorials","position":2},{"hierarchy":{"lvl1":"openEO","lvl2":"openEO Tutorials"},"content":"Creating an openEO workflow: This guide demonstrates how to create a basic openEO workflow, executing a simple processing task and publishing it as workflow that can be shared and reused.\n\nCreating an experiment: This guide shows how to set up an experiment using the previously created workflow, defining parameters such as area of interest and time range, and executing the workflow to generate output products.\n\nPublishing an experiment to EarthCODE: This guide explains how to publish the created experiment to the EarthCODE Open Science Catalogue (OSC) using the openEO Publishing tool, making it discoverable and reusable by the scientific community.\n\nReproducing an experiment: This guide illustrates how to reproduce the published experiment using openEO, verifying the output products against the original experiment to ensure consistency and correctness.","type":"content","url":"/index-3#openeo-tutorials","position":3}]}