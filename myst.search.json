{"version":"1","records":[{"hierarchy":{"lvl1":"Open Science Catalog Editor"},"type":"lvl1","url":"/git-clerk-example","position":0},{"hierarchy":{"lvl1":"Open Science Catalog Editor"},"content":"A tutorial on how to use the Open Science Catalog Editor, A visual interface to the Open Science Catalog, to upload data.\n\nFirst log in to  \n\nhttps://​workspace​.earthcode​.eox​.at/\n\nAccept the required permissions, so that git-clerk can open PR requests from your github account in the OSC repository.\n\nClick on the Open Science Catalog Editor.","type":"content","url":"/git-clerk-example","position":1},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"Starting with the OSC Editor"},"type":"lvl2","url":"/git-clerk-example#starting-with-the-osc-editor","position":2},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"Starting with the OSC Editor"},"content":"When you would like to add new entries, please start a new session (give it a meaningful name) and start with Adding New Project in the main page.\nThen you can proceed by providing the information directly in the form.\n\nPlease see the steps with an example below:\n\nYou can find here a description of required metadata for projects/products etc.: \n\nhttps://esa-earthcode.github.io/documentation/Technical Documentation/Open Science Catalog/Open Science Catalog Overview\n\nOnce you have all the fields filled in you can submit the project and products metadata for review which will automatically open pull request, which will be checked and merged by our team.","type":"content","url":"/git-clerk-example#starting-with-the-osc-editor","position":3},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"In case of erorrs"},"type":"lvl2","url":"/git-clerk-example#in-case-of-erorrs","position":4},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"In case of erorrs"},"content":"The tool is basically a user interface to create a github pull request on your behalf. You can view the results of your entry here: \n\nhttps://​github​.com​/ESA​-EarthCODE​/open​-science​-catalog​-metadata​/pulls. Whenever you create a pull request or branch we run an automatic validation to ensure your data conforms to the OSC schema.\n\nIf the validation has failed you can manually inspect the errors. If you click on the failed validation link (Validate / check (ubuntu-latest) ...) you can see the reasons for blocking the PR submissions.\n\n\nFor example, your errors might look like this:\n\n\nIn this case, the problems come from a missing:\n\nTemporal extent specification\n\nMissing contracts.\n\nMissing links to your website and the eo4society page for the project.\n\nYou can ignore the missing extensions/parent/root link/ errors those should be resolved automatically.","type":"content","url":"/git-clerk-example#in-case-of-erorrs","position":5},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"You can always ask the EarthCODE team for support if needed!"},"type":"lvl2","url":"/git-clerk-example#you-can-always-ask-the-earthcode-team-for-support-if-needed","position":6},{"hierarchy":{"lvl1":"Open Science Catalog Editor","lvl2":"You can always ask the EarthCODE team for support if needed!"},"content":"","type":"content","url":"/git-clerk-example#you-can-always-ask-the-earthcode-team-for-support-if-needed","position":7},{"hierarchy":{"lvl1":"Open Science Catalog"},"type":"lvl1","url":"/index-2","position":0},{"hierarchy":{"lvl1":"Open Science Catalog"},"content":"The \n\nOpen Science Catalog (OSC) is a key component of the ESA EO Open Science framework. It is a publicly available web-based application designed to provide easy access to scientific resources including geoscience products, workflows, experiments and documentation from activities and projects funded by ESA under the EO Programme.\n\nThere are three different ways and several tools to contribute to Open Science Catalog:","type":"content","url":"/index-2","position":1},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"1: Use a Visual GUI Interface (No coding required)"},"type":"lvl2","url":"/index-2#id-1-use-a-visual-gui-interface-no-coding-required","position":2},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"1: Use a Visual GUI Interface (No coding required)"},"content":"Git Clerk - A guide for using the Git Clerk tool which is a user interface for automatically creating product entries and creating a Pull Request in the OSC GitHub Repo.","type":"content","url":"/index-2#id-1-use-a-visual-gui-interface-no-coding-required","position":3},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"2: Manually contribution (For users familiar with Git)"},"type":"lvl2","url":"/index-2#id-2-manually-contribution-for-users-familiar-with-git","position":4},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"2: Manually contribution (For users familiar with Git)"},"content":"Direct editing metadata in JSON - A guide for manually creating Open Science Catalog entries by creating JSON files with metadata.\n\nGenerating OSC files with pystac - A guide for creating Open Science Catalog entries in more automated way by implementing pystac. Require familiarity with Python.","type":"content","url":"/index-2#id-2-manually-contribution-for-users-familiar-with-git","position":5},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"3: Using one of the platform tools (For EarthCODE integrated platforms users)"},"type":"lvl2","url":"/index-2#id-3-using-one-of-the-platform-tools-for-earthcode-integrated-platforms-users","position":6},{"hierarchy":{"lvl1":"Open Science Catalog","lvl2":"3: Using one of the platform tools (For EarthCODE integrated platforms users)"},"content":"DeepCode - An example using DeepCode: a library for automatically generating product entries for DeepESDL datasets.\n\nopenEO Publishing Tool - A platform agnostic tool to publish workflows, experiments and products created on openEO-based platforms. Learn more in our dedicated \n\npublication guide.","type":"content","url":"/index-2#id-3-using-one-of-the-platform-tools-for-earthcode-integrated-platforms-users","position":7},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)"},"type":"lvl1","url":"/osc-pr-manual","position":0},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)"},"content":"The \n\nOpen Science Catalog (OSC) is a key component of the ESA EO Open Science framework. It is a publicly available web-based application designed to provide easy access to scientific resources including geoscience products, workflows, experiments and documentation from activities and projects funded by ESA under the EO Programme.\n\nThe Open Science Catalog is built on the Spatio Temporal Asset Catalog (STAC), which is a standardised format for describing geospatial data. Throught the open source STAC browser, the catalog allows users to browse and explore interlinked elements such as themes, variables, EO missions, projects, products, workflows, and experiments, all described using STAC-compliant JSON files. This schema ensures that these can be easily reused by other scientists and automated workflowss and correclty displayed in the web browser. Data, workflows, and experiments are documented in the catalogue primarily through enriched metadata and direct links to the corresponding research outcomes. The physical location of these resources is typically indicated via the Project Results Repository or other secure external repositories. Further details on the OSC format can be found \n\nhere.","type":"content","url":"/osc-pr-manual","position":1},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Adding information to the OSC"},"type":"lvl2","url":"/osc-pr-manual#adding-information-to-the-osc","position":2},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Adding information to the OSC"},"content":"There are three ways to add information to the OSC.\n\nManually opening a pull request (this tutorial)\n\nUsing the GUI editor.\n\nUsing one of the platform specific tools\n\nThis notebook describes how you can add information to the OSC by manually creating and editting json files that describe STAC Collections and Catalogs. The steps to add information in this way are:\n\nFork the repository\n\nAdd the information about project/product/workflow/variables in STAC json format.\n\nOpen a PR to merge the new information into the OSC.\n\nIn general most of the information that you need, is already in your data or project documentation, so you will NOT need to generate anything new. All information that you provide will be automatically validated and manually verified by an EarthCODE team member. Therefore, you can use the automatic validation from the CI to make the appropriate changes to the format or information you provide.","type":"content","url":"/osc-pr-manual#adding-information-to-the-osc","position":3},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"1. Forking the repository","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-1-forking-the-repository","position":4},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"1. Forking the repository","lvl2":"Adding information to the OSC"},"content":"Since the OSC metadata is fully hosted on GitHub. Use your personal GitHub account to cotnribute to the catalog. If you do not have an account, you need to setup a new GitHub account: \n\nhttps://​docs​.github​.com​/en​/get​-started​/start​-your​-journey​/creating​-an​-account​-on​-github.\n\nTo contribute your research outputs, you need to create valid STAC objects and commit them to the \n\nopen-science-catalog-metadata repository on GitHub.\nThe first step in this process is to fork the open-science-catalog-metadata repository, that will create your own copy of the Open Science Catalog. ( More information about how to do this in GitHub is available here: \n\nhttps://​docs​.github​.com​/en​/pull​-requests​/collaborating​-with​-pull​-requests​/working​-with​-forks​/fork​-a​-repo )\n\nOnce you have a OSC copy, you should have a look at the folder structure and information for an existing Item of the same type as the one you want to add - product, project, workflow, variable etc. This will give you an idea of the required information for a valid STAC Object. These STAC objects, stored as JSON files, are be automatically processed and rendered in the catalog viewer.\n\n","type":"content","url":"/osc-pr-manual#id-1-forking-the-repository","position":5},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-2-add-the-information-about-project-product-workflow-experiments-variables","position":6},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"After you forked the repository, you can start adding the required information. \n\nThis document explains the Open Science Catalog Extension to the SpatioTemporal Asset Catalog (STAC) specification. There are different requirements depending on the catalog entry you are trying to add.Sometimes its easier to copy the folder of existing project/product/workflow, rename it and start changing its information.\nFor example, copying the contents of this folder products/sentinel3-ampli-ice-sheet-elevation/, renaming it to products/New_Project_Name/ and editing its values.\n\n\n","type":"content","url":"/osc-pr-manual#id-2-add-the-information-about-project-product-workflow-experiments-variables","position":7},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.1 Add new Project","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-1-add-new-project","position":8},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.1 Add new Project","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Projects are the containers that have the top level information about your work. It is the first type of information you should provide. Typically an OSC project corresponds to a project financed by the European Space Agency - Earth Observation programme. Before creating new project, check if your project is not already on the \n\nlist of onboarded projects. In such case you can use your project entry and only update it where needed.\n\nField\n\nDescription\n\nSTAC representation\n\nProject_ID\n\nNumeric identifier\n\n\n\nStatus\n\n“ongoing” or “completed”\n\nosc:status property\n\nProject_Name\n\nName\n\ntitle property\n\nShort_Description\n\n\n\ndescription property\n\nWebsite\n\n\n\nlink\n\nEo4Society_link\n\n\n\nlink\n\nConsortium\n\n\n\ncontacts[].name property\n\nStart_Date_Project\n\n\n\nextent.temporal[] property\n\nEnd_Date_Project\n\n\n\nextent.temporal[] property\n\nTO\n\n\n\ncontacts[].name property\n\nTO_E-mail\n\n\n\ncontacts[].emails[].value property\n\nTheme1 - Theme6\n\nTheme identifiers\n\nosc:themes property\n\nMetadata of each project is stored in a folder named after their unique id (collectionid). Each folder has one file - collection.json that has all the project information (metadata). Have a look at the structure of the Project entry below (with example values filled in):{\n    'type': 'Collection', // This is the STAC type specification. You dont need to change this\n    'stac_version': '1.1.0',  // This is the STAC version specification. You dont need to change this\n    'id': 'worldcereal2', // This is your project id. Please make sure to use unique id name for your project! The parent folder of the collection.json should have the same name as this id (not displayed in the browser).\n    'title': 'WorldCereal2', // Title of your project. Official acronym of the project may be used as well (this will be displayed to public)\n    'description': 'WorldCereal is an ESA initiative that provides global '\n                'cropland and crop type maps at 10-meter resolution, offering '\n                'seasonally updated data on temporary crops, croptypes (maize, '\n                'winter cereals and spring cereals), and irrigation.', // A short, but meaningful description of your project.\n    'links': [  // links to related elements of the catalog. The first two links should always be present and are always the same.\n        {'href': '../../catalog.json',\n        'rel': 'root',\n        'title': 'Open Science Catalog',\n        'type': 'application/json'},\n        {'href': '../catalog.json',\n        'rel': 'parent',\n        'title': 'Projects',\n        'type': 'application/json'},\n        // The next two links are external links to project websites.  These are mandatory and you have to adapt them to your project.\n        {'href': 'https://esa-worldcereal.org/en', # your dedicated project page\n            'rel': 'via',\n            'title': 'Website'},\n           {'href': 'https://eo4society.esa.int/projects/worldcereal-global-crop-monitoring-at-field-scale/', #link to the project page on EO4Society website\n            'rel': 'via',\n            'title': 'EO4Society Link'},\n        // The next link is a link to the themes specified in the themes field below. It is mandatory to have a link to all themes specified in the themes array\n        {'href': '../../themes/land/catalog.json',  #related theme of the project\n            'rel': 'related',\n            'title': 'Theme: Land',\n            'type': 'application/json'}\n    ],\n\n    'themes': [ // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having at least one theme is mandatory. Check available themes here: https://opensciencedata.esa.int/themes/catalog\n        {'concepts': [{'id': 'land'}], \n        'scheme': 'https://github.com/stac-extensions/osc#theme'}\n    ],\n\n    'stac_extensions': [ // which schemas is the project information validated against. Typically you would not change these.\n        'https://stac-extensions.github.io/osc/v1.0.0/schema.json', \n        'https://stac-extensions.github.io/themes/v1.0.0/schema.json',\n        'https://stac-extensions.github.io/contacts/v0.1.1/schema.json'\n        ]\n    'osc:status': 'completed', // status of the project - Select from: completed, ongoing, scheduled\n    'osc:type': 'project', // Type of OSC STAC collection, for projects should always be project\n    'updated': '2025-07-14T17:03:29Z', // when was last update made\n    'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]}, // The study area of the project and its planned duration.\n                'temporal': {'interval': [['2021-01-01T00:00:00Z',\n                                        '2021-12-31T23:59:59Z']]}}\n    'license': 'proprietary' // Top level license of project outcomes. Should be one of https://github.com/ESA-EarthCODE/open-science-catalog-validation/blob/main/schemas/license.json\n\n    // list of consortium members working on the project and contact to ESA TO following the project. This field is required.\n    'contacts': [{'emails': [{'value': 'Zoltan.Szantoi@esa.int'}],\n               'name': 'Zoltan Szantoi',\n               'roles': ['technical_officer']},\n              {'name': 'VITO Remote Sensing', 'roles': ['consortium_member']}\n \n }\n\n\nIn addition to specifying the links within the project collection.json entry (created above), you should also add an entry in the parent catalog, listing all projects to be correclty rendered into STAC Browser. Once done it is required to add the following link (as provided below) to: projects/catalog.json . \nAdd this links array into the project/catalog.json just after the last project entry. Edit the catalog.json direclty by copy-and paste the followinf link (updated according to the data from your collection.json){\n    'rel':'child', \n    'target: './{project_id}/collection.json', // use the collectionid of the project\n    'media_type': 'application/json',\n    'title': '{project_title}'   // title of th project as described in the collection.json file created before. \n}\n\n","type":"content","url":"/osc-pr-manual#id-2-1-add-new-project","position":9},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.2 Add new Product","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-2-add-new-product","position":10},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.2 Add new Product","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Products represent the outputs of you projects and typically reference datasets. Similarly to Projects, they are STAC items and follow similar structure, with some additional fields, improving their findability.\n\nField\n\nDescription\n\nSTAC representation\n\nID\n\nNumeric identifier\n\n\n\nStatus\n\n“ongoing” or “completed”\n\nosc:status property\n\nProject\n\nThe project identifier\n\nosc:project property, collection link\n\nWebsite\n\n\n\nlink\n\nProduct\n\nName\n\nlink\n\nShort_Name\n\n\n\nidentifier\n\nDescription\n\n\n\ndescription property\n\nAccess\n\nURL\n\nlink\n\nDocumentation\n\nURL\n\nlink\n\nVersion\n\n\n\nversion property\n\nDOI\n\nDigital Object Identifier\n\nsci:doi property and cite-as link\n\nVariable\n\nVariable identifier\n\ncollection link\n\nStart\n\n\n\nextent.temporal[]\n\nEnd\n\n\n\nextent.temporal[]\n\nRegion\n\n\n\nosc:region property\n\nPolygon\n\n\n\ngeometry\n\nReleased\n\n\n\ncreated property\n\nTheme1 - Theme6\n\nTheme identifiers\n\nosc:themes property\n\nEO_Missions\n\nSemi-colon separated list of missions\n\nosc:missions property\n\nStandard_Name\n\n\n\ncf:parameter.name property{\n 'type': 'Collection', // This is the STAC type specification. You dont need to change this\n 'id': 'worldcereal-crop-extent-belgium2', // This is the unique id of the product. Typically contains the dataset title+project name (or acronym)\n 'stac_version': '1.0.0', // This is the STAC version specification. You dont need to change this\n 'stac_extensions': [  // which schemas is the product information validated against. Typically you would not change these.\n    'https://stac-extensions.github.io/osc/v1.0.0/schema.json',\n    'https://stac-extensions.github.io/themes/v1.0.0/schema.json',\n    'https://stac-extensions.github.io/cf/v0.2.0/schema.json'\n ],\n 'created': '2025-07-14T17:37:16Z', //initial creation date\n 'updated': '2025-07-14T17:37:16Z'  // date of the last update\n 'title': 'WorldCereal Crop Extent - Belgium2', // product title\n 'description': 'WorldCereal is an ESA initiative that provides global ' // Short, but meaningful product description. It should provide enough information to the external users on the specific product.\n                'cropland and crop type maps at 10-meter resolution, offering '\n                'seasonally updated data on temporary crops, croptypes (maize, '\n                'winter cereals and spring cereals), and irrigation. This '\n                'dataset provides the outputs for Belgium.',\n\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]}, // the temporal and spatial extent of the product\n            'temporal': {'interval': [['2021-01-01T00:00:00Z',\n                                       '2021-12-31T23:59:59Z']]}},\n 'keywords': ['Crops', 'Cereal'], // list of keywords associated with the product. These are expected to be inline with the description.\n\n 'osc:project': 'worldcereal2', //unique id of the OSC project, this product is associated with. It must be the id provided in the ./project/(collectionid)\n 'osc:region': 'Belgium', //text description of the study area\n 'osc:status': 'ongoing', //product status\n 'osc:type': 'product', // Type of OSC STAC collection, for products should always be product\n \n 'links': [ // links to different elements of the catalog. The first two links should always be present and are always the same.\n    \n    {'href': '../../catalog.json',\n    'rel': 'root',\n    'title': 'Open Science Catalog',\n    'type': 'application/json'},\n    {'href': '../catalog.json',\n    'rel': 'parent',\n    'title': 'Products',\n    'type': 'application/json'},\n    {'href': '../../projects/worldcereal2/collection.json', // link to parent project (associated project)\n    'rel': 'related',\n    'title': 'Project: WorldCereal2',\n    'type': 'application/json'},\n\n    {'href': '../../themes/land/catalog.json', // link to the theme (scientific domain) this product is associated with.\n    'rel': 'related',\n    'title': 'Theme: Land',\n    'type': 'application/json'},\n    {'href': '../../eo-missions/sentinel-2/catalog.json', // link to eo-missions used to produce the outcomes\n    'rel': 'related',\n    'title': 'EO Mission: Sentinel-2',\n    'type': 'application/json'},\n    {'href': '../../variables/crop-yield-forecast/catalog.json', // link to variables specified below.\n    'rel': 'related',\n    'title': 'Variable: Crop Yield Forecast',\n    'type': 'application/json'},\n\n    {'href': 'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/ESA_WORLDCEREAL_SPRINGCEREALS', // link to dataset hosted in ESA Project Results Repository (PRR). \n    'rel': 'child',\n    'title': 'ESA WorldCereal Spring Cereals'},\n\n    {'href': 'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/ESA_WORLDCEREAL_SPRINGCEREALS',\n    'rel': 'via',\n    'title': 'Access'}, // external link to the actual data\n    {'href': 'https://worldcereal.github.io/worldcereal-documentation/',\n    'rel': 'via',\n    'title': 'Documentation'} // external link to data documentation\n],\n 'osc:missions': ['sentinel-2'], // array of ESA missions related to the product. This array of values is mandatory and limited to missions already existing in the OSC. If you would like to associate your product to a mission that is not on the list, create eo-mission entry first. \n 'osc:variables': ['crop-yield-forecast'], // array of variables related to the product. This array of values is mandatory and limited to variables already existing in the OSC. If you would like to associate your product to a mission that is not on the list, create eo-mission entry first. \n 'cf:parameter': [{'name': 'crop-yield-forecast'}], // optional parameters following cf conventions\n \n 'sci:doi': 'https://doi.org/10.57780/s3d-83ad619', // DOI, if already assigned\n \n 'themes': [ // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having at least one theme is mandatory.\n    {'concepts': [{'id': 'land'}],\n    'scheme': 'https://github.com/stac-extensions/osc#theme'}],\n \n 'license': 'proprietary', //  License of the product. Should be one of https://github.com/ESA-EarthCODE/open-science-catalog-validation/blob/main/schemas/license.json\n\n}\n\n\nIn addition to specifying the links from the product to other parts of the catalog, it is required to add the reverse links, as in case of the Project to following elements:\n\nFrom the Product Collection.json to the Catalog.json (listing all products in the OSC)\n\nFrom the associated Project to the Product\n\nFrom the associated EO-Missions catalog to the Product\n\nFrom the associated Variables Catalog to the Product\n\nFrom the associated Themes Catalog to the Product\n\nAdd the Product link to products/catalog.json by pasting the following in the links array:{\n    'rel':'child', \n    'target: './worldcereal-crop-extent-belgium2/collection.json', // use the collectionid of the product\n    'media_type': 'application/json',\n    'title': 'WorldCereal Crop Extent - Belgium2'   // title of the product as described in the collection.json file created before. \n}\n\nAdd the links array to associated elements of the OSC. For example add following product to parent project:{\n      \"rel\": \"related\",\n      \"href\": \"../../products/worldcereal-crop-extent-belgium2/collection.json\",\n      \"type\": \"application/json\",\n      \"title\": \"Product: WorldCereal Crop Extent - Belgium2\"\n}\n\nSimilarly, add links to other OSC elements like eo-missions, variables, themes etc.\n\n","type":"content","url":"/osc-pr-manual#id-2-2-add-new-product","position":11},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.3 Add new Workflow","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"type":"lvl4","url":"/osc-pr-manual#id-2-3-add-new-workflow","position":12},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl4":"2.3 Add new Workflow","lvl3":"2. Add the information about project/product/workflow/experiments/variables.","lvl2":"Adding information to the OSC"},"content":"Workflows are the code and workflows associated with a project, that have been used to generate a specific product. Workflows follow OGC record specifications in contrast to OSC Projects and Products entries. However, the metadata of a workflow is also expressed in JSON format.\n\nField Name\n\nDescription\n\nconformsTo\n\nAn array of URIs indicating which OGC API Records specifications this record conforms to.\n\ntype\n\nIndicates the GeoJSON object type. Required to be \"Feature\" for OGC compliance.\n\ngeometry\n\nSpatial representation of the item. Set to None here, as it may not be spatially explicit.\n\nlinkTemplates\n\nAn array of link templates as per the OGC API. Used for dynamic link generation.\n\nid\n\nUnique identifier for the workflow STAC item ('worldcereal-workflow2').\n\nlinks\n\nList of external and internal references including catalog navigation, project association, theme association, process graph, source code, and service endpoint.\n\nproperties.contacts\n\nList of individuals or organizations associated with the workflow. Each contact may include name, email, and roles such as technical_officer or consortium_member.\n\nproperties.created\n\nTimestamp representing when the workflow was first created (2025-07-14T18:02:13Z).\n\nproperties.updated\n\nTimestamp of the most recent update to the workflow (2025-07-14T18:02:13Z).\n\nproperties.version\n\nThe version number of the workflow (1).\n\nproperties.title\n\nA concise, descriptive title of the workflow: “ESA worldcereal global crop extent detector2”.\n\nproperties.description\n\nA summary of what the workflow does: “Detects crop land at 10m resolution, trained for global use...”.\n\nproperties.keywords\n\nArray of keywords to support discoverability (e.g., agriculture, crops).\n\nproperties.themes\n\nArray of themes the workflow relates to. Each entry includes a concepts array with IDs (e.g., 'land') and a scheme URL.\n\nproperties.formats\n\nOutput formats of the workflow (e.g., GeoTIFF).\n\nproperties.osc:project\n\nProject ID associated with the workflow (worldcereal2).\n\nproperties.osc:status\n\nCurrent status of the workflow (e.g., completed).\n\nproperties.osc:type\n\nType of OSC object, expected to be workflow.\n\nproperties.license\n\nLicense for the workflow (e.g., 'varuious' – likely a typo for various).\n\nAll data is stored in a record.json file, witin a folder that has the same name as the workflow id.{\n    'conformsTo': [ // OGC spec, does not need to change\n        'http://www.opengis.net/spec/ogcapi-records-1/1.0/req/record-core' \n    ],\n    'type': 'Feature'// OGC spec requirement, does not need to change\n    'geometry': None, // OGC spec requirement, does not need to change\n    'linkTemplates': [], // OGC spec, does not need to change\n    'id': 'worldcereal-workflow2',  // unique workflow id\n\n    'links': [  // links to different parts of the catalog. The first two links should always be present and are always the same.\n        \n        {'href': '../../catalog.json',\n            'rel': 'root',\n            'title': 'Open Science Catalog',\n            'type': 'application/json'},\n        {'href': '../catalog.json',\n            'rel': 'parent',\n            'title': 'Workflows',\n            'type': 'application/json'},\n        {'href': '../../projects/worldcereal2/collection.json', // link to associated project\n            'rel': 'related',\n            'title': 'Project: WorldCereal2',\n            'type': 'application/json'},\n        {'href': '../../themes/land/catalog.json', // link to associated themes in the themes array specified below\n        'rel': 'related',\n        'title': 'Theme: Land',\n        'type': 'application/json'},\n        { // link to the openeo-process process graph that describes the workflow\n            'href': 'https://raw.githubusercontent.com/WorldCereal/worldcereal-classification/refs/tags/worldcereal_crop_extent_v1.0.1/src/worldcereal/udp/worldcereal_crop_extent.json',\n            'rel': 'openeo-process',\n            'title': 'openEO Process Definition',\n            'type': 'application/json'},\n        { // external link to the full workflow codebase\n            'href': 'https://github.com/WorldCereal/worldcereal-classification.git',\n            'rel': 'git',\n            'title': 'Git source repository',\n            'type': 'application/json'},\n        { // external link to the service used to run the workflow\n            'href': 'https://openeofed.dataspace.copernicus.eu',\n            'rel': 'service',\n            'title': 'CDSE openEO federation',\n            'type': 'application/json'}\n        ],\n    // OGC spec requirement to have a properties field, that contains most of the workflow metadata\n\n    'properties': {\n        \n        'contacts': [{'emails': [{'value': 'marie-helene.rio@esa.int'}],\n                                'name': 'Marie-Helene Rio',\n                                'roles': ['technical_officer']},\n                                {'name': 'CNR-INSTITUTE OF MARINE SCIENCES-ISMAR '\n                                        '(IT)',\n                                'roles': ['consortium_member']},\n                                {'name': '+ATLANTIC – Association for an Atla '\n                                        '(PT)',\n                                'roles': ['consortium_member']}],\n        'created': '2025-07-14T18:02:13Z', // date of workflow creation\n        'updated': '2025-07-14T18:02:13Z', // date of workflow last update\n        'version': '1' //  workflow version\n        'title': 'ESA worldcereal global crop extent detector2', // Short and meaningful title of the workflow\n        'description': 'Detects crop land at 10m resolution, trained '\n                    'for global use. Based on Sentinel-1 and 2 '\n                    'data...', // Short and meaningful workflow description. Should provide specification on how the workflow can be executed and what it does.\n        'keywords': ['agriculture', 'crops'], // workflow keywords (to enhance the findability of the workflow)\n        'themes': [{'concepts': [{'id': 'land'}], // // this is an array of the ESA themes the project relates to. The fields are restricted to the themes available in the OCS. The format of the array is id:theme and having atleast one theme is mandatory.\n                            'scheme': 'https://github.com/stac-extensions/osc#theme'\n                    }],\n        'formats': [{'name': 'GeoTIFF'}], //format of worfklow output\n        'osc:project': 'worldcereal2', // workflow related project\n        'osc:status': 'completed', // workflow status\n        'osc:type': 'workflow', // OSC type, for workflows should always be workflow\n        'license': 'varuious', // workflow license\n        \n    }\n}\n\nIn addition to specifying the links from the workflow to other parts of the catalog, it is required to add the reverse links:\n\nFrom the Workflow record.json to the workflows/catalog.json (listing all workflows in the OSC)\n\nFrom the associated Project to the Workflow\n\nFrom the associated Themes to the Workflow\n\n","type":"content","url":"/osc-pr-manual#id-2-3-add-new-workflow","position":13},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"3. Open a PR to merge the new information into the OSC.","lvl2":"Adding information to the OSC"},"type":"lvl3","url":"/osc-pr-manual#id-3-open-a-pr-to-merge-the-new-information-into-the-osc","position":14},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl3":"3. Open a PR to merge the new information into the OSC.","lvl2":"Adding information to the OSC"},"content":"After you have added all the information, commit and push your changes to the forked repository and open a pull request against the OSC - \n\nhttps://​docs​.github​.com​/en​/pull​-requests​/collaborating​-with​-pull​-requests​/proposing​-changes​-to​-your​-work​-with​-pull​-requests​/creating​-a​-pull​-request .\n\nOnce you open the PR, there will be an automatic validation run against the information you added . If it fails you will have to change some of the added information. You can see if the PR is successfull based on the specific CI run, in the screen shot below. If you click on the red X, the validator will give you the specific reason for the failure. \nPlease be advised that once a pull request (PR) is submitted to the open-science-catalog-metadata repository, it will undergo a review process conducted by members of the EarthCODE team. During this process, the content will be evaluated for completeness and accuracy. Should any additional information or modifications be required, you may be asked to update your PR accordingly. All communication related to the review will be provided through comments within the PR.\n\n","type":"content","url":"/osc-pr-manual#id-3-open-a-pr-to-merge-the-new-information-into-the-osc","position":15},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Alternatives"},"type":"lvl2","url":"/osc-pr-manual#alternatives","position":16},{"hierarchy":{"lvl1":"Adding new content to Open Science Catalogue with Pull Request (PR)","lvl2":"Alternatives"},"content":"EarthCODE provides a \n\nGUI editor to automatically create links and open a PR for you.\n\nIf you are using one of the EarthCODE platforms, they provide specialised tools for automatic this work.\n\nYou can use libraries like pystac to automate some of the required work. \n\nThis tutorial shows how.","type":"content","url":"/osc-pr-manual#alternatives","position":17},{"hierarchy":{"lvl1":"Generating OSC information using pystac"},"type":"lvl1","url":"/osc-pr-pystac","position":0},{"hierarchy":{"lvl1":"Generating OSC information using pystac"},"content":"This notebook shows how to generate OSC Projects, Products and Workflows using pystac. EarthCODE provides a \n\nGUI editor that offers this and more functionality, including a user interface. However, if you decide to manually create items, using a library like pystac can save some time.\nThe code described here does not carry out all the required steps to pass the automated OSC validation. For example, you still have to generate all return links as described in the manual PR tutorial. You’ll also have to manually open the PR in the end.\n\nNOTE: Before you run the notebook you’ll need a fork of the open-science-catalog-metadata repository. See the Manual PR Tutorial about how to do it.","type":"content","url":"/osc-pr-pystac","position":1},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Import libraries"},"type":"lvl3","url":"/osc-pr-pystac#import-libraries","position":2},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Import libraries"},"content":"\n\nimport pystac\nfrom datetime import datetime\nfrom pystac.extensions.projection import ProjectionExtension\n\n","type":"content","url":"/osc-pr-pystac#import-libraries","position":3},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Get all entries from the Open Science Catalog"},"type":"lvl3","url":"/osc-pr-pystac#get-all-entries-from-the-open-science-catalog","position":4},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Get all entries from the Open Science Catalog"},"content":"\n\n# read the catalog root\ncatalog = pystac.Catalog.from_file('../../open-science-catalog-metadata/catalog.json')\n\n# access the list of the themes in open science catalog\nthemes = catalog.get_child('themes')\nallowed_themes = [child.id for child in themes.get_children()]\n\n\n# access the list of available ESA missions\nmissions = catalog.get_child('eo-missions')\nallowed_missions = [child.id for child in missions.get_children()]\n\n# access the list of avaiable variables\nvariables = catalog.get_child('variables')\nallowed_variables = [child.id for child in variables.get_children()]\n\n# access the list of existing projects, products and workflows\nproducts = catalog.get_child('products')\nprojects = catalog.get_child('projects')\nworkflows = catalog.get_child('workflows')\n\n","type":"content","url":"/osc-pr-pystac#get-all-entries-from-the-open-science-catalog","position":5},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Add new variables, theme and eo missions"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-add-new-variables-theme-and-eo-missions","position":6},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Add new variables, theme and eo missions"},"content":"\n\ndef add_product_variables(collection, variables_to_add):\n    '''Add variables to the collection custom fields and add links to the missions collection.'''\n    \n    for variable in variables_to_add:\n        \n        assert variable in allowed_variables\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=variables.get_child(variable).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"Variable: {variables.get_child(variable).title}\")\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n        \"osc:variables\": variables_to_add\n    })\n\ndef add_themes(collection, themes_to_add):\n    '''Add themes to the collection custom fields and add links to the themes collection.'''\n    \n    themes_list = []\n    for theme in themes_to_add:\n        \n        assert theme in allowed_themes\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=themes.get_child(theme).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"Theme: {themes.get_child(theme).title}\")\n        )\n        \n        themes_list.append(\n            {\n                \"scheme\": \"https://github.com/stac-extensions/osc#theme\",\n                \"concepts\": [{\"id\": theme}]\n            }\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n        \"themes\": themes_list\n    }\n    )\n\n\ndef add_links(collection, relations, targets, titles):\n\n    '''Add links from the collection to outside websites.'''\n    links = []\n    \n    for rel, target, title in zip(relations, targets, titles):\n        links.append(pystac.Link(rel=rel, target=target, title=title)),\n    \n    collection.add_links(links)\n\n\ndef create_contract(name, roles, emails):\n    '''Create a contact template'''\n    contact =  {\n        \"name\": name,\n        \"roles\": [r for r in roles]\n    }\n    if emails:\n        contact['emails'] = [{\"value\":email} for email in emails]\n    return contact\n\ndef add_product_missions(collection, missions_to_add):\n    '''Add missions to the collection custom fields and add links to the missions collection.'''\n    \n    for mission in missions_to_add:\n        \n        assert mission in allowed_missions\n\n        # add the correct link\n        collection.add_link(\n            pystac.Link(rel=\"related\", \n                        target=missions.get_child(mission).get_links('self')[0].href, \n                        media_type=\"application/json\",\n                        title=f\"EO Mission: {missions.get_child(mission).title}\"\n            )\n        )\n\n    # Add themes to the custom fields\n    collection.extra_fields.update({\n         \"osc:missions\": missions_to_add\n    }\n    )\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-add-new-variables-theme-and-eo-missions","position":7},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new project collection"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-project-collection","position":8},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new project collection"},"content":"\n\n\ndef create_project_collection(project_id, project_title, project_description, \n                      project_status, extent, project_license):\n\n    '''Create project collection template from the provided information.'''\n\n    # Create the collection\n    collection = pystac.Collection(\n        id=project_id,\n        description=project_description,\n        extent=extent,\n        license=project_license,\n        title=project_title,\n        extra_fields = {\n            \"osc:status\": project_status,\n            \"osc:type\": \"project\",\n            \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        },\n        stac_extensions=[\n            \"https://stac-extensions.github.io/osc/v1.0.0/schema.json\",\n            \"https://stac-extensions.github.io/themes/v1.0.0/schema.json\",\n            \"https://stac-extensions.github.io/contacts/v0.1.1/schema.json\"\n        ]\n    \n    )\n\n    # Add pre-determined links \n    collection.add_links([\n        pystac.Link(rel=\"root\", target=\"../../catalog.json\", media_type=\"application/json\", title=\"Open Science Catalog\"),\n        pystac.Link(rel=\"parent\", target=\"../catalog.json\", media_type=\"application/json\", title=\"Projects\"),\n        # pystac.Link(rel=\"self\", target=f\"https://esa-earthcode.github.io/open-science-catalog-metadata/projects/{project_id}/collection.json\", media_type=\"application/json\"),\n    ])\n\n    return collection\n\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-project-collection","position":9},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new product collection"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-product-collection","position":10},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new product collection"},"content":"\n\ndef create_product_collection(product_id, product_title, product_description, product_extent, product_license,\n                              product_keywords, product_status, product_region, product_project_id, product_project_title,\n                              product_parameters=None, product_doi=None):\n\n    collection = pystac.Collection(\n            id=product_id,\n            title=product_title,\n            description=product_description,\n            extent=product_extent,\n            license=product_license,\n            keywords=product_keywords,\n            stac_extensions=[\n                \"https://stac-extensions.github.io/osc/v1.0.0/schema.json\",\n                \"https://stac-extensions.github.io/themes/v1.0.0/schema.json\",\n                \"https://stac-extensions.github.io/cf/v0.2.0/schema.json\"\n            ],\n        )\n    \n    # Add pre-determined links \n    collection.add_links([\n        pystac.Link(rel=\"root\", target=\"../../catalog.json\", media_type=\"application/json\", title=\"Open Science Catalog\"),\n        pystac.Link(rel=\"parent\", target=\"../catalog.json\", media_type=\"application/json\", title=\"Products\"),\n        # pystac.Link(rel=\"self\", target=f\"https://esa-earthcode.github.io/open-science-catalog-metadata/products/{project_id}/collection.json\", media_type=\"application/json\"),\n        pystac.Link(rel=\"related\", target=f\"../../projects/{product_project_id}/collection.json\", media_type=\"application/json\", title=f\"Project: {product_project_title}\"),\n\n    ])\n\n    # Add extra properties\n    collection.extra_fields.update({\n        \"osc:project\": product_project_id,\n        \"osc:status\": product_status,\n        \"osc:region\": product_region,\n        \"osc:type\": \"product\",\n        \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n    })\n\n    if product_doi is not None:\n        collection.extra_fields[\"sci:doi\"] = product_doi\n\n\n    if product_parameters:\n        collection.extra_fields[\"cf:parameter\"] = [{\"name\": p} for p in product_parameters]\n    \n    return collection\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-product-collection","position":11},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new workflow record"},"type":"lvl3","url":"/osc-pr-pystac#define-helper-functions-create-new-workflow-record","position":12},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl3":"Define helper functions | Create new workflow record"},"content":"\n\ndef create_workflow_collection(workflow_id, workflow_title, \n                               workflow_description, workflow_license, workflow_extent,\n                               workflow_keywords, workflow_formats, workflow_project, workflow_project_title):\n\n    '''Create a workflow collection template from the provided information.'''\n\n    # Create the collection\n\n    collection = {\n        'id': workflow_id,\n        'type': 'Feature',\n        'geometry': None,\n        \"conformsTo\": [\"http://www.opengis.net/spec/ogcapi-records-1/1.0/req/record-core\"],\n        \"properties\": {\n            \"title\": workflow_title,\n            \"description\": workflow_description,\n            \"osc:type\": \"workflow\",\n            \"osc:project\": workflow_project,\n            \"osc:status\": \"completed\",\n            \"formats\": [{\"name\": f} for f in workflow_formats],\n            \"updated\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"keywords\": workflow_keywords,\n            \"license\": workflow_license,\n            \"version\": \"1\"\n        },\n        \"linkTemplates\": [],\n        \"links\": [\n            \n            {\n                \"rel\": \"root\",\n                \"href\": \"../../catalog.json\",\n                \"type\": \"application/json\",\n                \"title\": \"Open Science Catalog\"\n            },            \n            {\n                \"rel\": \"parent\",\n                \"href\": \"../catalog.json\",\n                \"type\": \"application/json\",\n                \"title\": \"Workflows\"\n            },            \n  \n            {\n                \"rel\": \"related\",\n                \"href\": f\"../../projects/{workflow_project}/collection.json\",\n                \"type\": \"application/json\",\n                \"title\": f\"Project: {workflow_project_title}\"\n            },\n            \n        ]\n\n    }\n    \n    return collection\n\n\n","type":"content","url":"/osc-pr-pystac#define-helper-functions-create-new-workflow-record","position":13},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new project"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-project","position":14},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new project"},"content":"\n\n# Define id, title, description, project status, license\nproject_id = \"worldcereal2\"\nproject_title = \"WorldCereal2\"\nproject_description = \"WorldCereal is an ESA initiative that provides global cropland and crop type maps at 10-meter resolution, offering seasonally updated data on temporary crops, croptypes (maize, winter cereals and spring cereals), and irrigation.\"\nproject_status = \"completed\"\nproject_license = 'proprietary'\n\n# Define spatial and temporal extent\nspatial_extent = pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]])\ntemporal_extent = pystac.TemporalExtent([[datetime(2021, 1, 1), datetime(2021, 12, 31, 23, 59, 59)]])\nextent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n\n# Define links and link titles\nproject_link_targets = [\"https://esa-worldcereal.org/en\", \n                        \"https://eo4society.esa.int/projects/worldcereal-global-crop-monitoring-at-field-scale/\"]\nproject_link_relations = [\"via\", \"via\"]\nproject_link_titles = [\"Website\", \"EO4Society Link\"]\n\n# Define project themes\nproject_themes = [\"land\"]\n\n# contacts\nproject_contracts_info = [\n    (\"Zoltan Szantoi\", [\"technical_officer\"], [\"Zoltan.Szantoi@esa.int\"]),\n    (\"VITO Remote Sensing\", [\"consortium_member\"], None)\n]\n\ncollection = create_project_collection(project_id, project_title, project_description, \n                      project_status, extent, project_license)\n\n# add links\nadd_links(collection, project_link_relations, project_link_targets, project_link_titles)\n\n## add themes\nadd_themes(collection, project_themes)\n\n\n# Add contacts\ncollection.extra_fields.update({\n\n    \"contacts\": [create_contract(*info) for info in project_contracts_info]\n    \n})\n\ncollection.validate()\n\ncollection\n\n# save this file and copy it to the catalog/projects/{project}/collection.json\ncollection.save_object(dest_href='project_collection.json')\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/projects/worldcereal2/\n!cp project_collection.json ../open-science-catalog-metadata-staging/projects/worldcereal2/collection.json\n\n","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-project","position":15},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new product"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-product","position":16},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new product"},"content":"\n\nproduct_id = \"worldcereal-crop-extent-belgium2\"\nproduct_title = \"WorldCereal Crop Extent - Belgium2\"\nproduct_description = \"WorldCereal is an ESA initiative that provides global cropland and crop type maps at 10-meter resolution, offering seasonally updated data on temporary crops, croptypes (maize, winter cereals and spring cereals), and irrigation. This dataset provides the outputs for Belgium.\"\nproduct_keywords = [\n    \"Crops\",\n    \"Cereal\"\n]\nproduct_status = \"ongoing\"\nproduct_license = \"proprietary\"\n\n# Define spatial and temporal extent\nproduct_spatial_extent = pystac.SpatialExtent([[2.5135, 49.529, 6.156, 51.475]])\nproduct_temporal_extent = pystac.TemporalExtent([[datetime(2021, 1, 1), datetime(2021, 12, 31, 23, 59, 59)]])\nproduct_extent = pystac.Extent(spatial=product_spatial_extent, temporal=product_temporal_extent)\nproduct_region = \"Belgium\"\nproduct_themes = [\"land\"]\nproduct_missions = [ \"sentinel-2\"]\nproduct_variables = [  \"crop-yield-forecast\" ]\nproduct_parameters = [  \"crop-yield-forecast\" ]\n\nproduct_project_id = \"worldcereal2\"\nproduct_project_title = \"WorldCereal2\"\n\nproduct_doi = \"https://doi.org/10.57780/s3d-83ad619\"\n\n\n# define links to add\n\nproduct_target_relations = ['child', 'via', 'via']\nproduct_target_links = ['https://eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation',\n                        'https://eoresults.esa.int/browser/#/external/eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation',\n                        'https://eoresults.esa.int/d/sentinel3-ampli-ice-sheet-elevation/2025/05/07/sentinel-3-ampli-user-handbook/S3_AMPLI_User_Handbook.pdf']\nproduct_target_titles = ['PRR link', 'Access', 'Documentation']\n\n\nproduct_collection = create_product_collection(\n    product_id, product_title, product_description, product_extent, \n    product_license, product_keywords, product_status, product_region, \n    product_project_id, product_project_title, product_parameters, product_doi)\n\n# add themes\nadd_themes(product_collection, product_themes)\n\n\n\nadd_product_missions(product_collection, product_missions)\n\nadd_product_variables(product_collection, product_variables)\n\n# add links\nadd_links(product_collection,\n          product_target_relations,\n          product_target_links,\n          product_target_titles\n)\n\nproduct_collection.validate()\n\nproduct_collection\n\n# save this file and copy it to the catalog/products/{product_id}/collection.json\nproduct_collection.save_object(dest_href='product_collection.json')\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/products/worldcereal-crop-extent-belgium2/\n!cp product_collection.json ../open-science-catalog-metadata-staging/products/worldcereal-crop-extent-belgium2/collection.json\n\n","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-product","position":17},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new workflow"},"type":"lvl2","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-workflow","position":18},{"hierarchy":{"lvl1":"Generating OSC information using pystac","lvl2":"Create a metadata collection for new workflow"},"content":"\n\nworkflow_id = \"worldcereal-workflow2\"\nworkflow_title=\"ESA worldcereal global crop extent detector2\"\nworkflow_description=\"Detects crop land at 10m resolution, trained for global use. Based on Sentinel-1 and 2 data...\"\nworkflow_license = \"proprietary\"\nworkflow_keywords= [\"agriculture\", \"crops\"]\nworkflow_formats = [\"GeoTIFF\"]\nworkflow_project = \"worldcereal2\"\nworkflow_project_title = \"WorldCereal2\"\n\nworkflow_themes = ['land']\n\n# Define spatial and temporal extent\nspatial_extent = pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]])\ntemporal_extent = pystac.TemporalExtent([[datetime(2022, 2, 1), datetime(2026, 1, 31, 23, 59, 59)]])\nworkflow_extent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n\n\n# add custom theme schemas\n\nworkflow_contracts_info = [\n    (\"Marie-Helene Rio\", [\"technical_officer\"], [\"marie-helene.rio@esa.int\"]),\n    (\"CNR-INSTITUTE OF MARINE SCIENCES-ISMAR (IT)\", [\"consortium_member\"], None),\n    (\"+ATLANTIC – Association for an Atla (PT)\", [\"consortium_member\"], None),\n]\n\nworkflow_collection = create_workflow_collection(workflow_id, workflow_title, \n                               workflow_description, workflow_license, workflow_extent,\n                               workflow_keywords, workflow_formats, workflow_project, workflow_project_title)\n\n# add contacts\nworkflow_collection['properties'].update({\n\n    \"contacts\": [create_contract(*info) for info in workflow_contracts_info]\n    \n})\n\n\nworkflow_collection['properties']['themes'] = [\n    {\n        \"scheme\": \"https://github.com/stac-extensions/osc#theme\",\n        \"concepts\": [{\"id\": t} for t in workflow_themes]\n    }\n]\n\nfor t in workflow_themes:\n    workflow_collection['links'].append(\n            {\n                    \"rel\": 'related',\n                    \"href\": f\"../../{t}/land/catalog.json\",\n                    \"type\": \"application/json\",\n                    \"title\": f'Theme: {t.capitalize()}'\n                }\n)\n\nworkflow_target_relations = ['openeo-process', 'git', 'service']\nworkflow_target_links = ['https://raw.githubusercontent.com/WorldCereal/worldcereal-classification/refs/tags/worldcereal_crop_extent_v1.0.1/src/worldcereal/udp/worldcereal_crop_extent.json',\n                        'https://github.com/WorldCereal/worldcereal-classification.git',\n                        'https://openeofed.dataspace.copernicus.eu']\nworkflow_target_titles = ['openEO Process Definition', 'Git source repository', 'CDSE openEO federation']\n\nfor rel, link, title in zip(workflow_target_relations, workflow_target_links, workflow_target_titles):\n    workflow_collection['links'].append(\n        {\n                \"rel\": rel,\n                \"href\": link,\n                \"type\": \"application/json\",\n                \"title\": title\n            }\n    )\n\nimport json\nwith open('record.json', 'w') as f:\n    json.dump(workflow_collection, f)\n\n# optionally run this code to transfer the generated file to the OSC folder, ready to be commited.\n!mkdir -p ../open-science-catalog-metadata-staging/workflows/worldcereal-workflow2/\n!cp record.json ../open-science-catalog-metadata-staging/workflows/worldcereal-workflow2/record.json","type":"content","url":"/osc-pr-pystac#create-a-metadata-collection-for-new-workflow","position":19},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project"},"type":"lvl1","url":"/careheat-v2","position":0},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). The code below demonstrates how to perform the necessary steps using real data from the ESA project deteCtion and threAts of maRinE HEAT waves (CAREHeat). The focus of CAREHeat is to improve existing extreme marine heatwave(MHW) detection algorithms, contributing to a better understanding of their impacts.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\n🔗 Check the project website: \n\ndeteCtion and threAts of maRinE HEAT waves (CAREHeat) – Website\n\n🔗 Check the eo4society page: \n\ndeteCtion and threAts of maRinE HEAT waves (CAREHeat) – eo4society\n\nCareHeat Dataset source: \n\nCheck out the Dataset of Marine heatwaves and cold spells events based on ESA-CCI SSTs","type":"content","url":"/careheat-v2","position":1},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl3":"Acknowledgment"},"type":"lvl3","url":"/careheat-v2#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl3":"Acknowledgment"},"content":"We gratefully acknowledge the deteCtion and threAts of maRinE HEAT waves (CAREHeat) for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\nimport os\n\n","type":"content","url":"/careheat-v2#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"1. Generate the parent collection"},"type":"lvl2","url":"/careheat-v2#id-1-generate-the-parent-collection","position":4},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"1. Generate the parent collection"},"content":"The root STAC Collection provides a general description of all project outputs which will be stored on the PRR.\nThe PRR STAC Collection template enforces some required fields that you need to provide in order to build its valid description. Most of these metadata fields should already be available and can be extracted from your data.\n\n# create the parent collection\ncollectionid = \"careheat-marine-heatwaves-cold-spells\"\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Marine heatwaves and cold spells events based on ESA-CCI SSTs\",\n  \"description\": \"Marine heatwaves (MHWs) and cold spells (MCSs) prepared by the National Research Council - Institute of Marine Sciences (CNR-ISMAR, Italy) within the ESA-funded CAREHeat project. The catalogues are based on the ESA-CCI sea surface temperature (SST) dataset (available from https://doi.org/10.24381/cds.cf608234) for the period 1982-2022, on a regular 1°x1° longitude-latitude grid.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n         [-180, -90, 180, 90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"1982-01-01T00:00:00Z\",\n          \"2022-12-31T23:59:59Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"CC-BY-4.0\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\n","type":"content","url":"/careheat-v2#id-1-generate-the-parent-collection","position":5},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl2","url":"/careheat-v2#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":6},{"hierarchy":{"lvl1":"Generating STAC collections for the CareHeat project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"The second step is to describe the different files as STAC Items and Assets. Take your time to decide how your data should be categorised to improve usability of the data, and ensure intuitive navigation through different items in the collections. There are multiple strategies for doing this and this tutorial demonstrate one of the possible ways of doing that. Examples of how other ESA projects are doing this are available in the \n\nEarthCODE documentation .\n\nbaseurl = './data/careheat-marine-heatwaves-cold-spells/'\n\nbbox = [-180, -90, 180, 90]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\nfrom pathlib import Path\nbaseurl = Path('../../data/careheat-marine-heatwaves-cold-spells/')\nfiles = list(baseurl.glob(\"*.nc\"))\n\n# Convert to POSIX-style strings\nfile_paths = [f.as_posix() for f in files]\n\nfor file in file_paths:\n    print(file)\n\nfor file in file_paths:\n\n    # open the dataset and read metadata + convert to STAC\n    ds = xr.open_dataset(file)\n    detrended = 'Detrended (SSA) ' if '_ssa_' in file else ''\n    \n    template = {\n\n        \"id\": f\"{collectionid}-{file.split('/')[-1][:-3].lower()}\",\n        \"type\": \"Feature\",\n        \"stac_version\": \"1.0.0\",\n        \"properties\": {\n            \"title\": detrended + ds.attrs['product'],\n            \"description\": detrended + ds.attrs['description'],\n            \"start_datetime\": pd.to_datetime(ds.attrs['climatologyPeriod'][0], format='%Y').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"end_datetime\": pd.to_datetime(ds.attrs['climatologyPeriod'][-1], format='%Y').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"license\": \"CC-BY-4.0\",\n            \"created\": pd.to_datetime(ds.attrs['date'], format='%Y-%m').strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"git_information\": ds.attrs['git_information'],\n            \"website\": ds.attrs['website'],\n            \"version\": ds.attrs['version'],\n            \"changelog\": ds.attrs['changelog'],\n            \"institution\": ds.attrs['institution'],\n            \"author\": ds.attrs['author'],\n            \"contact\": ds.attrs['contact'],\n        },\n        \"geometry\": geometry,\n        \"bbox\": bbox,\n        \"assets\": {\n            \"data\": {\n                \"href\": f\"./{collectionid}/{file.split('/')[-1]}\",  # or local path\n                \"type\": \"application/x-netcdf\",\n                \"roles\": [\"data\"],\n                \"title\": detrended + ds.attrs['product']\n            }\n        }\n    }\n    # 3. Generate the STAC Item\n    item = xarray_to_stac(\n        ds,\n        template,\n        temporal_dimension=\"time\" if 'time' in ds.coords else False,\n        x_dimension='lon',\n        y_dimension='lat',\n        reference_system=False\n    )\n\n    # validate and add the STAC Item to the collection\n    item.validate()\n    collection.add_item(item)\n\ncollection\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/careheat-v2#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":7},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset"},"type":"lvl1","url":"/creating-stac-catalog-from-prr-example","position":0},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset"},"content":"This is an example notebook for creating the STAC Items uploaded to ESA Project Results Repository and made available at: \n\nhttps://​eoresults​.esa​.int​/browser​/​#​/external​/eoresults​.esa​.int​/stac​/collections​/sentinel3​-ampli​-ice​-sheet​-elevation\n\nDataset is also discoverable via Open Science Catalogue, providing access to created in this tutorial collection stored in ESA Project Results Repository (PRR).\n\n\nhttps://​opensciencedata​.esa​.int​/products​/sentinel3​-ampli​-ice​-sheet​-elevation​/collection\n\nIt focuses on generating metadata for a project with a hundreads of items, each of which has hundreads of netcdf assets.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project **SRAL Processing over Land Ice\n**. With the focus of the project on improving Sentinel-3 altimetry performances over land ice.\n\n🔗 Check the : \n\nUser handbook\n\n🔗 Check the : \n\nScientifc publication","type":"content","url":"/creating-stac-catalog-from-prr-example","position":1},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl4":"Acknowledgment"},"type":"lvl4","url":"/creating-stac-catalog-from-prr-example#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl4":"Acknowledgment"},"content":"We gratefully acknowledge the SRAL Processing over Land Ice team for providing access to the data used in this example, as well as support in creating it.","type":"content","url":"/creating-stac-catalog-from-prr-example#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"Steps described in this notebook"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#steps-described-in-this-notebook","position":4},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"Steps described in this notebook"},"content":"This notebook presents the workflow for generating a PRR Collection for the entire dataset coming from the project. To create a valid STAC Items and Collection you should follow steps described below:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the collection\n\nSave the normalised collection\n\nDue to the complexity of the project and the time it takes to process the data, the STAC Items are generated first and stored locally. They are added to the collection afterwards.\n\nThis notebook can be used as an example for following scenario(s):\n\nCreating the STAC Items from the files stored locally\n\nCreating the STAC Items from files stored in the s3bucket or other cloud repository\n\nCreating the STAC Items from files already ingested into PRR\n\nOf course if your files are locally stored, or stored in a different S3 Bucket the access to them (roor_url and items paths) should be adapted according to your dataset location.\n\nNote: Due to the original size of the dataset ~ 100GB, running this notebook end to end may take hours. We do advise therefore to trying it on your own datasets by changing file paths to be able to produe valid STAC Collaction and STAC Items.\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#steps-described-in-this-notebook","position":5},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"Loading Libraries"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#loading-libraries","position":6},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"Loading Libraries"},"content":"\n\nimport json\nimport time\nimport pystac\nimport rasterio\nfrom shapely import box\nimport pandas as pd\nimport xarray as xr\nfrom datetime import datetime\nfrom dateutil.parser import isoparse\nfrom dateutil import parser\nfrom dateutil.parser import parse\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#loading-libraries","position":7},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"2. Get Product file links"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-2-get-product-file-links","position":8},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"2. Get Product file links"},"content":"Note: We are using the links from the PRR directly, however when the notebook was created originally all the files were available locally. Furthermore, due to the original size of the dataset ~ 100GB, running this notebook end to end may take hours. We do advise therefore to trying it on your own datasets by changing file paths to be able to produe valid STAC Collaction and STAC Items.\n\nroot_url = 'https://eoresults.esa.int' # provide a root url for the datasets items \n\n# get all items for the S3 AMPLI collection from the PRR STAC API\nitems = pystac.ItemCollection.from_file('https://eoresults.esa.int/stac/collections/sentinel3-ampli-ice-sheet-elevation/items?limit=10_000')\n\n# get the paths to all the data\n\n# using a dictionary is faster than using pystac\nitems_dict = items.to_dict()\nall_item_paths = []\nfor item in items_dict['features']:\n    assets = item['assets']\n    for asset_name, asset_dict in assets.items():\n        if asset_dict['roles'] == ['data']:\n            all_item_paths.append(asset_dict['href'])\n\n# Create a list of EO Missions and instruments as well as region of the dataset and cycles\ninstruments = ['sentinel-3a', 'sentinel-3b']\nregions = ['antarctica', 'greenland']\ncycles = [f\"cycle{str(i).zfill(3)}\" for i in range(5, 112)]  # Cycle005 to Cycle111\n\n# Assign the instrument name based on the acronym used in the file name\nrenaming = {\n    'S3A': 'sentinel-3a',\n    'S3B': 'sentinel-3b',\n    'ANT': 'antarctica',\n    'GRE': 'greenland'\n}\n\nDefine geometries, which are the same for all items within the same region. If they are not, these have to be extracted from the assets inside the item.\n\n# Define the spatial extent (bbox) for each region of interest\ngreenland_bbox = [-74.0, 59.0, -10.0, 84.0]\ngreenland_geometry = json.loads(json.dumps(box(*greenland_bbox).__geo_interface__))\n\nantarctica_bbox = [-180.0, -90.0, 180.0, -60.0]\nantarctica_geometry = json.loads(json.dumps(box(*antarctica_bbox).__geo_interface__))\n\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-2-get-product-file-links","position":9},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"2.1 Group the files by the instruments, region and cycle of the dataset","lvl2":"2. Get Product file links"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-2-1-group-the-files-by-the-instruments-region-and-cycle-of-the-dataset","position":10},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"2.1 Group the files by the instruments, region and cycle of the dataset","lvl2":"2. Get Product file links"},"content":"\n\ndata = []\n\nfor ipath in all_item_paths:\n    splitname = ipath.split('/')[-1].split('_')\n    instrument = splitname[0]\n    cycle = splitname[9]\n    region = splitname[-2]\n\n    data.append((renaming[instrument], renaming[region], cycle, ipath))\n\n\nfiledata = pd.DataFrame(data, columns=['instrument', 'region', 'cycle', 'path'])\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-2-1-group-the-files-by-the-instruments-region-and-cycle-of-the-dataset","position":11},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-3-create-the-stac-items-with-the-metadata-from-the-original-files-loaded-from-the-prr","position":12},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\n# group all files into items from the same instrument, region and cycle\nfor (instrument, region, cycle), links in filedata.groupby(['instrument', 'region', 'cycle']):\n    \n    # open the metadata attributes for each file in the group\n    datasets = [xr.open_dataset(root_url + link + '#mode=bytes') for link in links['path']]\n\n\n    # Define the Temporal extent\n    first_item = datasets[0]\n    last_item = datasets[-1]\n    props = first_item.attrs\n    props2 = last_item.attrs\n\n    start_datetime = props.get(\"first_meas_time\")\n    end_datetime = props2.get(\"last_meas_time\")\n\n    # Define the geometry\n    if props['zone'] == 'Antarctica':\n        bbox = antarctica_bbox\n        geometry = antarctica_geometry\n    elif props['zone'] == 'Greenland':\n        bbox = greenland_bbox\n        geometry = greenland_geometry\n\n\n    # Shared properties\n    properties = {\n        \"start_datetime\": start_datetime,\n        \"end_datetime\": end_datetime,\n        \"created\": props.get(\"processing_date\"),\n        \"description\": f\"Sentinel-3 AMPLI Land Ice Level-2 product acquired by {instrument.capitalize()} platform derived from the SRAL altimeter in Earth Observation mode over {region} region.\",\n        \"conventions\": props.get(\"Conventions\"),\n        \"platform_name\": props.get(\"platform_name\"),\n        \"platform_serial_identifier\": props.get(\"platform_serial_identifier\"),\n        \"altimeter_sensor_name\": props.get(\"altimeter_sensor_name\"),\n        \"operational_mode\": props.get(\"operational_mode\"),\n        \"cycle_number\": props.get(\"cycle_number\"),\n        \"netcdf_version\": props.get(\"netcdf_version\"),\n        \"product_type\": props.get(\"product_type\"),\n        \"timeliness\": props.get(\"timeliness\"),\n        \"institution\": props.get(\"institution\"),\n        \"processing_level\": props.get(\"processing_level\"),\n        \"processor_name\": props.get(\"processor_name\"),\n        \"processor_version\": props.get(\"processor_version\"),\n        \"references\": props.get(\"references\"),\n        \"zone\": props.get(\"zone\"),\n    }\n\n\n    # Create STAC item for the cycle\n    item = pystac.Item(\n        id=f\"sentinel-3{props.get(\"platform_serial_identifier\").lower()}-{props.get(\"zone\").lower()}-{cycle.lower()}\",\n        geometry=geometry,\n        bbox=bbox,\n        datetime=isoparse(start_datetime),\n        properties=properties\n    )\n\n    item.stac_version = \"1.1.0\"\n    item.stac_extensions = [\n        \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\",\n        \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\",\n        \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\"\n    ]\n\n    item.assets = {}\n\n    # Add assets from that cycle\n    for nc_href, ds in zip(links['path'], datasets):\n\n        asset_title = ds.attrs['product_name']\n        extra_fields = {\n            \"cycle_number\": str(ds.attrs.get(\"cycle_number\")),\n            \"orbit_number\": str(ds.attrs.get(\"orbit_number\")),\n            \"relative_orbit_number\": str(ds.attrs.get(\"relative_orbit_number\")),\n            \"orbit_direction\": ds.attrs.get(\"orbit_direction\"),\n        }\n\n        item.add_asset(\n            key=asset_title,\n            asset=pystac.Asset(\n                href=nc_href,\n                media_type=\"application/x-netcdf\",\n                roles=[\"data\"],\n                extra_fields=extra_fields\n            )\n        )\n\n    # Save STAC item per cycle\n    json_filename = f\"sentinel-3{props.get(\"platform_serial_identifier\").lower()}-{props.get(\"zone\").lower()}-{cycle.lower()}.json\"\n    item.save_object(dest_href='examples/' + json_filename, include_self_link=False)\n    print(f\" Saved {json_filename}\")\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-create-the-stac-items-with-the-metadata-from-the-original-files-loaded-from-the-prr","position":13},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.1  Import documentation","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-3-1-import-documentation","position":14},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.1  Import documentation","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\nimport pystac\nfrom datetime import datetime\nimport os\nfrom datetime import datetime, timezone\n\ndate_str = \"07/05/2025\"\n\n# Convert to ISO format string (YYYY-MM-DD)\niso_like_str = datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n\n# Parse with isoparse and attach UTC timezone\ndt_utc = isoparse(iso_like_str).replace(tzinfo=timezone.utc)\n\nprint(dt_utc.isoformat())\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-1-import-documentation","position":15},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.2  Create STAC Item for the documentation associated to the dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-3-2-create-stac-item-for-the-documentation-associated-to-the-dataset","position":16},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"3.2  Create STAC Item for the documentation associated to the dataset","lvl2":"3. Create the STAC Items with the metadata from the original files loaded from the PRR"},"content":"\n\n# Basic metadata\ndoc_href = \"/d/S3_AMPLI_User_Handbook.pdf\"  # Relative or absolute href\ndoc_title = \"Sentinel-3 Altimetry over Land Ice: AMPLI level-2 Products\"\ndoc_description = \"User Handbook for Sentinel-3 Altimetry over Land Ice: AMPLI level-2 Products\"\n\n# Create STAC item\nitem = pystac.Item(\n    id=\"sentinel-3-ampli-user-handbook\",\n    geometry=None,\n    bbox=None,\n    datetime=dt_utc,\n    properties={\n        \"title\": doc_title,\n        \"description\": doc_description,\n        \"reference\": \"CLS-ENV-MU-24-0389\",\n        \"issue_n\": dt_utc.isoformat()\n    }\n)\n\n# Add asset for the PDF\nitem.add_asset(\n    key=\"documentation\",\n    asset=pystac.Asset(\n        href=doc_href,\n        media_type=\"application/pdf\",\n        roles=[\"documentation\"],\n        title=doc_title\n    )\n)\n\n# Save to file\nitem.set_self_href(\"examples/sentinel-3-ampli-user-handbook.json\")\nitem.save_object(include_self_link=False)\n\nprint(\"📄 STAC Item for documentation created: sentinel-3-ampli-user-handbook.json\")\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-3-2-create-stac-item-for-the-documentation-associated-to-the-dataset","position":17},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"4. Generate valid STAC collection"},"type":"lvl2","url":"/creating-stac-catalog-from-prr-example#id-4-generate-valid-stac-collection","position":18},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl2":"4. Generate valid STAC collection"},"content":"Once all the assets are processed, create the parent collection for all Items created in the previous step.\n\ncollection = pystac.Collection.from_dict(\n\n{\n  \"id\": \"sentinel3-ampli-ice-sheet-elevation\",\n  \"type\": \"Collection\",\n  \"links\": [\n  ],\n  \"title\": \"Sentinel-3 AMPLI Ice Sheet Elevation\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-180, -90, 180, 90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2016-06-01T00:00:00Z\",\n          \"2024-05-09T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"CC-BY-4.0\",\n  \"summaries\": {\n    \"references\": [\n      \"https://doi.org/10.5194/egusphere-2024-1323\"\n    ],\n    \"institution\": [\n      \"CNES\"\n    ],\n    \"platform_name\": [\n      \"SENTINEL-3\"\n    ],\n    \"processor_name\": [\n      \"Altimeter data Modelling and Processing for Land Ice (AMPLI)\"\n    ],\n    \"operational_mode\": [\n      \"Earth Observation\"\n    ],\n    \"processing_level\": [\n      \"2\"\n    ],\n    \"processor_version\": [\n      \"v1.0\"\n    ],\n    \"altimeter_sensor_name\": [\n      \"SRAL\"\n    ]\n  },\n  \"description\": \"Ice sheet elevation estimated along the Sentinel-3 satellite track, as retrieved with the Altimeter data Modelling and Processing for Land Ice (AMPLI). The products cover Antarctica and Greenland.\",\n  \"stac_version\": \"1.1.0\"\n}\n)\ncollection\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-generate-valid-stac-collection","position":19},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.1. Add items to collection","lvl2":"4. Generate valid STAC collection"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-4-1-add-items-to-collection","position":20},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.1. Add items to collection","lvl2":"4. Generate valid STAC collection"},"content":"Once the collection is created read all the items from disk and add the necassary links.\n\nimport glob\nfor fpath in glob.glob('examples/*'):\n    collection.add_item(pystac.Item.from_file(fpath))\n\n","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-1-add-items-to-collection","position":21},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.2 Save the normalised collection","lvl2":"4. Generate valid STAC collection"},"type":"lvl3","url":"/creating-stac-catalog-from-prr-example#id-4-2-save-the-normalised-collection","position":22},{"hierarchy":{"lvl1":"Generating metadata for the SRAL Processing over Land Ice Dataset","lvl3":"4.2 Save the normalised collection","lvl2":"4. Generate valid STAC collection"},"content":"\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href='../data/example_catalog_ampli/',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)","type":"content","url":"/creating-stac-catalog-from-prr-example#id-4-2-save-the-normalised-collection","position":23},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview"},"type":"lvl1","url":"/prr-stac-download-example","position":0},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview"},"content":"This notebook has been created to support the access to the users of EarthCODE and APEX, who would like to exploit available products and project results stored in the \n\nESA Project Results Repository (PRR). PRR provides access to data, workflows, experiments and documentation from ESA EOP-S Projects organised across Collections, accessible via \n\nOGC Records e S\n\nTAC API.\n\nEach collection contains \n\nSTAC Items, with their related assets stored within the PRR storage.\n\nScientists/commercial companies can access the PRR via the \n\nEarthCODE and \n\nAPEx projects.\n\nUse following notebook cells to preview the content of the ESA PRR and request the download of selected products.\n\n","type":"content","url":"/prr-stac-download-example","position":1},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Loading Libraries and set up logging level"},"type":"lvl3","url":"/prr-stac-download-example#loading-libraries-and-set-up-logging-level","position":2},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Loading Libraries and set up logging level"},"content":"\n\nimport os\nimport logging\nimport pprint\nimport shutil\nfrom urllib.parse import urljoin\nfrom urllib.request import urlretrieve\n\n#Make sure you have installed pystac_client before running this\nfrom pystac_client import Client\n\n# set pystac_client logger to DEBUG to see API calls\nlogging.basicConfig()\nlogger = logging.getLogger(\"pystac_client\")\nlogger.setLevel(logging.DEBUG)\n\n\n","type":"content","url":"/prr-stac-download-example#loading-libraries-and-set-up-logging-level","position":3},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Connect to ESA PRR Catalog and display the list of collections available"},"type":"lvl3","url":"/prr-stac-download-example#connect-to-esa-prr-catalog-and-display-the-list-of-collections-available","position":4},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Connect to ESA PRR Catalog and display the list of collections available"},"content":"\n\n# URL of the STAC Catalog to query\ncatalog_url = \"https://eoresults.esa.int/stac\"\n\n# custom headers\nheaders = []\n\ncat = Client.open(catalog_url, headers=headers)\ncat # display the basic informaiton about PRR Catalog in STAC Format\n\n\n\nUse the cell below to access entire list of collections available in ESA PRR.\n\ncollection_search = cat.collection_search(limit=150)\nprint(f\"Total number of collections found in ESA PRR is {collection_search.matched()}\")\n\n# Display the name of the names of collection (collection-ids) to be used to filter the colleciton of interest\nfor collection in collection_search.collections_as_dicts():\n    print(collection.get(\"id\", \"Unnamed Collection\"))\n\n\n\nAlternatively, you can display the metadata of all STAC Collections available\n\n# Or they can be displayed with their full metadata\ncollection_search = cat.collection_search(\n    datetime='2023-04-02T00:00:00Z/2024-08-10T23:59:59Z',  #this is an additional filter to be added to filter the collections based on the date.\n    limit=10\n)\nprint(f\"{collection_search.matched()} collections found\")\nprint(\"PRR available Collections\\n\")\n\nfor results in collection_search.collections_as_dicts():  # maybe this part should not display entire dic\n    pp = pprint.PrettyPrinter(depth=4)\n    pp.pprint(results)\n\n","type":"content","url":"/prr-stac-download-example#connect-to-esa-prr-catalog-and-display-the-list-of-collections-available","position":5},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Open Sentinel-3 AMPLI Ice Sheet Elevation collection"},"type":"lvl3","url":"/prr-stac-download-example#open-sentinel-3-ampli-ice-sheet-elevation-collection","position":6},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Open Sentinel-3 AMPLI Ice Sheet Elevation collection"},"content":"\n\nTo access specific collection, we will use the collection id from the cell above. Type sentinel3-ampli-ice-sheet-elevation to connect to selected collection and display its metadata.\n\ncollection = cat.get_collection(\"sentinel3-ampli-ice-sheet-elevation\") # place here the id of the selected collection\n#collection # or use simply json metadata to display the information \nprint(\"PRR Sentinel-3 AMPLI Collection\\n\")\npp = pprint.PrettyPrinter(depth=4)\npp.pprint(collection.to_dict())\n\n#Or display it in the STAC file format to better visualise the attributes and properties \ncollection\n\n\n\nFrom the cell below, we will retrieve and explore queryable fields from a STAC API, which allows us to understand what parameters we can use for filtering our searches.\n\nqueryable = collection.get_queryables()\n\npp = pprint.PrettyPrinter(depth=4)\npp.pprint(queryable)\n\n","type":"content","url":"/prr-stac-download-example#open-sentinel-3-ampli-ice-sheet-elevation-collection","position":7},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Display STAC Items from Sentinel-3 AMPLI Ice Sheet Elevation collection"},"type":"lvl3","url":"/prr-stac-download-example#display-stac-items-from-sentinel-3-ampli-ice-sheet-elevation-collection","position":8},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl3":"Display STAC Items from Sentinel-3 AMPLI Ice Sheet Elevation collection"},"content":"\n\nBy executing the cell below you will get the ids of items that can be found in the specific collection (requested above).\nFirst five items from the list are printed out.\n\nitems = collection.get_items()\n\n# flush stdout so we can see the exact order that things happen\ndef get_five_items(items):\n    for i, item in enumerate(items):\n        print(f\"{i}: {item}\", flush=True)\n        if i == 4:\n            return\n        \nprint(\"First page\", flush=True)\nget_five_items(items)\n\nprint(\"Second page\", flush=True)\nget_five_items(items)\n\nNow execute a search with a set of parameters. In this case it returns just one item because we filter on one queryable parameter (id)\n\n#Search for items based on spatio-temporal properties\n\n# AOI entire world\ngeom = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-180, -90],\n            [-180, 90],\n            [180 , 90],\n            [180, -90],\n            [-180, -90],\n        ]\n    ],\n}\n\n# limit sets the # of items per page so we can see multiple pages getting fetched\n#In this search we apply also filtering on ID that is one of the searchable parameters for the colletion\nsearch = cat.search(\n    max_items=7,\n    limit=5,\n    collections=\"sentinel3-ampli-ice-sheet-elevation\",        # specify collection id\n    intersects=geom,\n    query={\"id\": {\"eq\": \"sentinel-3a-antarctica-cycle107\"}},  # search for the specific Item in the collection \n    datetime=\"2023-04-02T00:00:00Z/2024-08-10T23:59:59Z\",     # specify the start and end date of the time frame to perform the search \n)\n\nitems = list(search.items())\n\nprint(len(items))\n\npp = pprint.PrettyPrinter(depth=4)\npp.pprint([i.to_dict() for i in items])\n\n\n\nIf you do not know the item id, search through available satellite instrument name, region, number of the cycle and the datetime range of the products of interest. \nYou can specify them by filtering based on following possible values: \n\nmissions: 3a or 3b\n\nregions: anarctica or greenland\n\ncycle range: for sentinel-3a possible cycle range is from 005 to 112; while sentinel-3b has range from 011-093\n\ndatetime: specify the time frame of the products from the range between: 2016-06-01 00:00:00 UTC – 2024-05-09 00:00:00 UTC \n\n#Search for items from specific mission and type of the instrument (based on the id) and the region as well as cycle number \n# Define your cycle range and mission types\ncycle_range = [f\"{i:03d}\" for i in range(90, 111)] #005 to 111   # for sentinel-3a possible cycle range is from 005 to 111; while s3b has range from 011-092\nmissions = [\"3b\"]          # select the mission and sensor type from:\"sentinel-3a\" or \"sentinel-3b\"]  \nregions = [\"antarctica\"]              # specify the region from: \"antarctica\" or \"greenland\"\n\n# AOI entire world\ngeom = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-180, -90],\n            [-180, 90],\n            [180 , 90],\n            [180, -90],\n            [-180, -90],\n        ]\n    ],\n}\n\n# limit sets the # of items per page so we can see multiple pages getting fetched\n#In this search we apply also filtering on ID that is one of the searchable parameters for the colletion\nsearch = cat.search(\n    max_items=7,\n    limit=5,\n    collections=\"sentinel3-ampli-ice-sheet-elevation\",\n    intersects=geom,  # search for the specific Item in the collection \n    datetime=\"2021-04-02T00:00:00Z/2024-08-10T23:59:59Z\",     # specify the start and end date of the time frame to perform the search which are: 2016-06-01 00:00:00 UTC – 2024-05-09 00:00:00 UTC\n)\nitems = list(search.items())\nprint(f\"Number of items found: {len(items)}\")\nprint(items)\n\npp = pprint.PrettyPrinter(depth=4)\n\nfiltered = [\n    item for item in items\n    if any(m in item.id.lower()  for m in missions)\n    and any(r in item.id.lower()  for r in regions)\n    and any(f\"cycle{c}\" in item.id.lower() for c in cycle_range)\n]\n\n\n#for i, item in enumerate(filtered, 2):\n   # print(f\"{i}. {item.id} @ {item.datetime}\")\n\n## Print number of filtered items\nprint(f\"Number of filtered items: {len(filtered)}\")\nfor i, item in enumerate(filtered, 2):\n    print(f\"{i}. {item.id} @ {item.datetime}\")\n\n","type":"content","url":"/prr-stac-download-example#display-stac-items-from-sentinel-3-ampli-ice-sheet-elevation-collection","position":9},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download all assets from the selected item "},"type":"lvl2","url":"/prr-stac-download-example#download-all-assets-from-the-selected-item","position":10},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download all assets from the selected item "},"content":"Based on the selection done in the previous cell, download the products to the downloads folder in your workspace\n\nbase_url = \"https://eoresults.esa.int\"\n\nitem_to_be_downloaded = 3\ntarget = items[item_to_be_downloaded]\n\noutput_dir = f\"downloads/{target.id}\"\nos.makedirs(output_dir, exist_ok=True)\n\nassets_total=len(target.assets.items())\nassets_current=0\nfor asset_key, asset in target.assets.items():\n    filename = os.path.basename(asset.href)\n    full_href = urljoin(base_url, asset.href)\n    local_path = os.path.join(output_dir, filename)\n    assets_current+=1\n    print(f\"[{assets_current}/{assets_total}] Downloading {filename}...\")\n    try:\n        urlretrieve(full_href, local_path)\n    except Exception as e:\n        print(f\"Failed to download {full_href}. {e}\")\n\n\n","type":"content","url":"/prr-stac-download-example#download-all-assets-from-the-selected-item","position":11},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download filtered items "},"type":"lvl2","url":"/prr-stac-download-example#download-filtered-items","position":12},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"Download filtered items "},"content":"Based on the selection done in the previous cell, download the products to the downloads folder in your workspace. You will download here the items which result from further filtering options (by mission type, cycle number, region etc.)\n\ntarget = filtered[0] if len(filtered) > 0 else None\n\noutput_dir = f\"downloads/{target.id}\"\nos.makedirs(output_dir, exist_ok=True)\n\nassets_total=len(target.assets.items())\nassets_current=0\nfor asset_key, asset in target.assets.items():\n    filename = os.path.basename(asset.href)\n    full_href = urljoin(base_url, asset.href)\n    local_path = os.path.join(output_dir, filename)\n    assets_current+=1\n    print(f\"[{assets_current}/{assets_total}] Downloading {filename}...\")\n    try:\n        urlretrieve(full_href, local_path)\n    except Exception as e:\n        print(f\"Failed to download {full_href}. {e}\")     \n\nbase_url = \"https://eoresults.esa.int\"\nfor index, item in enumerate(filtered, 2):\n    output_dir = f\"filtered/{item.id}\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    assets_total = len(item.assets.items())\n    assets_current = 0\n\n    for asset_key, asset in item.assets.items():\n        filename = os.path.basename(asset.href)\n        full_href = urljoin(base_url, asset.href)\n        local_path = os.path.join(output_dir, filename)\n\n        assets_current += 1\n        print(f\"[{index}] [{assets_current}/{assets_total}] Downloading {filename} for item {item.id}...\")\n\n        try:\n            urlretrieve(full_href, local_path)\n        except Exception as e:\n            print(f\"Failed to download {full_href}. {e}\")\n\nprint(f\"Downloaded assets for {len(filtered)} items.\")\n\n","type":"content","url":"/prr-stac-download-example#download-filtered-items","position":13},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Read some data to ensure all items are downloaded properly"},"type":"lvl2","url":"/prr-stac-download-example#id-optional-read-some-data-to-ensure-all-items-are-downloaded-properly","position":14},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Read some data to ensure all items are downloaded properly"},"content":"\n\nimport xarray as xr\nimport numpy as np\n\n# change this to a downloaded file\nexample_filepath = f'./downloads/{target.id}/S3A_SR_2_TDP_LI_20240403T201315_20240403T201615_20250416T191921_0180_111_014______CNE_GRE_V001.nc'\n\n# Open selected product and check the values\n# Note: You can select another group of values to read : satellite_and_altimeter, or ESA_L2_processing\nds = xr.open_dataset(example_filepath, group='AMPLI_processing')\nvalues = ds['elevation_radar_ampli'].values\nvalues[~np.isnan(values)]\n\n","type":"content","url":"/prr-stac-download-example#id-optional-read-some-data-to-ensure-all-items-are-downloaded-properly","position":15},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Create an archive of products downloaded"},"type":"lvl2","url":"/prr-stac-download-example#id-optional-create-an-archive-of-products-downloaded","position":16},{"hierarchy":{"lvl1":"ESA Project Results Repository (PRR) Data Access and Collections Preview","lvl2":"(Optional) Create an archive of products downloaded"},"content":"\n\nCreate an archive of the products downloaded to your workspace and save them in .zip format to make them compressed\n\n# Create an archive of downloaded products \nzip_path = shutil.make_archive(output_dir, 'zip', root_dir=output_dir)\nprint(f\"Created ZIP archive: {zip_path}\")","type":"content","url":"/prr-stac-download-example#id-optional-create-an-archive-of-products-downloaded","position":17},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset"},"type":"lvl1","url":"/prr-stac-introduction","position":0},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset"},"content":"","type":"content","url":"/prr-stac-introduction","position":1},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Introduction"},"type":"lvl3","url":"/prr-stac-introduction#introduction","position":2},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Introduction"},"content":"\n\nThis notebook has been created to show the core steps required of EarthCODE users to upload their research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with a single  netcdf file. Checkout the other projects on the webpage for more complex examples.\n\nPRR provides access to data, workflows, experiments and documentation from ESA Projects organised across Collections, accessible via the \n\nSTAC API. Each Collection contains \n\nSTAC Items, with their related Assets stored within the PRR storage. Scientists/commercial companies can access the PRR via the \n\nEarthCODE and \n\nAPEx projects.\n\nThe \n\nSTAC Specification, provides detailed explanation and more information on this metadata format.\n\nIn order to upload data to the ESA Project Results Repository (PRR) you have to generate a STAC Collection that is associated to your files. The STAC Collection provides metadata about your files and makes them searchable and machine readable. The metadata generation process is organised in four steps process:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the Collection\n\nSave the normalised Collection\n\nThe easiest way to generate all the required files is to use a STAC library, such as pystac or riostac. This library will take care of creating the links and formating the files in the correct way.  In the examples below we are using pystac.\n\nHave a look at the steps below and learn how to prepare your dataset to generate a valid STAC Collection. You will find all the steps descibed in the markdown cell, together with the example code (executable) to make this process easier. Please adjust the information in the fields required to describe your Collection and Items according to the comments, starting with : “#”\n\nNOTE: Depending on the information that you put in the Assets or Items the code, you may get an error about an object not being json-serialisable. If this happens, you have to transform the problem field into an object that can be described using standard JSON. For example, transforming a numpy array into a list.\n\n","type":"content","url":"/prr-stac-introduction#introduction","position":3},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"type":"lvl3","url":"/prr-stac-introduction#id-example-4datlantic-ohc-project","position":4},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"content":"The code below demonstrates how to perform the necessary steps using real data from the ESA Regional Initiative Project 4DATLANTIC-OHC. The project focuses on ocean heat content and provides monthly gridded Atlantic Ocean heat content change as well as OHC trends and their uncertainties.\n\n🔗 Learn more about the project here: \n\n4DATLANTIC-OHC – EO4Society \n🔗 Check the project website: \n\n4DATLANTIC-OHC – Website","type":"content","url":"/prr-stac-introduction#id-example-4datlantic-ohc-project","position":5},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl4":"Acknowledgment","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"type":"lvl4","url":"/prr-stac-introduction#acknowledgment","position":6},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl4":"Acknowledgment","lvl3":"🌊 Example: 4DATLANTIC-OHC Project"},"content":"We gratefully acknowledge the 4DATLANTIC-OHC project team for providing access to the data used in this example.\n\nThis example is intended to help you understand the workflow and apply similar steps to your own Earth observation data analysis. \n\n","type":"content","url":"/prr-stac-introduction#acknowledgment","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Import necessary Python libraries"},"type":"lvl3","url":"/prr-stac-introduction#import-necessary-python-libraries","position":8},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"Import necessary Python libraries"},"content":"You can create an example conda/miniconda enviroment to run the below code using:conda create -n prr_stack_example pystac xarray shapely\nconda activate prr_stack_example\n\n# import libraries\nfrom pystac import Collection\nimport pystac\nimport xarray as xr\nimport shapely\nimport json\nfrom datetime import datetime\n\n","type":"content","url":"/prr-stac-introduction#import-necessary-python-libraries","position":9},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"1. Generate a root STAC collection"},"type":"lvl3","url":"/prr-stac-introduction#id-1-generate-a-root-stac-collection","position":10},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"1. Generate a root STAC collection"},"content":"The root STAC Collection provides a general description of the enitre dataset, that you would like to store in ESA PRR. In the STAC Specification a Collection is defined as  an extension of the STAC Catalog with additional information such as the extents, license, keywords, providers, etc that describe STAC Items that fall within the Collection. \n\nIn short: it behaves as the container to store the various Items that build up your dataset. \n\nSTAC Collection has some required fields that you need to provide in order to build its valid description. Most of these metadata fields should be extracted from your data.\nPlease have a look at the example below.{\n  \"type\": \"Collection\", # Do not change\n  \"id\": \"\", # add a unique variation of project name + dataset name \n  \"stac_version\": \"1.1.0\", # Do not change\n  \"title\": \"\", # Meaningful title of your dataset\n  \"description\": \"\", # General description of your dataset\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          -180.0,\n          -90.0,\n          180.0,\n          90.0\n        ]\n      ]\n    }, # Spatial extent of your dataset. If you have multiple data files take the minimum bounding box that covers all.\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"1982-01-01T00:00:00Z\",\n          \"2022-12-31T23:59:59Z\"\n        ] # Temporal extent of your dataset. If you have multiple data files take the minimum temporal range that covers all.\n      ]\n    }\n  },\n\"license\": \"\", # the license that applies to entire dataset\n\"links\": [] # do not change\n\n}\n\n","type":"content","url":"/prr-stac-introduction#id-1-generate-a-root-stac-collection","position":11},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create Collection","lvl3":"1. Generate a root STAC collection"},"type":"lvl5","url":"/prr-stac-introduction#example-create-collection","position":12},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create Collection","lvl3":"1. Generate a root STAC collection"},"content":"\n\n# define collection id, since it will be reused\ncollectionid = \"4datlantic-ohc\"\n\n# create the root collection using pystac.Collection\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Atlantic Ocean heat content change\",\n  \"description\": \"Given the major role of the Atlantic Ocean in the climate system, it is essential to characterize the temporal and spatial variations of its heat content. The OHC product results from the space geodetic approach also called altimetry-gravimetry approach. This dataset contains variables as 3D grids of ocean heat content anomalies at 1x1 resolution and monthly time step. Error variance-covariance matrices of OHC at regional scale and annual resolution are also provided. See Experimental Dataset Description for details: https://www.aviso.altimetry.fr/fileadmin/documents/data/tools/OHC-EEI/OHCATL-DT-035-MAG_EDD_V2.0.pdf.Version. V2-0 of Dataset published 2022 in Centre National d’Etudes Spatiales. This dataset has been produced within the framework of the 4DAtlantic-Ocean heat content Project funded by ESA.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-100, \n         -90, \n         25,\n         90]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2002-04-15T18:07:12Z\",\n          \"2023-09-01T18:59:59Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"Aviso License\",\n  \"links\": []\n\n}\n\n)\n\ncollection\n\n","type":"content","url":"/prr-stac-introduction#example-create-collection","position":13},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl3","url":"/prr-stac-introduction#id-2-group-your-dataset-files-into-stac-items-and-stac-assets","position":14},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"The second step is to describe the different files as Items and Assets. This is the most time-consuming step. There are multiple strategies for doing this and it is up to you to decide how to do it. The main consideration should be usability of the data.\n\nFor example:\n\nMicrosoft Planatery Computer groups its Sentinel-2 data into Items which represent individual regions, and each Item has 13 Assets each representing a band - \n\nhttps://​stacindex​.org​/catalogs​/microsoft​-pc​#​/43bjKKcJQfxYaT1ir3Ep6uENfjEoQrjkzhd2​?cp​=​1​&​t​=5 .\n\nThe California Forest Observatory (on Google Earth Engine) groups its data into Items, where each Item represents a specific year, data type and resolution for the whole study area. Each Item has only one Asset ( dataset ) associated with it - \n\nhttps://​stacindex​.org​/catalogs​/forest​-observatory​#​/4dGsSbK8F5jjmhRZYE6kjUMmgWCUKe6J2qqw​?t​=2.\n\nA More complex example from real-data from ESA-funded project: \n\nESA Projects Results Repository, gives the researchers flexibility in terms on how their datasets will be grouped into Items and Assets. You may need to consider that the more Items you have in your Collection, the slower the browsing would be if the user would like to browse through the publicly open STAC Browser. Please have a look at one example, that provides one Sentinel-3 AMPLI Ice Sheet Elevation Collection with around 400 Items complemented by around 360 Assets each.\n\n\nhttps://​eoresults​.esa​.int​/browser​/​#​/external​/eoresults​.esa​.int​/stac​/collections​/sentinel3​-ampli​-ice​-sheet​-elevation\n\nMore general examples about creating STAC catalogs are available here - \n\nhttps://​github​.com​/stac​-utils​/pystac​/tree​/main​/docs​/tutorials.\n\nThe easiest way to generate the required STAC Items is to copy over the metadata directly from your files.\n\n","type":"content","url":"/prr-stac-introduction#id-2-group-your-dataset-files-into-stac-items-and-stac-assets","position":15},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Open Dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl5","url":"/prr-stac-introduction#example-open-dataset","position":16},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Open Dataset","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"\n\n# open dataset\n\n# define relative filepath within the folder structure you want to upload to the PRRs\nfilepath = 'https://data.aviso.altimetry.fr/aviso-gateway/data/indicators/OHC_EEI/4DAtlantic_OHC/OHC_4DATLANTIC_200204_202212_V2-0.nc'\n\nds = xr.open_dataset(filepath + '#mode=bytes')\nds\n\n# helper function to convert numpy arrays to lists\nimport numpy as np\ndef convert_to_json_serialisable(attrs):\n    attrs = attrs.copy()\n    for attr in attrs.keys():\n        if isinstance(attrs[attr], np.ndarray):\n            attrs[attr] = attrs[attr].tolist()\n    return attrs\n\n# sometimes attributes are not json serialisable, so we convert them to JSON serialisable formats\nfor var in ds.data_vars:\n    ds[var].attrs = convert_to_json_serialisable(ds[var].attrs)\n\nfrom xstac import xarray_to_stac\n\n","type":"content","url":"/prr-stac-introduction#example-open-dataset","position":17},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create valid STAC Item from your product (nc)","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"type":"lvl5","url":"/prr-stac-introduction#example-create-valid-stac-item-from-your-product-nc","position":18},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Example | Create valid STAC Item from your product (nc)","lvl3":"2.  Group your dataset files into STAC Items and STAC Assets"},"content":"\n\n# Describe the first file following the datacube stac extension standards.\n# All data is extracted from the metadata / data already present in the file we only specify\n# the template and what information is extracted\n\nbbox = [ds['longitude'].values.min(), ds['latitude'].values.min(), ds['longitude'].values.max(), ds['latitude'].values.max(), ]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\ntemplate = {\n\n    \"id\": f\"{collectionid}-{'OHC_4DATLANTIC'.lower()}\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.1.0\",\n    \"description\": ds.attrs['summary'],\n    \"title\": 'OHC 4D Atlantic',\n    \"properties\": {\n            \"history\": ds.attrs['history'],\n            \"source\": ds.attrs['source'],\n            \"comment\": ds.attrs['comment'],\n            \"references\": ds.attrs['references'],\n            \"version\": ds.attrs['version'],\n            \"conventions\": ds.attrs['Conventions'],\n            \"contact\": ds.attrs['contact'],\n            \"start_datetime\": ds.attrs['start_date'] + 'T00:00:00Z',\n            \"end_datetime\": ds.attrs['end_date'] + 'T00:00:00Z',\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": f\"./{collectionid}/OHC_4DATLANTIC_200204_202212_V2-0.nc\",  # or local path\n            \"type\": \"application/x-netcdf\",\n            \"roles\": [\"data\"],\n            \"title\": 'OHC 4D Atlantic'\n        }\n    }\n}\n\n# 3. Generate the STAC Item\nitem = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\" if 'time' in ds.coords else False,\n    x_dimension='longitude',\n    y_dimension='latitude',\n    reference_system=False\n)\n\nitem # Preview created Item\n\n","type":"content","url":"/prr-stac-introduction#example-create-valid-stac-item-from-your-product-nc","position":19},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"3. Add the STAC Item to the STAC Collection"},"type":"lvl2","url":"/prr-stac-introduction#id-3-add-the-stac-item-to-the-stac-collection","position":20},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"3. Add the STAC Item to the STAC Collection"},"content":"Adding the Items to the Collection is a single function call when using a library such as pystac.\n\ncollection.add_item(item)\n\n","type":"content","url":"/prr-stac-introduction#id-3-add-the-stac-item-to-the-stac-collection","position":21},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"4. Save the Collection"},"type":"lvl2","url":"/prr-stac-introduction#id-4-save-the-collection","position":22},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl2":"4. Save the Collection"},"content":"Again this step is a single function call.\n\ncollection.normalize_and_save(\n    root_href='example_4datlantic/', # path to the self-contained folder with STAC Collection\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/prr-stac-introduction#id-4-save-the-collection","position":23},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Congratulations, you have created your first STAC Collection. ","lvl2":"4. Save the Collection"},"type":"lvl5","url":"/prr-stac-introduction#congratulations-you-have-created-your-first-stac-collection","position":24},{"hierarchy":{"lvl1":"Generating STAC metadata for the OHC 4d Atlantic dataset","lvl5":"Congratulations, you have created your first STAC Collection. ","lvl2":"4. Save the Collection"},"content":"Now, you have your results ready to be ingested into ESA PRR. To request data storage in ESA PRR, contact EarthCODE team at: \n\nearth-code@esa.int and provide following information:\n\nyour project name\n\ntotal size of your dataset\n\nlink to STAC Collection created together with associated Items (e.g. entire example_4datlantic folder) - can be provided as a .zip or link to online repository / GitHub public repository\n\nlink to the datasets (access link to final outcomes of the project or assets)\n\nspecify any restrictions related to the access of your dataset.\n\nin the email, do not forget to CC your ESA TO to acknowledge that the dataset will be imported into PRR.\n\nOnce the email is received, the EarthCODE team will make a request to publish your product into PRR on your behalf (in the future the self-ingestion system will be supported).\n\nOnce the collection is imported you will receive a dedicated URL to your products, which you can use to create the record on Open Science Data Catalogue to make your data discoverable or/and request a DOI for your dataset (at the moment this has to be done by external service of your choice).","type":"content","url":"/prr-stac-introduction#congratulations-you-have-created-your-first-stac-collection","position":25},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset"},"type":"lvl1","url":"/tccas-v2","position":0},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with multiple data files of different types.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project Terrestrial Carbon Community Assimilation System (TCCAS). The focus of TCCAS is the combination of a diverse array of observational data streams with the D&B terrestrial biosphere model into a consistent picture of the terrestrial carbon, water, and energy cycles.\n\n🔗 Check the project website: \n\nTerrestrial Carbon Community Assimilation System (TCCAS) – Website\n\n🛢️ TCCAS Dataset: \n\nTerrestrial Carbon Community Assimilation System (TCCAS) – Data base: Sodankylä and Lapland region","type":"content","url":"/tccas-v2","position":1},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"Acknowledgment"},"type":"lvl3","url":"/tccas-v2#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"Acknowledgment"},"content":"We gratefully acknowledge the Terrestrial Carbon Community Assimilation System (TCCAS) team for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport pandas as pd\n\n","type":"content","url":"/tccas-v2#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl2":"2. Define functions to be reused between regions"},"type":"lvl2","url":"/tccas-v2#id-2-define-functions-to-be-reused-between-regions","position":4},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl2":"2. Define functions to be reused between regions"},"content":"\n\ndef add_insitu_data(collectionid, geometry, bbox, start_time, collection, insitu_href):\n    print(start_time)\n    item = Item(\n        id=f\"{collectionid}-insitu_package\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": 'Insitu package with FloX, VOD and Miscellaneous field datasets related to the TCCAS project. ',\n        }\n    )\n\n    # 3. add an asset (the actual link to the file)\n    item.add_asset(\n                key=f'Insitu package', # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/{insitu_href}',\n                    media_type=\"application/tar+gzip\",\n                    roles=[\"data\"],\n                )\n    )\n\n    item.validate()\n    collection.add_item(item)\n\ndef add_documentation_item(collectionid, geometry, bbox, start_time, collection, ):\n    # add all the documentation under a single item\n    item = Item(\n        id=f\"{collectionid}-documentation\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": 'Documentation for the TCCAS project datasets.',\n        }\n    )\n\n    item.add_asset(\n                key=f'TCCAS user manual.', # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/TCCAS_manual.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    item.add_asset(\n                key=\"Satellite Data Uncertainty analysis Scientific Report\", # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/D7.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    item.add_asset(\n                key=\"Campaign Data User Manual\", # title can be arbitrary\n                asset=pystac.Asset(\n                    href=f'./{collectionid}/D11_CDUM-all_sites.pdf',\n                    media_type=\"application/pdf\",\n                    roles=[\"documentation\"],\n                )\n    )\n\n    collection.add_item(item)\n\n# add an item with multiple model forcing assets\n\ndef create_model_forcing_item(collectionid, geometry, bbox, start_time, collection, model_forcing):\n\n    item = Item(\n        id=f\"{collectionid}-model_forcing\",\n        geometry=geometry,\n        datetime=start_time,\n        bbox=bbox,\n        properties= {\n            \"license\": \"CC-BY-4.0\",\n            \"description\": ' Regional and Site-level model forcing Data Sets for Sodankylä and Lapland region, part of the TCCAS project.',\n        }\n    )\n\n    for k,v in model_forcing.items():\n        item.add_asset(\n                    key=k, # title can be arbitrary\n                    asset=pystac.Asset(\n                        href=f'./{collectionid}/{v}',\n                        media_type=\"application/x-netcdf\",\n                        roles=[\"data\"],\n                    )\n        )\n\n    item.validate()\n    collection.add_item(item)\n\n\n# some attributes extracted from xarray are not json serialisable and have to be cast to other types.\ndef convert_to_json_serialisable(attrs):\n    attrs = attrs.copy()\n    for attr in attrs.keys():\n        if isinstance(attrs[attr], np.ndarray):\n            attrs[attr] = attrs[attr].tolist()\n        elif str(type(attrs[attr])).__contains__('numpy.int'):\n            attrs[attr] = int(attrs[attr])\n    return attrs\n\ndef create_items_from_data(collectionid, geometry, bbox, collection, root_url, data_files, region):\n\n    for dataset_name, dataset_filepath in data_files.items():\n\n        # 1. open the netcdf file\n        ds = xr.open_dataset(root_url + dataset_filepath + '#mode=bytes')\n\n        if 'time' in ds.coords:\n            start_time = ds['time'][0].values\n            ts = (start_time - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n            start_time = datetime.fromtimestamp(ts)\n\n            end_time = ds['time'][-1].values\n            ts = (end_time - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n            end_time = datetime.fromtimestamp(ts)\n\n        elif 'yymmddHH' in ds.variables:\n            string_date = '-'.join(ds['yymmddHH'][0].values.astype(str)[:3])\n            start_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n            string_date = '-'.join(ds['yymmddHH'][-1].values.astype(str)[:3])\n            end_time = datetime.strptime(string_date, '%Y-%m-%d')\n        else:\n            string_date = '-'.join(ds['yymmddHHMMSS'][0].values.astype(int).astype(str)[:3])\n            start_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n            string_date = '-'.join(ds['yymmddHHMMSS'][0].values.astype(int).astype(str)[:3])\n            end_time = datetime.strptime(string_date, '%Y-%m-%d')\n\n        if 'description' in ds.attrs:\n            description = ds.attrs['description']\n        else:\n            description = f'Dataset with variables related to {dataset_name}.'\n            \n        template = {\n\n            \"id\": f\"{collection.id}-{dataset_name.lower().replace(' ', '_')}\",\n            \"type\": \"Feature\",\n            \"stac_version\": \"1.0.0\",\n            \"properties\": {\n                \"title\": dataset_name,\n                \"description\": description,\n                \"start_datetime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                \"end_datetime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                \"region\": region,\n            },\n            \"geometry\": geometry,\n            \"bbox\": bbox,\n            \"assets\": {\n                \"data\": {\n                    \"href\": f\"./{collectionid}/{dataset_filepath.split('/')[-1]}\",  # or local path\n                    \"type\": \"application/x-netcdf\",\n                    \"roles\": [\"data\"],\n                    \"title\": dataset_name\n                }\n            }\n        }\n\n        # remove numpy values from attrs:\n        for var in ds.variables:\n            ds[var].attrs = convert_to_json_serialisable(ds[var].attrs)\n\n        if 'lon' in ds.coords:\n            x_dim, y_dim = 'lon', 'lat'\n        if 'longitude' in ds.coords:\n            x_dim, y_dim ='longitude', 'latitude'\n        elif 'x' in ds.coords:\n            x_dim, y_dim = 'x', 'y'\n        else:\n            x_dim, y_dim = False, False\n        \n        # 3. Generate the STAC Item\n        item = xarray_to_stac(\n            ds,\n            template,\n            temporal_dimension=\"time\" if 'time' in ds.variables else False,\n            x_dimension=x_dim,\n            y_dimension=y_dim,  \n            reference_system=False\n        )\n\n        # validate and add the STAC Item to the collection\n        item.validate()\n\n        # manually add license after validation since its new\n        item.properties['license'] = ds.attrs['license']\n        collection.add_item(item)\n    \n\n","type":"content","url":"/tccas-v2#id-2-define-functions-to-be-reused-between-regions","position":5},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Sodankylae and Lapland’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-sodankylae-and-lapland-data-and-links","position":6},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Sodankylae and Lapland’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'sodankylae'\ncollectionid = \"tccas-sodankylae\"\nbbox = [18.00, 65.00, 32.00, 69.00]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Fraction of absorbed Photosynthetic Active Radiation Leaf Area Index (JRC-TIP)\": \"/jrc-tip/jrctip_fapar-lai_sodankyla_20110101-20220105.nc\",\n    \"Brightness temperature (SMOS TB)\": \"smos/smos_l3tb/SMOS_L3TB__sodankyla.nc\",\n    \"Soil moisture and Vegetation Optical Depth (SMOS SM and SMOS L-VOD)\": \"smos/smosL2/smosL2_1D_v700_sodankyla_trans.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (Sentinel 5P)\": \"sif/tropomi/Sodankyla_SIF_TROPOMI_final.nc4\",\n    \"Slope (ASCAT Slope)\": \"ascat/local_slope.final/ASCAT_slope_so.nc\",\n    \"Photochemical Reflectance Index (MODIS PRI)\": \"modis/final/PRI_ESTIMATE_SODANKYLA_SINUSOIDAL.nc\",\n    \"Land Surface Temperature (MODIS LST)\": \"modis/final/LST_ESTIMATE_SODANKYLA_SINUSOIDAL.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO-2 SIF)\": \"sif/oco2/Sodankyla_SIF_OCO2_final.nc4\",\n    \"Vegetation Optical Depth (AMSR-2 VOD)\": \"amsr2/final/AMSR2_so.nc\"\n} \n\nmodel_forcing = {\n    \"static-site-level\": \"FI-Sod_staticforcing.nc\",\n    \"time-dependent (ERA5) - site level\": \"FI-Sod_dynforcing-era5_20090101-20211231_with-lwdown.nc\",\n    \"time-dependent (in-situ) - site level\": \"FI-Sod_dynforcing-insitu_20090101-20211231_with-insitu-lwdown.nc\",\n    \"static-regional\": \"sodankyla-region_cgls-pft-crops-redistributed_staticforcing.nc\",\n    \"time-dependent (ERA5) - regional\": \"sodankyla-region_dynforcing_era5_2009-2021.nc\"\n}\n\n\n\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for Lapland and Sodankyla region\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for Sodankylä and Lapland region.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          18.00,\n          65.00,\n          32.00,\n          69.00\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'sodankyla-insitu-package.tgz',\n)\n\ncreate_model_forcing_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    model_forcing)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/tccas-v2#id-2-2-define-sodankylae-and-lapland-data-and-links","position":7},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Netherlands region (including Reusel)’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-netherlands-region-including-reusel-data-and-links","position":8},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Netherlands region (including Reusel)’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'netherlands'\ncollectionid = \"tccas-netherlands\"\nbbox = [4.502, 47.502, 11.477, 51.999]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Vegetation Optical Depth (SMOS)\": \"smos/smosL2/smosL2_1D_v700_reusel_trans.nc\",\n    \"Slope\": \"ascat/local_slope.final/ASCAT_slope_re.nc\",\n    \"Solar Induced Chlorophyll Fluorescence\": \"sif/tropomi/Reusel_SIF_TROPOMI_final.nc4\",\n    \"Soil moisture\": \"smos/smosL2/smosL2_1D_v700_reusel_trans.nc\",\n    \"Brightness temperature\": \"smos/smos_l3tb/SMOS_L3TB__reusel.nc\",\n    \"Vegetation Optical Depth\": \"amsr2/final/AMSR2_re.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO2)\": \"sif/oco2/Reusel_SIF_OCO2_final.nc4\",\n    \"Land Surface Temperature\": \"modis/final/LST_ESTIMATE_REUSEL_SINUSOIDAL.nc\",\n    \"Photochemical Reflectance Index\": \"modis/final/PRI_ESTIMATE_REUSEL_SINUSOIDAL.nc\"\n}\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for the Netherlands region (including Reusel)\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for the Netherlands region, including Reusel.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n         [4.502, 47.502, 11.477, 51.999]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'reusel-insitu-package.tgz',\n)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection\n\n","type":"content","url":"/tccas-v2#id-2-2-define-netherlands-region-including-reusel-data-and-links","position":9},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Majadas del Tietar and Iberian region’ data and links","lvl2":"2. Define functions to be reused between regions"},"type":"lvl3","url":"/tccas-v2#id-2-2-define-majadas-del-tietar-and-iberian-region-data-and-links","position":10},{"hierarchy":{"lvl1":"Generating STAC metadatas for the TCCAS dataset","lvl3":"2.2 Define ‘Majadas del Tietar and Iberian region’ data and links","lvl2":"2. Define functions to be reused between regions"},"content":"\n\n# define dataset names and base url. If the data is locally stored, then you have to adjust these paths.\n\n\n# create the parent collection\nroot_url = 'https://lcc.inversion-lab.com/data/eo/'\nregion = 'iberia'\ncollectionid = \"tccas-iberia\"\nbbox = [-8.496, 38.501, -2.505, 42.997]\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\ndata_time = pd.to_datetime('2021-12-31T00:00:00Z')\n\ndata_files = {\n    \"Vegetation Optical Depth (SMOS)\": \"smos/smosL2/smosL2_1D_v700_lasmajadas_trans.nc\",\n    \"Slope\": \"ascat/local_slope.final/ASCAT_slope_lm.nc\",\n    \"Solar Induced Chlorophyll Fluorescence\": \"sif/tropomi/Majadas_SIF_TROPOMI_final.nc4\",\n    \"Fraction of absorbed Photosynthetic Active Radiation, Leaf Area Index\": \"jrc-tip/jrctip_fapar-lai_majadas_20110101-20220105.nc\",\n    \"Soil moisture\": \"smos/smosL2/smosL2_1D_v700_lasmajadas_trans.nc\",\n    \"Brightness temperature\": \"smos/smos_l3tb/SMOS_L3TB__lasmajadas.nc\",\n    \"Vegetation Optical Depth\": \"amsr2/final/AMSR2_lm.nc\",\n    \"Land Surface Temperature\": \"modis/final/LST_ESTIMATE_MAJADAS_SINUSOIDAL.nc\",\n    \"Photochemical Reflectance Index\": \"modis/final/PRI_ESTIMATE_MAJADAS_SINUSOIDAL.nc\",\n    \"Solar Induced Chlorophyll Fluorescence (OCO2)\": \"sif/oco2/Majadas_SIF_OCO2_final.nc4\"\n}\n\nmodel_forcing = {\n    \"static\": \"ES-LM1_staticforcing.nc\",\n    \"time-dependent (ERA5)\": \"ES-LM1_dynforcing-era5_20090101-20211231_with-lwdown.nc\",\n    \"time-dependent (in-situ)\": \"ES-LM1_dynforcing-insitu_20140401-20220930_with-insitu-lwdown.nc\",\n    \"static-regional\": \"majadas-region_cgls-pft-moli2bare_staticforcing.nc\",\n    \"time-dependent (ERA5)-regional\": \"majadas-region_dynforcing_era5_2009-2021.nc\"\n}\n\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Terrestrial Carbon Community Assimilation System: Database for Iberian region (including Majadas del Tietar)\",\n  \"description\": \"The Terrestrial Carbon Community Assimilation System (TCCAS) is built around the coupled D&B terrestrial biosphere model. D&B has been newly developed based on the well-established DALEC and BETHY models and builds on the strengths of each component model. In particular, D&B combines the dynamic simulation of the carbon pools and canopy phenology of DALEC with the dynamic simulation of water pools, and the canopy model of photosynthesis and energy balance of BETHY. D&B includes a set of observation operators for optical as well as active and passive microwave observations. The focus of TCCAS is the combination of this diverse array of observational data streams with the D&B model into a consistent picture of the terrestrial carbon, water, and energy cycles. TCCAS applies a variational assimilation approach that adjusts a combination of initial pool sizes and process parameters to match the observational data streams. This dataset includes Satelite, Field and model forcing data sets for the Iberian region, including Majadas del Tietar.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [-8.496, 38.501, -2.505, 42.997]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2011-01-01T00:00:00Z\",\n          \"2021-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\ncreate_items_from_data(\n    collectionid,\n    geometry,\n    bbox,\n    collection,\n    root_url,\n    data_files,\n    region,\n)\n\n# add a single item with all the in-situ data, since it comes in a single .tgz file\n\nadd_insitu_data(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    'lasmajadas-insitu-package.tgz',\n)\n\ncreate_model_forcing_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection,\n    model_forcing)\n\nadd_documentation_item(\n    collectionid,\n    geometry,\n    bbox,\n    data_time,\n    collection)\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href=f'../../prr_preview/{collectionid}',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/tccas-v2#id-2-2-define-majadas-del-tietar-and-iberian-region-data-and-links","position":11},{"hierarchy":{"lvl1":"ESA Project Results Repository"},"type":"lvl1","url":"/index-1","position":0},{"hierarchy":{"lvl1":"ESA Project Results Repository"},"content":"The \n\nESA Project Results Repository (PRR) provides long term storage for research outcomes. It provides access to data, workflows, experiments and documentation from ESA Projects organised across Collections, accessible via the \n\nSTAC API. Each Collection contains \n\nSTAC Items, with their related Assets stored within the PRR storage. Scientists/commercial companies can upload data to the PRR via the \n\nEarthCODE and \n\nAPEx projects. Most data in the PRR is open access and anyone can download and use it, subject to the dataset’s particular license.","type":"content","url":"/index-1","position":1},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Uploading data to the PRR"},"type":"lvl2","url":"/index-1#uploading-data-to-the-prr","position":2},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Uploading data to the PRR"},"content":"In order to upload data to the ESA Project Results Repository (PRR) you have to generate a STAC Collection that is associated to your files. The STAC Collection provides metadata about your files and makes them searchable and machine readable. The metadata generation process is organised in four steps process:\n\nGenerate a root STAC Collection\n\nGroup your dataset files into STAC Items and STAC Assets\n\nAdd the Items to the Collection\n\nSave the normalised Collection\n\nSend the data, metadata and some extra information to the EarthCODE team.","type":"content","url":"/index-1#uploading-data-to-the-prr","position":3},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Getting Started with PRR Metadata Creation"},"type":"lvl2","url":"/index-1#getting-started-with-prr-metadata-creation","position":4},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Getting Started with PRR Metadata Creation"},"content":"These notebooks are designed for users who are new to the process of publishing data to the ESA Project Results Repository (PRR). They provide step-by-step guidance on how to generate STAC Collections that meet PRR ingestion requirements.\nWhether you’re working with a single raster file or a large, multi-format dataset, notebooks below will assist you in this process.\n\nGenerating a STAC Collection for the PRR (Introduction) - Detailed explanation on how to create valid metadata to ingest simple raster data file (.nc) into PRR.\n\nGenerating a STAC Collection for the PRR (Multiple file types) - Example how to generate metadata for a more complex dataset with varied data types and formats.\n\nGenerating a STAC Collection for the PRR (Large dataset for multiple regions) - Example focuses on handling large dataset across multiple disjoint regions.\n\nGenerating STAC Collection for the PRR (zarr files) - A guide for generating a collection from zarr files.","type":"content","url":"/index-1#getting-started-with-prr-metadata-creation","position":5},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Accessing and Exploring data products on ESA PRR"},"type":"lvl2","url":"/index-1#accessing-and-exploring-data-products-on-esa-prr","position":6},{"hierarchy":{"lvl1":"ESA Project Results Repository","lvl2":"Accessing and Exploring data products on ESA PRR"},"content":"This notebook is generated for users willing to explore the ESA PRR repository, by browsing, previewing and/or downloading data published on the PRR.\n\nESA Project Results Repository (PRR) Data Access and Collections Preview - Use this notebook to access, explore, query, and download data from the ESA Project Results Repository (PRR).","type":"content","url":"/index-1#accessing-and-exploring-data-products-on-esa-prr","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project"},"type":"lvl1","url":"/prr-zarr","position":0},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project"},"content":"This notebook shows how to generate a valid STAC collection, which is a requirement to upload research outcomes to the \n\nESA Project Results Repository (PRR). It focuses on generating metadata for a project with zarr data. The product has two zarr files, covering different regions, created using Sentinel 1 and Sentinel 2 data respectively.\n\nCheck the \n\nEarthCODE documentation, and \n\nPRR STAC introduction example for a more general introduction to STAC and the ESA PRR.\n\nThe code below demonstrates how to perform the necessary steps using real data from the ESA project Yield Prediction and Estimation from Earth Observation (YIPEEO). The focus of YIPEEO is to improve field-scale crop yield forecasts through the usage of high-resolution remote sensing data and cutting edge scientific methods.\n\n🔗 Check the project website: \n\nYield Prediction and Estimation from Earth Observation (YIPEEO) – Website","type":"content","url":"/prr-zarr","position":1},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl4":"Acknowledgment"},"type":"lvl4","url":"/prr-zarr#acknowledgment","position":2},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl4":"Acknowledgment"},"content":"We gratefully acknowledge the Yield Prediction and Estimation from Earth Observation (YIPEEO) team for providing access to the data used in this example, as well as support in creating it.\n\n# import libraries\nimport xarray as xr\nfrom pystac import Item, Collection\nimport pystac\nfrom datetime import datetime\nfrom shapely.geometry import box, mapping\nfrom xstac import xarray_to_stac\nimport glob\nimport json\nimport shapely\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\n\n","type":"content","url":"/prr-zarr#acknowledgment","position":3},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"1. Generate the parent collection"},"type":"lvl2","url":"/prr-zarr#id-1-generate-the-parent-collection","position":4},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"1. Generate the parent collection"},"content":"The root STAC Collection provides a general description of all project outputs which will be stored on the PRR.\nThe PRR STAC Collection template enforces some required fields that you need to provide in order to build its valid description. Most of these metadata fields should already be available and can be extracted from your data.\n\n# create the parent collection\ncollectionid = \"yipeeo-cropyields\"\n\n\ncollection = Collection.from_dict(\n    \n{\n  \"type\": \"Collection\",\n  \"id\": collectionid,\n  \"stac_version\": \"1.1.0\",\n  \"title\": \"Yield Prediction and Estimation features from Sentinel1 and Sentinel2 data\",\n  \"description\": \"This dataset contains the processed Sentinel 1 and Sentinel 2 features used for yield rediction  in the Yield Prediction and Estimation from Earth Observation (YIPEEO) project. Sentinel-2 L2A collection is used to compute a set of features based on the provided bands as well as various vegetation indices. Sentinel-1 data for the years 2016-2023 was pre-processed by TUW RS on the Vienna Scientific Cluster using the software SNAP8 and software packages developed by the TUW RS group.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.844270319251073,\n          49.040729923617775,\n          31.01967739451807,\n          52.869947524440924\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2016-01-01T00:00:00Z\",\n          \"2022-12-31T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"license\": \"various\",\n  \"links\": []\n\n}\n\n)\n\ncollection # visualise the metadata of your collection \n\n","type":"content","url":"/prr-zarr#id-1-generate-the-parent-collection","position":5},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl2","url":"/prr-zarr#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":6},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"The second step is to describe the different files as STAC Items and Assets. Take your time to decide how your data should be categorised to improve usability of the data, and ensure intuitive navigation through different items in the collections. There are multiple strategies for doing this and this tutorial demonstrate one of the possible ways of doing that. Examples of how other ESA projects are doing this are available in the \n\nEarthCODE documentation .","type":"content","url":"/prr-zarr#id-2-create-stac-items-and-stac-assets-from-original-dataset","position":7},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.1 Add the Sentinel 1 features to a STAC Item","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl3","url":"/prr-zarr#id-2-1-add-the-sentinel-1-features-to-a-stac-item","position":8},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.1 Add the Sentinel 1 features to a STAC Item","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"\n\nsentinel1_url = 'https://objectstore.eodc.eu:2222/68e13833a1624f43ba2cac01376a18af:ASP_ZARR/S1_out.zarr'\nds = xr.open_zarr(sentinel1_url)\nds\n\nbbox = (\n    float(ds.min_lon.min().values), \n    float(ds.min_lat.min().values), \n    float(ds.max_lon.max().values), \n    float(ds.max_lat.max().values)\n)\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\n\ntemplate = {\n    \"id\": f\"{collectionid}-sentinel1-features\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.0.0\",\n    \"properties\": {\n        \"title\": \"Sentinel-1 Features\",\n        \"description\": 'Sentinel 1 features for crop yield prediction and estimatation from 2015 to 2022. The processing workflow consists of the following steps:\\n1. Apply precise orbit data\\n2. Border-noise removal\\n3. Radiometric calibration\\n4. Radiometric terrain-flattening\\n5. Range-Doppler terrain correction\\nFor steps 4. and 5. the 30 m Copernicus Digital Elevation Model (DEM) was used. To extract time series on field level from the pre-processed Sentinel-1 data, several further processing steps were performed to mitigate the impact of the viewing geometry and undesired objects within or near the fields. In a first step, an incidence angle normalization to 40\\u00b0 was performed. Afterwards, all pixels below a standard deviation of 5dB within one year were filtered out as they are typically stemming from radar shadow pixels or are no crop pixels. Finally, the cross-ratio was calculated by subtracting VV and VH polarized backscatter. ',\n        \"start_datetime\": pd.to_datetime(ds.time.min().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"end_datetime\": pd.to_datetime(ds.time.max().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"license\": \"CC-BY-4.0\",\n        \"platform\": \"sentinel-1\",\n        \"instruments\": [\"c-sar\"],\n        \"created\": datetime.utcnow().isoformat() + \"Z\"\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": \"f'/d/{collectionid}/S1_out.zarr\",  # or local path\n            \"type\": \"application/vnd+zarr\",\n            \"roles\": [\"data\"],\n            \"title\": \"Zarr Store of Sentinel1 Field Stats\"\n        }\n    }\n}\n# 3. Generate the STAC Item\nsentinel1_item = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\",\n    x_dimension=False,\n    y_dimension=False\n)\n\nsentinel1_item.validate()\nsentinel1_item\n\n","type":"content","url":"/prr-zarr#id-2-1-add-the-sentinel-1-features-to-a-stac-item","position":9},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.2 Add the sentinel 2 features","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"type":"lvl3","url":"/prr-zarr#id-2-2-add-the-sentinel-2-features","position":10},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl3":"2.2 Add the sentinel 2 features","lvl2":"2. Create STAC Items and STAC Assets from original dataset"},"content":"\n\nsentinel2_url = 'https://objectstore.eodc.eu:2222/68e13833a1624f43ba2cac01376a18af:ASP_ZARR/S2_out.zarr'\nds = xr.open_dataset(sentinel2_url, engine='zarr')\nds\n\nbbox = (\n    float(ds.min_lon.min().values), \n    float(ds.min_lat.min().values), \n    float(ds.max_lon.max().values), \n    float(ds.max_lat.max().values)\n)\ngeometry = json.loads(json.dumps(shapely.box(*bbox).__geo_interface__))\n\n\ntemplate = {\n    \"id\": f\"{collectionid}-sentinel2-features\",\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.0.0\",\n    \"properties\": {\n        \"title\": \"Sentinel-2 Features\",\n        \"description\": 'Sentinel 2 features based on the provided bands as well as various vegetation indices. The Sentinel-2 L2A data cube is dynamically created by utilising the STAC API. The datacube is pre-filter with scenes of a cloud cover less than 80%. The following features are extracted per field and timestamp: Band Medians and Standard Deviations: B02, B03, B04, B05, B06, B07, B08, B8A, B11, B12; Vegetation indices based on median bands of NDVI, EVI, NDWI, NMDI. An outlier removal was added on a field scale level utilising the SCL band and outlier removal based on 2 x inter quartile range (IQR).',\n        \"start_datetime\": pd.to_datetime(ds.time.min().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"end_datetime\": pd.to_datetime(ds.time.max().values).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"license\": \"CC-BY-4.0\",\n        \"platform\": \"sentinel-2\",\n        \"instruments\": [\"msi\"],\n        \"created\": datetime.utcnow().isoformat() + \"Z\"\n    },\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"assets\": {\n        \"data\": {\n            \"href\": \"f'/d/{collectionid}/S2_out.zarr\",  # or local path\n            \"type\": \"application/vnd+zarr\",\n            \"roles\": [\"data\"],\n            \"title\": \"Zarr Store of Sentinel2 Field Stats\"\n        }\n    }\n}\n# 3. Generate the STAC Item\nsentinel2_item = xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\",\n    x_dimension=False,\n    y_dimension=False\n)\nsentinel2_item.validate()\nsentinel2_item\n\n","type":"content","url":"/prr-zarr#id-2-2-add-the-sentinel-2-features","position":11},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"3. Add the Items to the collection and Save the metadata as a self-contained collection"},"type":"lvl2","url":"/prr-zarr#id-3-add-the-items-to-the-collection-and-save-the-metadata-as-a-self-contained-collection","position":12},{"hierarchy":{"lvl1":"Generating STAC metadata for the YIPEEO project","lvl2":"3. Add the Items to the collection and Save the metadata as a self-contained collection"},"content":"\n\ncollection.add_items([sentinel1_item, sentinel2_item])\n\n# save the full self-contained collection\ncollection.normalize_and_save(\n    root_href='../../data/yippeo_collection/',\n    catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\n\ncollection","type":"content","url":"/prr-zarr#id-3-add-the-items-to-the-collection-and-save-the-metadata-as-a-self-contained-collection","position":13},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube"},"type":"lvl1","url":"/sen4amazonas","position":0},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube"},"content":"Goal: Produce a time-indexed Zarr data cube of forest-loss masks for the Amazonas region to enable scalable analysis and visualization. This example is based on the \n\nSentinel for amazonas project project.\n\nClasses:\n\n1 — forest loss\n\n0 — no data / background (treated as missing)\n\nStudy area and grid:\n\nCRS: EPSG:4326\n\nBounds: -79.1997438, -17.27354892, -44.91217178, 9.0467434\n\nResolution: 0.00017966 degrees (see gdalwarp step)\n\nWorkflow overview:\n\nConvert each GeoTIFF mosaic to per-date Zarr with gdalwarp.\n\nBuild a sorted list of Zarr stores and timestamps.\n\nInitialize an empty target cube (cube.zarr) with the full time axis.\n\nAppend each slice into the cube using region writes.\n\nTune chunking for performance and memory.\n\nRequirements:\n\nGDAL built with Zarr driver (gdalwarp -of Zarr)\n\nPython: xarray, rioxarray, zarr, dask[distributed], numpy, pandas, optionally hvplot.xarray for quick QA\n\nData layout:\n\nInput: mosaics/*.tif (per-date masks)\n\nIntermediate: mosaics/*.zarr (per-date stores)\n\nOutput: cube.zarr (time-series cube)\n\n","type":"content","url":"/sen4amazonas","position":1},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl2":"Discussion"},"type":"lvl2","url":"/sen4amazonas#discussion","position":2},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl2":"Discussion"},"content":"When handling many temporal slices in Earth Observation, storing them as a single Zarr data cube is far superior to managing hundreds of COGs linked via a STAC collection because Zarr treats time as a first-class dimension within a unified array structure rather than as disconnected files. This enables efficient temporal queries and parallel access using chunked, compressed blocks that map directly to analysis patterns like pixel-wise time series or rolling statistics, all without the per-file latency, catalog management, and HTTP overhead that cripple COG-based stacks at scale.\n\nTo learn more read the \n\nEarthCODE data best practices pages\n\n","type":"content","url":"/sen4amazonas#discussion","position":3},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"1) Convert GeoTIFFs to Zarr with GDAL","lvl2":"Discussion"},"type":"lvl3","url":"/sen4amazonas#id-1-convert-geotiffs-to-zarr-with-gdal","position":4},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"1) Convert GeoTIFFs to Zarr with GDAL","lvl2":"Discussion"},"content":"Use GDAL to warp each input GeoTIFF to EPSG:4326, clip to the Amazonas extent, set the target resolution, and write per-date Zarr stores. Adjust IN_DIR, OUT_DIR, bounds (-te), resolution (-tr), and compression to match your data and hardware.\n\nTip: Confirm Zarr support with gdalinfo --formats | grep -i zarr.\n\n%%bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nIN_DIR=\"mosaics\"\nOUT_DIR=\"mosaics\"\n\nfor tif in \"${IN_DIR}\"/*.tif; do\n  base=\"$(basename \"${tif}\" .tif)\"\n  out=\"${OUT_DIR}/${base}.zarr\"\n\n  echo \"Warping ${tif} -> ${out}\"\n  gdalwarp -overwrite \\\n    -t_srs EPSG:4326 \\\n    -te -79.1997438 -17.27354892 -44.91217178 9.0467434 \\\n    -tr 0.00017966 0.00017966 \\\n    -r near \\\n    -dstnodata 0 \\\n    \"${tif}\" \\\n    \"${out}\" \\\n    -of Zarr \\\n    -multi -wo NUM_THREADS=ALL_CPUS \\\n    -co COMPRESS=ZSTD -co ZSTD_LEVEL=3 -co BLOCKSIZE=512,512\n\ndone\n\n","type":"content","url":"/sen4amazonas#id-1-convert-geotiffs-to-zarr-with-gdal","position":5},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"type":"lvl3","url":"/sen4amazonas#id-2-python-setup-and-file-discovery","position":6},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"content":"Import libraries, set the input folder (mosaics/), and collect per-date Zarr stores produced in step 1. Timestamps are parsed from filenames like prefix_YYYYMMDD.zarr. If your naming differs, adjust the parsing logic accordingly.\n\nimport os\nimport rioxarray\nimport xarray as xr\nimport hvplot.xarray\nimport pandas as pd\nfrom pathlib import Path\n\nfolder = \"mosaics\"\nfiles = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".zarr\")]\n# print(files)\n\ntimestamps = [\n    pd.to_datetime(os.path.splitext(Path(f).name)[0].split(\"_\")[1]) for f in files\n]\n# timestamps\n\n","type":"content","url":"/sen4amazonas#id-2-python-setup-and-file-discovery","position":7},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Sort files by timestamp","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"type":"lvl4","url":"/sen4amazonas#sort-files-by-timestamp","position":8},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Sort files by timestamp","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"content":"Ensure files and timestamps are aligned chronologically for consistent time indexing.\n\nimport numpy as np\nii = np.argsort(timestamps)\ntimestamps = np.array(timestamps)[ii]\nfiles = np.array(files)[ii]\n# files\n\n","type":"content","url":"/sen4amazonas#sort-files-by-timestamp","position":9},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Optional: Start a Dask client","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"type":"lvl4","url":"/sen4amazonas#optional-start-a-dask-client","position":10},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Optional: Start a Dask client","lvl3":"2) Python setup and file discovery","lvl2":"Discussion"},"content":"Spins up a local Dask scheduler/workers to parallelize IO and computation.\n\nfrom distributed.client import Client\ncl = Client()\ncl\n\n","type":"content","url":"/sen4amazonas#optional-start-a-dask-client","position":11},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"type":"lvl3","url":"/sen4amazonas#id-3-initialize-target-cube-template","position":12},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"content":"Define chunk sizes and build a template dataset containing only the time coordinate. This pre-allocates the cube.zarr store with the desired chunking.\n\nNote: Adjust chunk sizes (x, y, time) to balance memory, IO, and parallelism for your environment.\n\n%%time\nchunk_size = {\"time\": 1,\"x\": 512*10, \"y\": 512*10}\ntemp_data = da.expand_dims({'time': timestamps.tolist()})\ntemp_das = xr.Dataset({'forest_loss': temp_data})\ntemp_das = temp_das.chunk(chunk_size)\ntemp_das\n\n","type":"content","url":"/sen4amazonas#id-3-initialize-target-cube-template","position":13},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Write template to cube.zarr","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"type":"lvl4","url":"/sen4amazonas#write-template-to-cube-zarr","position":14},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Write template to cube.zarr","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"content":"Create the target store with the full time axis and chosen chunks without writing data yet (compute=False).\n\n# write tempalte data that has al the timestamps\ntemp_das.forest_loss.attrs.pop(\"_FillValue\")\n\nenc = {\n    \"forest_loss\": {\n        \"_FillValue\": np.nan,\n        \"dtype\": np.float32,\n    }\n}\n\ntemp_das.to_zarr('cube.zarr', mode='w', compute=False, encoding=enc, write_empty_chunks=False, \n                 safe_chunks=True)\n\n","type":"content","url":"/sen4amazonas#write-template-to-cube-zarr","position":15},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Chunking strategy","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"type":"lvl4","url":"/sen4amazonas#chunking-strategy","position":16},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl4":"Chunking strategy","lvl3":"3) Initialize target cube (template)","lvl2":"Discussion"},"content":"Choose chunk sizes that fit in memory and align with access patterns (typical: larger x/y, small time).\n\nYou can override chunks when opening inputs, e.g.: xr.open_dataset(f, engine=\"zarr\", chunks={\"X\": 512*10, \"Y\": 512*10}).\n\nKeep chunk shapes consistent across inputs and the target cube for best performance.\n\n","type":"content","url":"/sen4amazonas#chunking-strategy","position":17},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"4) Append slices to the cube","lvl2":"Discussion"},"type":"lvl3","url":"/sen4amazonas#id-4-append-slices-to-the-cube","position":18},{"hierarchy":{"lvl1":"Sen4Amazonas: Time-Series Forest Loss Zarr Cube","lvl3":"4) Append slices to the cube","lvl2":"Discussion"},"content":"For each per-date Zarr store:\n\nOpen with xarray and rename dims X/Y to x/y.\n\nConvert 0 to missing (NaN) so background doesn’t accumulate and skip writing empty chunks\n\nAttach CRS (EPSG:4326) and set spatial dims via rioxarray.\n\nExpand with a singleton time coordinate and write into cube.zarr at the matching time index using region.\n\nfor i, (date, f) in enumerate(zip(timestamps, files)):\n    da = xr.open_dataset(f,\n             engine=\"zarr\", chunks={\"X\": 512*10, \"Y\": 512*10}) \n    da = da[next(iter(da.data_vars))]\n    da = da.rename({\"X\":\"x\", \"Y\":\"y\"})\n    da.name = \"forest_loss\"\n    da = da.where(da != 0)\n    da = da.rio.write_crs(\"EPSG:4326\")\n    da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")   # <-- solves MissingSpatialDimensionError\n    da = da.expand_dims({'time': [date]})\n    da = da.chunk(chunk_size)\n    da.forest_loss.attrs.pop(\"_FillValue\")\n\n    enc = {\n        \"forest_loss\": {\n            \"_FillValue\": np.nan,\n            \"dtype\": np.float32,\n        }\n    }\n    da.drop_vars(['x', 'y', 'spatial_ref']).to_zarr('cube.zarr', encoding=enc, write_empty_chunks=False, safe_chunks=True, region={'time': slice(i, i+1)})\n    print(i)\n","type":"content","url":"/sen4amazonas#id-4-append-slices-to-the-cube","position":19},{"hierarchy":{"lvl1":"EarthCODE Tutorials"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"EarthCODE Tutorials"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"EarthCODE Tutorials","lvl2":"Welcome to the EarthCODE Tutorials Book!"},"type":"lvl2","url":"/#welcome-to-the-earthcode-tutorials-book","position":2},{"hierarchy":{"lvl1":"EarthCODE Tutorials","lvl2":"Welcome to the EarthCODE Tutorials Book!"},"content":"Here you will find guides and practical tutorials on how to use the various EarthCODE resources including publication process, data access and exploitation and working with the platforms!\n\nLooking how to upload data to the ESA Project Results Repository (PRR)? Start with our \n\nPRR Tutorials to learn how to easily generate STAC Collections direclty from your files and share your datasets.\n\nLooking how to contribute to the Open Science Catalog, Start with our \n\nOpen Science Catalog Tutorials to learn how to add / change content in the metadata catalog, by enriching it with your research outcomes.\n\nLooking for how to use the openEO to create workflows and experiments? Check out our \n\nopenEO Tutorials.\n\nLooking for how to use the Pangeo deployment on EarthCODE? Check out our \n\nPangeo Tutorials.","type":"content","url":"/#welcome-to-the-earthcode-tutorials-book","position":3},{"hierarchy":{"lvl1":"Creating a workflow"},"type":"lvl1","url":"/workflow","position":0},{"hierarchy":{"lvl1":"Creating a workflow"},"content":"This notebook showcases how to create a workflow using the \n\nopenEO Python client within the Copernicus Data Space Ecosystem (CDSE). The workflow will be published as a \n\nUser Defined Process (UDP). UDPs allow you to encapsulate your processing workflow, which consists of multiple steps, into reusable openEO building blocks that can be called with a single command. This approach enables you to share and reuse your workflows across different projects and experiments.\n\nIf your workflow is part of a scientific or research experiment, you can publish it in the \n\nEarthCODE Open Science Data Catalog once finalized. This ensures that your workflow is findable and accessible to other users in the scientific community.\n\nIn this example, we will create a workflow to generate a variability map, using Sentinel-2 data products that are available on the Copernicus Data Space Ecosystem. . In a variability map, each image pixel is assigned a specific category, which represents the deviation of the pixel value from the mean pixel value. These maps can be used to implement precision agriculture practices by applying different fertilization strategies for each category. For instance, a farmer might choose to apply more product to areas of the field that exhibit a negative deviation and less to those with positive deviations (enhancing poorer areas), or concentrate the application on regions with positive deviations (focusing on the more productive areas of the field).\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/workflow","position":1},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/workflow#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before creating any processing workflow in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\nimport json\nfrom openeo.rest.udp import build_process_dict\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/workflow#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Defining the workflow parameters"},"type":"lvl2","url":"/workflow#defining-the-workflow-parameters","position":4},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Defining the workflow parameters"},"content":"The first step in creating an openEO workflow is specifying the \n\ninput parameters. These parameters enable users to execute the workflow with their own custom settings, making it adaptable to different datasets and use cases. openEO provides built-in \n\nhelper functions that assist in defining these parameters correctly.\n\nfrom openeo.api.process import Parameter\n\narea_of_interest = Parameter.geojson(name='spatial_extent', description=\"Spatial extent for which to generate the variability map\")\ntime_of_interest = Parameter.date(name='date', description=\"Date for which to generate the variability map\")\n\n","type":"content","url":"/workflow#defining-the-workflow-parameters","position":5},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Implementation of the workflow"},"type":"lvl2","url":"/workflow#implementation-of-the-workflow","position":6},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Implementation of the workflow"},"content":"Next, we will begin implementing the variability map workflow. This involves using the predefined functions in openEO to create a straightforward workflow consisting of the following steps:\n\nSelect the S2 data based on the area_of_interest and time_of_interest parameters.\n\nCalculate the NDVI for the S2 data.\n\nApply an openEO User Defined Function (UDF) to calculate the deviation of each pixel against the mean pixel value of the datacube.\n\n# Step 1. Select the S2 data based on the workflow parameters\ns2_cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent=area_of_interest,\n    temporal_extent=[time_of_interest,time_of_interest],\n)\n    \ns2_masked = s2_cube.mask_polygon(area_of_interest)\n\n# Step 2. Calculate the S2 NDVI\ns2_ndvi = s2_masked.ndvi()\n\n# Step 3. Apply the UDF to calculate the variability map\ncalculate_udf = openeo.UDF.from_file(\"./files/variability_map.py\")\nvarmap_dc = s2_ndvi.reduce_temporal(calculate_udf)\n\nfrom IPython.display import JSON\n\nJSON(varmap_dc.to_json())\n\n","type":"content","url":"/workflow#implementation-of-the-workflow","position":7},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Create an openEO-based workflow"},"type":"lvl2","url":"/workflow#create-an-openeo-based-workflow","position":8},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Create an openEO-based workflow"},"content":"In this next step, we will create our workflow by establishing our openEO User Defined Process (UDP). This action will create a public reference to the workflow we developed in the preceding steps. This can be achieved by using the \n\nsave_user_defined_process function.\n\nNote\n\nThe publication of the UDP contains a public reference to the workflow, which can be shared with others. This allows users to execute the workflow without needing to recreate it from scratch, promoting collaboration and reuse of processing workflows.\n\nworkflow = connection.save_user_defined_process(\n    \"variability_map\",\n    varmap_dc,\n    parameters=[area_of_interest, time_of_interest],\n    public=True\n)\nworkflow\n\nIn the previous step, we created a workflow as a UDP in openEO. We can now use the public URL to share the workflow with others or to execute it in different contexts. The UDP encapsulates the entire processing logic, making it easy to apply the same workflow to different datasets or parameters without needing to redefine the steps each time. In this example, the published UDP is available at the following URL: \n\nVariability Map UDP.\n\n","type":"content","url":"/workflow#create-an-openeo-based-workflow","position":9},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Testing the workflow"},"type":"lvl2","url":"/workflow#testing-the-workflow","position":10},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Testing the workflow"},"content":"After saving the workflow, we can test it by executing the UDP with specific parameters. This step allows us to verify that the workflow operates as expected and produces the desired results. We start by defining the parameters that we want to use for the test run. These parameters will be passed to the UDP when it is executed.\n\nspatial_extent_value = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              5.170043941798298,\n              51.25050990858725\n            ],\n            [\n              5.171035037521989,\n              51.24865722468999\n            ],\n            [\n              5.178521828188366,\n              51.24674578027137\n            ],\n            [\n              5.179084341977159,\n              51.24984764553983\n            ],\n            [\n              5.170043941798298,\n              51.25050990858725\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    }\n  ]\n}\n\ndate_value = \"2025-05-01\"\n\nNext we use our previously created datacube varmap_dc to execute our workflow as an openEO batch job. This step involves submitting the job to the openEO backend, which will process the data according to the defined workflow and parameters. The backend will handle the execution of the workflow and return the results, which can then be analyzed or visualized as needed.\n\npath =  \"./files/varmap_workflow_test.tiff\"\n\nvarmap_test = connection.datacube_from_process(\n    \"variability_map\",\n    spatial_extent=spatial_extent_value,\n    date=date_value,\n)\nvarmap_test.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Workflow Test\", \n    description=\"This is an example of a workflow test containing the calculation of a variability map in Belgium\",\n)\n\nFinally, we can visualize the results of our workflow. This step allows us to see the output of the variability map and assess its quality and relevance for our specific use case. Visualization is a crucial part of the workflow, as it helps in interpreting the results and making informed decisions based on the data processed by our openEO workflow.\n\nvisualise_tif(path)\n\n\n\n","type":"content","url":"/workflow#testing-the-workflow","position":11},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Sharing your workflow"},"type":"lvl2","url":"/workflow#sharing-your-workflow","position":12},{"hierarchy":{"lvl1":"Creating a workflow","lvl2":"Sharing your workflow"},"content":"Now that our workflow has been created using save_user_defined_process and we’ve confirmed that it works, we can share it with others and the broader community. Using the openEO functions demonstrated before, the workflow is automatically stored on the openEO backend we connected to in the initial steps. The workflow, referred to as a \n\nUser Defined Process (UDP) in openEO terminology, is a JSON-based structure that contains the steps of the workflow, represented as an \n\nopenEO process graph, along with additional metadata such as a description of the workflow parameters.","type":"content","url":"/workflow#sharing-your-workflow","position":13},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Exporting your workflow","lvl2":"Sharing your workflow"},"type":"lvl3","url":"/workflow#exporting-your-workflow","position":14},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Exporting your workflow","lvl2":"Sharing your workflow"},"content":"There are several ways to make your workflow accessible for reuse among peers and within your communities:\n\nShare the public URL with your peersSince we used public=True in our save_user_defined_process, a public URL was automatically added to the workflow definition.\nIn this case, the public URL is:\n\nhttps://​openeo​.dataspace​.copernicus​.eu​/openeo​/1​.1​/processes​/u:6391851f​-9042​-4108​-8b2a​-3dd2e8a9dd0b​/variability​_map\n\nExport the workflow definition to your preferred storageAlternatively, you can also export the workflow and store it in a version-controlled environment like GitHub or your own preferred storage. This gives you full control over its content and version history. In this case, instead of using save_user_defined_process, you can use build_process_dict to create a dictionary representation of the workflow, which can then be written to a file. However, if you want others to reuse your workflow, make sure the file is accessible via a public URL. This is necessary for the openEO backend to retrieve and execute the workflow definition.\n\nspec = build_process_dict(\n    process_id=\"variability_map\",\n    process_graph=varmap_dc,\n    parameters=[area_of_interest, time_of_interest],\n)\n\nwith open(\"files/variability_map_workflow.json\", \"w\") as f:\n    json.dump(spec, f, indent=2)\n\nTip\n\nDuring the development phase, the public URL of the workflow can be used to quickly share early versions with your colleagues. However, once the workflow reaches a level of maturity suitable for broader community use, we recommend storing the workflow definition on GitHub. Since version control within openEO is limited, using open-source tools like GitHub allows for better management of release procedures and workflow updates.\n\n","type":"content","url":"/workflow#exporting-your-workflow","position":15},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Sharing your workflow","lvl2":"Sharing your workflow"},"type":"lvl3","url":"/workflow#sharing-your-workflow-1","position":16},{"hierarchy":{"lvl1":"Creating a workflow","lvl3":"Sharing your workflow","lvl2":"Sharing your workflow"},"content":"\n\nOnce you have a public reference to your workflow, either through the openEO backend or a public URL pointing to a definition stored on GitHub or another platform, you can share it in various ways to enable others to execute it, as shown in our \n\nCreating an experiment example. There are many different ways to share your workflow:\n\nDirect sharing with peers and communities\n\nPublishing your workflow on the \n\nEarthCODE Open Science Catalogue, as demonstrated in our \n\npublication example\n\nPublishing on platform marketplaces, such as the \n\nCopernicus Data Space Ecosystem Algorithm Plaza.","type":"content","url":"/workflow#sharing-your-workflow-1","position":17},{"hierarchy":{"lvl1":"Creating an experiment"},"type":"lvl1","url":"/experiment","position":0},{"hierarchy":{"lvl1":"Creating an experiment"},"content":"With our workflow successfully created as an openEO User Defined Process (UDP) in our \n\nprevious tutorial, we can now set up an experiment. This involves executing the workflow with a predefined set of input parameters. By specifying parameters such as the area of interest and time range, we can tailor the experiment to generate meaningful results.\n\nRunning the experiment will produce output products, which will be displayed at the end of the notebook. These results can then be further analyzed, shared with the scientific community, or published in the \n\nEarthCODE Open Science Catalogue to promote reproducibility and collaboration.\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/experiment","position":1},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/experiment#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before creating the experiment in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/experiment#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Setting up the experiment"},"type":"lvl2","url":"/experiment#setting-up-the-experiment","position":4},{"hierarchy":{"lvl1":"Creating an experiment","lvl2":"Setting up the experiment"},"content":"In this step, we will set up the experiment by defining the parameters for the variability map calculation. This includes specifying the area of interest, the time range, and any other relevant parameters that are required for the processing workflow. The experiment will be executed using the UDP from \n\nprevious tutorial which yielded a public URL that can be used to execute the workflow. We will use this URL to set up our experiment, ensuring that the processing steps are applied correctly to the specified input data.\n\nNote\n\nAs mentioned in our \n\nworkflow tutorial, the workflow URL can either refer to a definition stored on the openEO backend—created using save_user_defined_process with public=True, or to a public URL pointing to a JSON file hosted on GitHub or another platform.\n\nworkflow_url = \"https://openeo.dataspace.copernicus.eu/openeo/1.1/processes/u:6391851f-9042-4108-8b2a-3dd2e8a9dd0b/variability_map\" \n\nspatial_extent_value = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              5.170043941798298,\n              51.25050990858725\n            ],\n            [\n              5.171035037521989,\n              51.24865722468999\n            ],\n            [\n              5.178521828188366,\n              51.24674578027137\n            ],\n            [\n              5.179084341977159,\n              51.24984764553983\n            ],\n            [\n              5.170043941798298,\n              51.25050990858725\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    }\n  ]\n}\n\ndate_value = \"2025-05-01\"\n\nWe can now execute our experiment using the datacube_from_process function and the information from the workflow we created:\n\nprocess_id: the ID that we have assigned to our workflow\n\nnamespace: the public URL of the workflow, which is provided as output of the save_user_defined_process call\n\n**kwargs: the parameters of the workflow. If a parameter is not specified, its default value will be used\n\nvarmap_experiment = connection.datacube_from_process(\n    process_id=\"variability_map\",\n    namespace=workflow_url,\n    spatial_extent=spatial_extent_value,\n    date=date_value\n)\n\nNow that we have created our experiment, we can explore how it is defined within openEO.\n\nfrom IPython.display import JSON\n\nJSON(varmap_experiment.to_json())\n\nNow that we have created our experiment, we can execute it by submitting it as an openEO batch job to the backend. This will trigger the processing workflow defined in our UDP, applying it to the specified input data. The output will be saved to a specified path, which we can then visualize or analyze further.\n\npath =  \"./files/varmap_experiment.tiff\"\nvarmap_experiment.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Experiment\", \n    description=\"This is an example experiment from CDSE containing the calculation of a variability map in Belgium\",\n)\n\nvisualise_tif(path)","type":"content","url":"/experiment#setting-up-the-experiment","position":5},{"hierarchy":{"lvl1":"Publishing an experiment to EarthCODE"},"type":"lvl1","url":"/publication","position":0},{"hierarchy":{"lvl1":"Publishing an experiment to EarthCODE"},"content":"In this guide we will explore the step needed to publish our \n\npreviously created experiment to the EarthCODE Open Science Catalogue (OSC). To support this process, a dedicated tool, called the \n\nopenEO Publishing tool, has been created, which will guide you through the process of publishing the experiment. The publishing tool will create a GitHub pull request on the OSC with all the necessary information required to publish the experiment, including details about the product and the workflow. After approval of the pull request by the EarthCODE team, your experiment, its corresponding workflow and the resulting output products will become available in the catalogue for users to discover and reuse.\n\nTo start the publishing process, you can access the openEO Publishing tool at \n\npublish​.earthcode​.vito​.be. The tool will prompt you to log in using your GitHub account, which is necessary to create the pull request in the EarthCODE OSC repository.\n\nOnce logged in, you will need to select one of the supported openEO backends from which you want to publish your experiment. The tool currently supports the following backends:\n\nCopernicus Data Space Ecosystem (CDSE) openEO Federation\n\nClick the Next button to proceed after selecting the backend. The openEO Publishing tool will then connect to the selected backend. If you are not already authenticated, you will be prompted to log in to the backend using your credentials. Clicking the Authenticate button will redirect you to the selected backend’s authentication page, where you can log in with your credentials. Once authenticated, you will be redirected back to the openEO Publishing tool.\nThis step is necessary to retrieve the list of experiments that you have created on the selected backend.\n\nNow that you are authenticated, you can select the experiment you want to publish from the list of available jobs. The tool will display all the processing jobs associated with your account on the selected backend. You can select one or multiple experiments to publish. Click the Next button to proceed to the next step.\n\nHint\n\nThe list of jobs only includes those that have been executed successfully. If you do not see your experiment in the list, make sure it has been executed and completed without errors.\n\nAfter selecting one or more jobs, you will be prompted to provide additional information about the experiment. First you will need to select if you want to publish the full experiment, only the workflow, or only the output products. The full experiment includes both the workflow and the output products, while selecting only the workflow or output products will publish only those components.\n\nBased on your selection, the tool will show a dedicated form to fill in the necessary details for the publication. Some of the fields are automatically filled in based on the experiment metadata retrieved from the platform, while others require manual input. The following sections describe the fields you need to fill in for each publication type.\n\nFor experiments:\n\nProject: The project under which the experiment is published. This should be a valid \n\nEarthCODE project name.\n\nID: A unique identifier for the experiment.\n\nTitle: A descriptive title for your experiment.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nLicense (optional): The license under which the experiment is published. This should be a valid license identifier.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nExperiment Process Graph: Selection of the openEO process graph that represents your experiment. You can either choose to take the process graph from the selected job or refer to a public process graph URL that you have created previously.\n\nFor workflows:\n\nProject: The project under which the workflow is published. This should be a valid \n\nEarthCODE project name\n\nID: A unique identifier for the workflow.\n\nURL: The public URL of the openEO User Defined Process hat represents your workflow. This should be a valid URL pointing to a public process graph. See our tutorial on \n\ncreating a workflow for more information on how to create a User Defined Process and get its URL.\n\nTitle: A descriptive title for your workflow.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nNote\n\nWhenever you publish a workflow as part of an experiment, some of the above fields are covered by the experiment metadata. Therefore, you only need to fill in the fields that are not already provided by the experiment. Additionally in the case of an experiment, you can also choose to select an existing workflows from the list of workflows that are already available on the EarthCODE OSC.\n\nFor products:\n\nProject: The project under which the product is published. This should be a valid \n\nEarthCODE project name\n\nID: A unique identifier for the product.\n\nTitle: A descriptive title for your product.\n\nDescription: A detailed description of the experiment, including its purpose and methodology.\n\nThemes: The thematic categories that best describe your experiment. You can select multiple themes from the provided list.\n\nAssets: A list of assets that part of the product. By default the result of the openEO job is automatically added to the list of assets. You can add additional assets by providing their URLs and a corresponding name. The assets should be publicly accessible URLs pointing to the output files generated by the experiment.\n\nNote\n\nWhenever you publish a product as part of an experiment, some of the above fields are covered by the experiment metadata. Therefore, you only need to fill in the fields that are not already provided by the experiment.\n\nAfter filling in the necessary details, you can click the Next button to proceed to the final step. The openEO Publishing tool will then create a pull request on the EarthCODE OSC repository with all the information you provided. You will be able to access the pull request by clicking the link provided in the tool and track its status either through that link or on the \n\nGitHub pull request page. The pull request will contain all the necessary files and metadata required to publish your experiment, workflow, or product in the EarthCODE OSC.\n\nYour experiment, workflow, or product will be now reviewed by the EarthCODE team. Once approved, it will be published in the EarthCODE Open Science Catalogue, making it available for other users to discover and reuse.","type":"content","url":"/publication","position":1},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue"},"type":"lvl1","url":"/reproduce","position":0},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue"},"content":"In this guide, we will use your newly published experiment from \n\nprevious tutorial and reproduce it. This process will demonstrate how to leverage the openEO federation and EarthCODE tools to reproduce experiments, ensuring transparency and reproducibility in scientific research, reinforcing the principles of Open Science.\n\nNote\n\nIn this guide, we will use a link to a testing experiment from a GitHub repository. The links that we use point to the EarthCODE Open Science Catalogue GitHub repository. This link may not work in the future, as it is a test experiment. However, you can use the same steps to reproduce any experiments published to the EarthCODE Open Science Catalogue.\n\nexperiment_url = \"https://raw.githubusercontent.com/ESA-EarthCODE/open-science-catalog-metadata-testing/c9ad31fd63330818e7895faf10f5104d9a101c01/experiments/cdse_federation_-_variability_map_experiment/process_graph.json\"\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\ndef visualise_tif(path: str):\n    with rasterio.open(path) as src:\n        data = src.read(1)  # Read the first band\n        plt.figure(figsize=(10, 10))\n        plt.imshow(data, cmap='viridis')\n        plt.colorbar()\n        plt.show()\n\n","type":"content","url":"/reproduce","position":1},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Connection with CDSE openEO Federation"},"type":"lvl2","url":"/reproduce#connection-with-cdse-openeo-federation","position":2},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Connection with CDSE openEO Federation"},"content":"The first step, before executing our published experiment in openEO, is to authenticate with an available openEO backend. In this example, we will use the CDSE openEO federation, which provides seamless access to both datasets and processing resources across multiple federated openEO backends.\n\nimport openeo\n\nconnection = openeo.connect(url=\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n","type":"content","url":"/reproduce#connection-with-cdse-openeo-federation","position":3},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Using the openEO client"},"type":"lvl2","url":"/reproduce#using-the-openeo-client","position":4},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl2":"Using the openEO client"},"content":"The first option to execute an existing experiment is through the openEO Python client. By using openEO’s datacube_from_json, you can import the published experiment from the OSC into openEO.\n\nexperiment = connection.datacube_from_json(experiment_url)\n\nfrom IPython.display import JSON\n\nJSON(experiment.to_json())\n\npath =  \"./files/varmap_experiment_reproduce.tiff\"\nexperiment.execute_batch(\n    path,\n    title=\"CDSE Federation - Variability Map Experiment (reproduce)\"\n)\n\nvisualise_tif(path)\n\nIn this next step we will verify if the output products of the original experiment, created in the \n\nprevious tutorial are the same as the output products of the reproduced experiment. This is done by comparing the output products of the original experiment with the output products of the reproduced experiment.\n\nimport numpy as np\n\n\ndef compare_geotiff(file1: str, file2: str) -> bool:\n    with rasterio.open(file1) as src1, rasterio.open(file2) as src2:\n        # Check if dimensions match\n        if src1.width != src2.width or src1.height != src2.height:\n            return False\n        \n        # Check if coordinate reference systems match\n        if src1.crs != src2.crs:\n            return False\n        \n        # Check if transform properties match\n        if src1.transform != src2.transform:\n            return False\n        \n        # Compare pixel values\n        data1 = src1.read()\n        data2 = src2.read()\n        if not np.array_equal(data1, data2):\n            return False\n\n    return True\n\n# Call the function with the specified files\nare_equal = compare_geotiff(\"./files/varmap_experiment.tiff\", \"./files/varmap_experiment_reproduce.tiff\")\nprint(f\"Are the GeoTIFF files equal? {are_equal}\")\n\n","type":"content","url":"/reproduce#using-the-openeo-client","position":5},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"type":"lvl3","url":"/reproduce#using-the-openeo-web-editor","position":6},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"content":"","type":"content","url":"/reproduce#using-the-openeo-web-editor","position":7},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl4":"Using the experiment URL","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"type":"lvl4","url":"/reproduce#using-the-experiment-url","position":8},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl4":"Using the experiment URL","lvl3":"Using the openEO Web Editor","lvl2":"Using the openEO client"},"content":"Alternatively, you can also open the experiment through the \n\nopenEO Web Editor using the experiment URL. This can be done by clicking the Import process from an external source button located in the top navigation bar.\n\nNext, you can past the experiment URL into the window to fetch the experiment and open it in the editor.\n\nFinally, click the Create Batch Job button to initiate the execution of the experiment.","type":"content","url":"/reproduce#using-the-experiment-url","position":9},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the execution link from the published experiment","lvl2":"Using the openEO client"},"type":"lvl3","url":"/reproduce#using-the-execution-link-from-the-published-experiment","position":10},{"hierarchy":{"lvl1":"Reproducing an openEO Experiment published to the EarthCODE Open Science Catalogue","lvl3":"Using the execution link from the published experiment","lvl2":"Using the openEO client"},"content":"Warning\n\nThis feature is still in development and is not yet fully integrated in the EarthCODE Open Science Catalogue. Therefore, the retrieval of the execution link requires you to access the experiment through GitHub. In future release, this link will be available directly from the EarthCODE Open Science Catalogue.\n\nWhenever an experiment or workflow is published to the EarthCODE Open Science Catalogue, an execution link is generated. This link can be used to execute the experiment or workflow directly in the openEO Web Editor. As the integration of this execution link in the EarthCODE OSC is still in development, for now you can access the execution link through the GitHub repository of the EarthCODE Open Science Catalogue. The execution link is available in the links field of the experiment metadata. For example: \n\nhttps://​editor​.openeo​.org​/​?process​=​https://​raw​.githubusercontent​.com​/ESA​-EarthCODE​/open​-science​-catalog​-metadata​-testing​/c9ad31fd63330818e7895faf10f5104d9a101c01​/experiments​/cdse​_federation​_​-​_variability​_map​_experiment​/process​_graph​.json​&​server​=​https://​openeofed​.dataspace​.copernicus​.eu​/openeo.\n\nNavigating to this link will open the experiment in the openEO Web Editor, allowing you to execute it directly.","type":"content","url":"/reproduce#using-the-execution-link-from-the-published-experiment","position":11},{"hierarchy":{"lvl1":"openEO"},"type":"lvl1","url":"/index-3","position":0},{"hierarchy":{"lvl1":"openEO"},"content":"openEO is an open standard designed to simplify access to and processing of EO data. It provides a unified API that abstracts away differences between cloud platforms, enabling scientists, developers, and analysts to build workflows and experiments in a consistent and portable way. Instead of learning the specifics of each platform or provider, users can work with a common interface to query datasets, chain processing steps, and run analyses at scale.\n\nFrom a technical perspective, openEO defines a set of RESTful APIs and \n\nclient libraries in popular programming languages such as Python, R, and JavaScript. These libraries allow users to connect to different backends, discover available EO collections and processing capabilities, and construct workflows as \n\nprocess graphs. These process graphs describe the sequence of operations in a platform-independent format, making them reusable across environments. Whether the goal is atmospheric correction, time-series analysis, or machine learning integration, openEO provides the building blocks for scalable EO data science.\n\nFrom a user perspective, openEO lowers the barrier to experimentation and collaboration. Users can start with small exploratory analyses in notebooks, gradually expand workflows into more complex experiments, and finally scale them up on powerful cloud infrastructures, all without changing the underlying logic. This portability fosters reproducibility and ensures that workflows developed once can be reused, adapted, or shared easily across projects.\n\nThe notebook tutorials provided in this section illustrate how to use openEO for common scientific tasks: from creating workflow using user defined processes and open source input data, incorporating it into scientific experiment, to publishing experiment with the workflow in EarthCODE. At the end reproducibility of published experiment is also demonstrated.","type":"content","url":"/index-3","position":1},{"hierarchy":{"lvl1":"openEO","lvl2":"openEO Tutorials"},"type":"lvl2","url":"/index-3#openeo-tutorials","position":2},{"hierarchy":{"lvl1":"openEO","lvl2":"openEO Tutorials"},"content":"Creating an openEO workflow: This guide demonstrates how to create a basic openEO workflow, executing a simple processing task and publishing it as workflow that can be shared and reused.\n\nCreating an experiment: This guide shows how to set up an experiment using the previously created workflow, defining parameters such as area of interest and time range, and executing the workflow to generate output products.\n\nPublishing an experiment to EarthCODE: This guide explains how to publish the created experiment to the EarthCODE Open Science Catalogue (OSC) using the openEO Publishing tool, making it discoverable and reusable by the scientific community.\n\nReproducing an experiment: This guide illustrates how to reproduce the published experiment using openEO, verifying the output products against the original experiment to ensure consistency and correctness.","type":"content","url":"/index-3#openeo-tutorials","position":3},{"hierarchy":{"lvl1":"Example EarthCODE Workflow"},"type":"lvl1","url":"/pangeo-on-earthcode","position":0},{"hierarchy":{"lvl1":"Example EarthCODE Workflow"},"content":"","type":"content","url":"/pangeo-on-earthcode","position":1},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Context"},"type":"lvl3","url":"/pangeo-on-earthcode#context","position":2},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Context"},"content":"We will be using the \n\nPangeo open-source software stack to demonstrate how to fetch EarthCODE published data and publically available Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires.\n\nIn this workshop, we will be using the \n\nSeasFire Data Cube published to the EarthCODE Open Science Catalog.","type":"content","url":"/pangeo-on-earthcode#context","position":3},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Methodology approach","lvl3":"Context"},"type":"lvl4","url":"/pangeo-on-earthcode#methodology-approach","position":4},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Methodology approach","lvl3":"Context"},"content":"Analyse and find burnt areas using SeasFire Data Cube\n\nAccess Sentinel-2 L2A cloud optimised dataset through STAC\n\nCompute the Normalised Burn Ratio (NBR) index to highlight burned areas\n\nClassify burn severity","type":"content","url":"/pangeo-on-earthcode#methodology-approach","position":5},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Highlights","lvl3":"Context"},"type":"lvl4","url":"/pangeo-on-earthcode#highlights","position":6},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Highlights","lvl3":"Context"},"content":"Using OSC data\n\nThe NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths.","type":"content","url":"/pangeo-on-earthcode#highlights","position":7},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Data"},"type":"lvl3","url":"/pangeo-on-earthcode#data","position":8},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Data"},"content":"We will use Sentinel-2 data accessed via \n\nelement84’s STAC API endpoint and the \n\nSeasFire Data Cube to find burned areas, inspect them in more detail and generate burn severity maps for the assessment of the areas affected by wildfires.","type":"content","url":"/pangeo-on-earthcode#data","position":9},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl5":"Related publications","lvl3":"Data"},"type":"lvl5","url":"/pangeo-on-earthcode#related-publications","position":10},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl5":"Related publications","lvl3":"Data"},"content":"https://​www​.sciencedirect​.com​/science​/article​/pii​/S1470160X22004708​#f0035\n\nhttps://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire Analysis Compiled Scripts (Gadi)/dNBR_full.py\n\nAlonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. \n\nAlonso et al. (2024). The same dataset can also be downloaded from Zenodo: \n\nhttps://​zenodo​.org​/records​/13834057\n\nhttps://​registry​.opendata​.aws​/sentinel​-2​-l2a​-cogs/\n\n","type":"content","url":"/pangeo-on-earthcode#related-publications","position":11},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Import Packages","lvl3":"Data"},"type":"lvl4","url":"/pangeo-on-earthcode#import-packages","position":12},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Import Packages","lvl3":"Data"},"content":"As best practices dictate, we recommend that you install and import all the necessary libraries at the top of your Jupyter notebook.\n\nimport json\nimport xarray\nimport rasterio\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\nimport hvplot.xarray\nimport dask.distributed\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport shapely\n\nfrom pystac_client import Client as pystac_client\nfrom odc.stac import stac_load\n\nimport os\nimport xrlint.all as xrl\nfrom xcube.core.verify import assert_cube\n\n\n","type":"content","url":"/pangeo-on-earthcode#import-packages","position":13},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Create and startup your Dask Cluster"},"type":"lvl2","url":"/pangeo-on-earthcode#create-and-startup-your-dask-cluster","position":14},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Create and startup your Dask Cluster"},"content":"Create dask cluster as described in the \n\ndask 101 guide.\n\nfrom dask_gateway import Gateway\ngateway = Gateway()\n\ncluster_options = gateway.cluster_options()\ncluster_options.profile = 'medium'\ncluster_options\n\ncluster = gateway.new_cluster(cluster_options=cluster_options)\ncluster.scale(2)\ncluster\n\nclient = cluster.get_client()\nclient\n\n# or create a local dask cluster on a local machine.\n\n# from dask.distributed import Client\n# client = Client()   \n# client\n# # client.close()\n\n","type":"content","url":"/pangeo-on-earthcode#create-and-startup-your-dask-cluster","position":15},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Load the Data"},"type":"lvl2","url":"/pangeo-on-earthcode#load-the-data","position":16},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Load the Data"},"content":"Lazy load in data in Xarray as described in the \n\nxarray 101 guide. Note that the SeasFire data is +100GB and hosted remotely, we are just loading metadata, xarray will load only the chunks (dask data arrays) needed for our computations as discussed in the \n\ndask 101 guide.\n\nThe SeasFire Cube contains various variables for forecasting seasonal fires across the globe, we will specifically be using it to access historical wildfire data:\n\nThe SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses ‘Earth System Deep Learning for Seasonal Fire Forecasting’ and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions.\n\nhttp_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n\nds = xarray.open_dataset(\n\thttp_url,\n\tengine='zarr',\n    chunks={},\n\tconsolidated=True\n\t# storage_options = {'token': 'anon'}\n)\nds\n\n","type":"content","url":"/pangeo-on-earthcode#load-the-data","position":17},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analyse Wildfires"},"type":"lvl2","url":"/pangeo-on-earthcode#analyse-wildfires","position":18},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analyse Wildfires"},"content":"Our first task will be to find the date of the largest burnt area over the region we were previously exploring, so that we may analyse it in detail.\n\nOne of the data variables from the SeasFire DataCube is the Burned Areas data from Global Wildfire Information System (GWIS). Each entry gives us the hectares (ha) of burnt area in that region. Note that our input resolution is 0.25 degrees (approx. 28x28 km per pixel), spanning 20 years (2001-2021) at an 8 days time resolution.\n\nFirst we lazy load just the GWIS dataset in a variable for convenience (note that again, this does not take up a large amount of memory since we’re just working with the metadata at this point)\n\ngwis = ds.gwis_ba\ngwis\n\n","type":"content","url":"/pangeo-on-earthcode#analyse-wildfires","position":19},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Spatial and Temporal Subsetting"},"type":"lvl2","url":"/pangeo-on-earthcode#spatial-and-temporal-subsetting","position":20},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Spatial and Temporal Subsetting"},"content":"It’s best practice to subset only the relevant data before doing computations, to reduce needed computations and data downloads. Especialyl when working with larger datasets. We will select an area of interest (aoi) as below using the polygon we previously downloaded from the xcube viewer.\n\n# Spatial Slices\nepsg = 4326 # our data's projection\n\ngdf = gpd.read_file(\"../aoi/feature.geojson\")\ngdf = gdf.set_crs(epsg=epsg)\ngdf.explore()\n\n\n\nmin_lon, min_lat, max_lon, max_lat = gdf.total_bounds\n\n# find the nearest points on our grid\nlat_start = gwis.latitude.sel(latitude=max_lat, method=\"nearest\").item()\nlat_stop  = gwis.latitude.sel(latitude=min_lat, method=\"nearest\").item()\nlon_start = gwis.longitude.sel(longitude=min_lon, method=\"nearest\").item()\nlon_stop  = gwis.longitude.sel(longitude=max_lon, method=\"nearest\").item()\n\nlat_slice = slice(lat_start, lat_stop)\nlon_slice = slice(lon_start, lon_stop)\n\nlon_start, lat_start, lon_stop, lat_stop\n\nbbox=[lon_stop , lat_stop , lon_start, lat_start]\n\nWe additionally will subest our data temporally to fetch only more recent periods from 2018 - 2021.\n\ntime_slice = slice('2018-01-01','2021-01-01')\n\nWe now simply subset the data using the slices we created. Note how the data of our computation has reduced significantly\n\n# spatio temporal subset\ngwis_aoi = gwis.sel(time=time_slice, latitude=lat_slice,longitude=lon_slice)\ngwis_aoi\n\n","type":"content","url":"/pangeo-on-earthcode#spatial-and-temporal-subsetting","position":21},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Finding the date with the most burnt area"},"type":"lvl2","url":"/pangeo-on-earthcode#finding-the-date-with-the-most-burnt-area","position":22},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Finding the date with the most burnt area"},"content":"To find the date with the most burnt area we first do a spatial sum over our data for each period; this will return an dataarray with the total amount of ha of burnt area for each day. We will then fetch the date with the max burned area using idmax.\n\nNote idmax is equivalent to doing ds.sel(time=ds.argmax.item(dim=‘time’)) where argmax returns the index of the max item\n\n# date where the sum of burnt area is the highest\ndate_max_fire = gwis_aoi.sum(dim={'latitude','longitude'}).idxmax(dim='time')\n\nBy subsetting only to the relevant data, we have reduced our dataset from 3.7 GB to a very small managable chunk of relevant data. Note that up to this point, we have not downloaded any data or done any computations over it. We (via the Dask Client) have just built up in the background the dask task graph.\n\ndate_max_fire.data.visualize(optimize_graph=True)\n\n","type":"content","url":"/pangeo-on-earthcode#finding-the-date-with-the-most-burnt-area","position":23},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Let’s start computing!","lvl2":"Finding the date with the most burnt area"},"type":"lvl3","url":"/pangeo-on-earthcode#lets-start-computing","position":24},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Let’s start computing!","lvl2":"Finding the date with the most burnt area"},"content":"Let’s get our date by forcing dask to execute the above task graph using .compute()\n\ndate_max_fire = date_max_fire.compute()\ndate_max_fire\n\n","type":"content","url":"/pangeo-on-earthcode#lets-start-computing","position":25},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Exploring the Data"},"type":"lvl2","url":"/pangeo-on-earthcode#exploring-the-data","position":26},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Exploring the Data"},"content":"Great, we’ve found the date with the most burnt areas, now let’s explore the data\n\nbiggest_fire_aoi = gwis_aoi.sel(time=date_max_fire)\nbiggest_fire_aoi\n\nPlot the forest fire areas to get an idea about our data\n\nLet’s plot our data to explore it a bit further\n\nbiggest_fire_aoi.plot()\n\nThe image above does not really give us a detailed view of which areas have been burned and to what degree, as the resolution of each pixel is approx 28km. To get some more context of our data we can plot it on a base map.\n\nIn the Pangeo stack there are visualization tools that can help us easily plot data in a more interactive way, with a simple interface - such as hvplot.\n\n# Plot it interactively, with some context\nbiggest_fire_aoi.hvplot(\n    x='longitude',\n    y='latitude',\n    cmap='viridis',\n    colorbar=True,\n    frame_width=600,\n    frame_height=400,\n    geo=True,\n    tiles='OSM',\n    alpha=0.5\n)\n\n\n","type":"content","url":"/pangeo-on-earthcode#exploring-the-data","position":27},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analysing in Detail"},"type":"lvl2","url":"/pangeo-on-earthcode#analysing-in-detail","position":28},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analysing in Detail"},"content":"Now that we have an idea of a potential wildfire event, we can explore in a bit more detail to see what areas where affected most.\n\nWe will use higher resolution sentinel-2-l2a data to compute the Normalised Burn Ratio (NBR) index to highlight burned areas. The NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths. We will use a STAC API \n\nelement84’s STAC API endpoint to search for the data and load only the relevant, cloud-free data.\n\nTo estimate the amount of burnt area we will compute the difference between the NBR from period before the fire date and the NBR from the period after. The first step is to select the week before and the week after the wildfire\n\nfire_date_t = pd.to_datetime(date_max_fire.values.item()) # get the date of the forest fire and a the dates before and after it\nweek_before = (fire_date_t - timedelta(days=7))\nweek_after = (fire_date_t + timedelta(days=7))\n\nprint(week_before.date(), \"---\" , week_after.date())\n\n","type":"content","url":"/pangeo-on-earthcode#analysing-in-detail","position":29},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Fetching the data","lvl2":"Analysing in Detail"},"type":"lvl3","url":"/pangeo-on-earthcode#fetching-the-data","position":30},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Fetching the data","lvl2":"Analysing in Detail"},"content":"We will now fetch the data using pystac to search through the \n\nearth​-search​.aws​.element84​.com​/v1 STAC API endpoint. As mentioned in the \n\nSTAC and Data Access guide, the STAC API provides us with endpoints to query STAC collections.\n\nThere is an ecosystem of libraries that implement STAC standards (pystac, odc-stac, stackstac, etc..) that allow us to analyse, load and use data described in STAC. We will briefly explore in the following cells below.\n\nAs a first step, we will open a catalog with the pystac library: \n\nhttps://​earth​-search​.aws​.element84​.com​/v1 This catalog contains STAC collections of various datasets (such as sentinel-1 and sentinel-2 data) in cloud optimised formats (COGs in this case) - see \n\nhttps://​earth​-search​.aws​.element84​.com. for more details. With these formats we are able to use the assets using cloud-native access patterns (see \n\nhttps://​guide​.cloudnativegeo​.org​/cookbooks/ for more details).\n\nNote that a STAC endpoint like the one below, only returns STAC items and not the actual data. The STAC items returned have Assets defined which link to the actual data (and provide metadata).\n\ncatalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\nchunk={} # <-- use dask\nres=10 # 10m resolution\n\nIn the next step we will do two things:\n\n1. Search for multiple cloudless sentinel-2 satellite images within a month of our pre-fire (week_before) date. The STAC API allows us to do a couple of things in a simple API call:\n\nWe can define an arrea of interest (bbox) and search for items that cover this region\n\nWe can subset for the time of interest as well (datetime)\n\nWe can define custom querries over implemented STAC extensions. For example, the Sentinel STAC collection we are querrying implements the eo STAC extension, and defines cloud_cover - this allows us to search for quality assets with minimal cloudy pixels\n\nThis will return the relevant STAC items - which contain assets that point to our data of interest. We now need to load this data into a dataarray to start analysing it.\n\n2. Note that in the above step, we only have items that point to some data - this data can be tiff, zarr, netcdf, COG, or other SpatioTemporal asset data. In our case, the element84 endpoint points to the data collection of  https://​registry​.opendata​.aws​/sentinel​-2​-l2a​-cogs/ - the format of our data is cloud-optimized GeoTiff. A cloud-friendly format such as this enables us cloud-native access patterns, such as easily fetching only our area of interest (as opposed to several tiff files and manually subsetting them after downloading).\n\nLibraries such as odc-stac integrate with STAC standards and allow us to load data as well as leveraging the cloud-optimised formats. For example, in the cell below we define how we want to transform/load our data by:\n\nPassing the STAC item (or multiple items) we want to load\n\ndefining a particular chunk size (the passed {} asks for the data to be automatically chunked as it is originally);\n\nWe can request only the spectral bands of interest, in this case nir and swir22, to reduce the amount of data that we fetch.\n\nWe can define a resolution to retrieve the data at, note that this will also resample automatically. For example the nir band data has a 10m resolution, but the swir22 - 20m resolution.\n\nThere are multiple other options as well, such as defining in which projection we want our data in. More information can be found at: \n\nhttps://​odc​-stac​.readthedocs​.io​/en​/latest​/​_api​/odc​.stac​.load​.html\n\n# STAC search for relevant items\nweek_before_start = (week_before - timedelta(days=30))\ntime_range_start = str(week_before_start.date()) + \"/\" + str(week_before.date())\n\nquery1 = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=time_range_start, limit=100,\n    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\nitems = list(query1.items())\nprint(f\"Found: {len(items):d} datasets\")\n\n# plot all the STAC assets\npoly_pre = gpd.GeoSeries([shapely.Polygon(item.geometry['coordinates'][0]) for item in items], name='geometry', crs='epsg:4236')\npoly_pre.explore()\n\nprefire_ds = stac_load(\n    items,\n    bands=(\"nir\", \"swir22\"),\n    chunks=chunk,  # <-- use Dask\n    resolution=res,\n    crs=\"EPSG:32629\",\n    groupby=\"datetime\",\n    bbox=bbox,\n)\nprefire_ds = prefire_ds.mean(dim=\"time\")\nprefire_ds\n\nWe now do the same for the month after..\n\nweek_after_end = (week_after + timedelta(days=30))\ntime_range_end = str(week_after.date()) + \"/\" + str(week_after_end.date())\n\nquery2 = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=time_range_end, limit=100,\n    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\nitems = list(query2.items())\nprint(f\"Found: {len(items):d} datasets\")\n\npoly_post = gpd.GeoSeries([shapely.Polygon(item.geometry['coordinates'][0]) for item in items], name='geometry', crs='epsg:4236')\npoly_post.explore()\n\npostfire_ds = stac_load(\n    items,\n    bands=(\"nir\", \"swir22\"),\n    chunks=chunk,  # <-- use Dask\n    resolution=res,\n    crs=\"EPSG:32629\",\n    groupby=\"datetime\",\n    bbox=bbox,\n)\npostfire_ds = postfire_ds.mean(dim=\"time\")\npostfire_ds\n\nmax_poly_pre = poly_pre[[poly_pre.to_crs(epsg=3035).area.argmax()]]\nmax_poly_post = poly_post[[poly_post.to_crs(epsg=3035).area.argmax()]]\n\n# note we're reprojecting to calculate area, as Geometry is in a geographic CRS. Results from 'area' are incorrect since geopandas doesn't calc spherical geometry!\npoly_pre.area\n\nm = max_poly_pre.explore( )\nm = max_poly_post.explore(m=m, color='r')\nm\n\nNote: The above STAC API query might return assets that intersect the region but do not fully cover it. I.e. if we fetched only one asset for each period with the above query they could potentially point to different regions (as there would be data missing). Although not relevant in this example, as all assets from both pre and after overlap, this will be a problem if not checked (as you’d be analysing different regions). In this example we visually inspect the overlap between assets and further take the mean of all the assets to get better quality pixels across time. Rerun the same example with cloud_cover = 0.2 to see where you might get some problem code!\n\n","type":"content","url":"/pangeo-on-earthcode#fetching-the-data","position":31},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Calculating the NBR index"},"type":"lvl2","url":"/pangeo-on-earthcode#calculating-the-nbr-index","position":32},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Calculating the NBR index"},"content":"In the next step we will calculate our index, using simple band math as explained in the \n\nxarray 101 guide. We first define a function that calculates our index given a dataset with nir/swir22 band data and then add the additional variable to our pre-fire/post-fire datasets\n\nindex_name = 'NBR'\n\n# Normalised Burn Ratio, Lopez Garcia 1991\ndef calc_nbr(ds):\n    return (ds.nir - ds.swir22) / (ds.nir + ds.swir22)\n\nindex_dict = {'NBR': calc_nbr}\nindex_dict\n\n# prefire - calls the calc_nbr function with our dataset as input to create a new NBR index (dict) and assigns it as the NBR index \n# note that we have to apply scaling (1000) and offset (0) to our band data - as defined in the dataset collection\nprefire_ds[index_name] = index_dict[index_name](prefire_ds / 10000.0) \n\n# postfire - calls the calc_nbr function with our dataset as input to create a new NBR index (dict) and assigns it as the NBR index \n# note that we have to apply scaling (1000) and offset (0) to our band data - as defined in the dataset collection\npostfire_ds[index_name] = index_dict[index_name](postfire_ds / 10000.0)\n\nNow that we have the indecies calculated we can calculate the difference in burnt area between the two periods to analyse which regions have been burnt.\n\nNote that at this point we haven’t actually loaded the data or done the calculations, we’ve just defined a task graph for dask to execute. When we call persist() the graph up to that point will be executed and the data saved for in the distributed memory of our workers. We do this at this stage to avoid fetching the data multiple times in future computations.\n\nWe then save our result (which is a dataarray) in a new dataset: dnbr_dataset.\n\n# calculate delta NBR\nprefire_burnratio = prefire_ds.NBR.persist() # <--- load and keep data into your workers\npostfire_burnratio = postfire_ds.NBR.persist() # <--- load and keep data into your workers\n\ndelta_NBR = prefire_burnratio - postfire_burnratio\n\ndnbr_dataset = delta_NBR.to_dataset(name='delta_NBR').persist()\n\ndnbr_dataset\ndelta_NBR\n\n","type":"content","url":"/pangeo-on-earthcode#calculating-the-nbr-index","position":33},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Plotting and visualization"},"type":"lvl2","url":"/pangeo-on-earthcode#plotting-and-visualization","position":34},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Plotting and visualization"},"content":"We will now plot our data from before, after and the delta of our wildfire event to further analyse. Note that, this will trigger the execution of our dask task graph. The Pangeo stack offers many tools for visualization! In the example below we will showcase plotting with Cartopy\n\n","type":"content","url":"/pangeo-on-earthcode#plotting-and-visualization","position":35},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting Before","lvl2":"Plotting and visualization"},"type":"lvl3","url":"/pangeo-on-earthcode#plotting-before","position":36},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting Before","lvl2":"Plotting and visualization"},"content":"\n\nfig = plt.figure(1, figsize=[7, 10])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\nprefire_burnratio.plot(ax=ax, transform=ccrs.epsg(prefire_burnratio.spatial_ref.values), cmap='RdBu_r',\n                       cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title(\"Pre-Fire: \" + time_range_start, fontsize=18)\n\n","type":"content","url":"/pangeo-on-earthcode#plotting-before","position":37},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting After","lvl2":"Plotting and visualization"},"type":"lvl3","url":"/pangeo-on-earthcode#plotting-after","position":38},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting After","lvl2":"Plotting and visualization"},"content":"\n\nfig = plt.figure(1, figsize=[7, 9])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\npostfire_burnratio.plot(ax=ax, transform=ccrs.epsg(postfire_burnratio.spatial_ref.values), cmap='RdBu_r',\n                        cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title(\"Post-Fire: \" + time_range_end, fontsize=18)\n\n","type":"content","url":"/pangeo-on-earthcode#plotting-after","position":39},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting Delta","lvl2":"Plotting and visualization"},"type":"lvl3","url":"/pangeo-on-earthcode#plotting-delta","position":40},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting Delta","lvl2":"Plotting and visualization"},"content":"The plot below highligths the burnt regions\n\nfig = plt.figure(1, figsize=[7, 10])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\ndnbr_dataset.delta_NBR.plot(ax=ax, transform=ccrs.epsg(dnbr_dataset.delta_NBR.spatial_ref.values), cmap='RdBu_r',\n                            cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title( \"Delta NBR\", fontsize=18)\n\n","type":"content","url":"/pangeo-on-earthcode#plotting-delta","position":41},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analysing Burnt Areas"},"type":"lvl2","url":"/pangeo-on-earthcode#analysing-burnt-areas","position":42},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Analysing Burnt Areas"},"content":"Great, now that we’ve calculated the delta NBR (dNBR), we can estimate the amount of area that was burned. First we need to classify which pixels are actually burned and then calculate the area that was burned.\n\nTo classify burned area we can use the dNBR reference ratio ratios described in: \n\nhttps://​un​-spider​.org​/advisory​-support​/recommended​-practices​/recommended​-practice​-burn​-severity​/in​-detail​/normalized​-burn​-ratio - and estimate the low to severe burnt areas. i.e. the areas with a dNBR of above 0.270\n\nBURN_THRESH = 0.270\nburn_mask = dnbr_dataset.delta_NBR > BURN_THRESH           # True/False mask, same shape as raster\nburn_mask.plot()\n\n# save the burn_mask to our dataset\ndnbr_dataset['burned_ha_mask'] = burn_mask\n\n","type":"content","url":"/pangeo-on-earthcode#analysing-burnt-areas","position":43},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Simple Validation"},"type":"lvl2","url":"/pangeo-on-earthcode#simple-validation","position":44},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Simple Validation"},"content":"Now with the above a data mask where all pixels classified as burnt are 1 and the non burnt ones are 0. To calculate burnt area, we simply sum all pixels and multiply them by the area which a pixel covers (i.e. the resolution, which in our case is 10m/p).\n\nAt the same step we will convert our metric to hectares (ha) so that we can compare and validate our findings against the original GWIS data.\n\nNote that in an actual study, the ground truth data and validation would involve field studies and more precise validation, but we simplify the example for the training.\n\n\n\ndx, dy = dnbr_dataset.delta_NBR.rio.resolution()\npixel_area_ha = abs(dx * dy) / 1e4       # 10m × 10m = 0.01 ha\n\npixels_burned   = burn_mask.sum().compute().item()   # integer number of burned pixels\nburned_area_ha  = pixels_burned * pixel_area_ha\n\nprint(f\"Burned area   : {burned_area_ha:,.2f} ha\")\nprint(f\"Actual Burned Area : {biggest_fire_aoi.sum().compute():,.2f}, ha\")\n\nThere’s only a difference of about 100 ha, which, given our simple analysis, is actually quite accurate!\n\nnote this is based on the geojson feature in the github repo with the following result:\n\nBurned area   : 2,373.14 ha\n\nActual Burned Area : 2,214.27, ha","type":"content","url":"/pangeo-on-earthcode#simple-validation","position":45},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting and Analysing Final Results","lvl2":"Simple Validation"},"type":"lvl3","url":"/pangeo-on-earthcode#plotting-and-analysing-final-results","position":46},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Plotting and Analysing Final Results","lvl2":"Simple Validation"},"content":"We can visually inspect our plot to see if our data follows the same trends as our ground truth GWIS dataset.\n\nTo achieve this though, we need to reproject our data, since the sentinel-2 data we are using is in a different projection with a higher resolution (10m/pixel) and the SeasFire GWIS ground truth data has a resolution of 0.25 degrees (approx 28km x 28 km).\n\nWe will do this using the rioxarray library, which offers convenient methods such as reproject_match, which takes in as an argument a dataarray to which to match the original dataset. It essentially reprojects and matches the grid of a datarray to match the grid/projection of another and upsamples (or downsamples) the data to match resolutions, as well as subsetting the data to match.\n\nLearn more about how it works at: \n\nhttps://​corteva​.github​.io​/rioxarray​/stable​/examples​/reproject​_match​.html\n\n# ensure that our dataarray has metadata about its projection - this is used by rioxarray.\nbiggest_fire_aoi = biggest_fire_aoi.rio.write_crs(ds.rio.crs)\n\nbiggest_fire_aoi_reprojected = biggest_fire_aoi.rio.reproject_match(dnbr_dataset.burned_ha_mask)\n\n\nburned_ha_mask_plot = dnbr_dataset.burned_ha_mask.hvplot(\n    width=700,\n    height=700,\n    title='dNBR (10 m) with GWIS overlay',\n    alpha=1.0\n)\n\n# Plot the reprojected coarse dataset as transparent overlay\ngwis_plot = biggest_fire_aoi_reprojected.hvplot(\n    cmap='Reds',\n    alpha=0.3,\n    clim=(0, biggest_fire_aoi.max().compute().item())\n)\n\n# Combine them interactively\ncombined_plot = burned_ha_mask_plot * gwis_plot\n\ncombined_plot\n\n\nGreat! Our plot generally follows the same trends as our burn analysis, the north-eastern and north-eastern regions seemed to be mainly burnt with two separately affected regions.\n\nOur plot and values are quite a bit off though if you try to sum them - why is that?\n\nHomework: Fix it and add it as a variable!\n\nhint: ha\n\n","type":"content","url":"/pangeo-on-earthcode#plotting-and-analysing-final-results","position":47},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Saving Your Work"},"type":"lvl2","url":"/pangeo-on-earthcode#saving-your-work","position":48},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Saving Your Work"},"content":"Now let’s save our work! For intraoperability, we will save our data as a valid data cube in Zarr.\n\nKeep in mind that EarthCODE provides different tooling that makes it easy to publish our data to the wider EO community on the EarthCODE Open Science Catalog (such as deep-code for publishing data cubes, we will see in the \n\npublishing guide) by following common standards and using common file formats we ensure that there will be a tool to help us!","type":"content","url":"/pangeo-on-earthcode#saving-your-work","position":49},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Linting","lvl2":"Saving Your Work"},"type":"lvl3","url":"/pangeo-on-earthcode#linting","position":50},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Linting","lvl2":"Saving Your Work"},"content":"There are also tools to ensure the quality of our data/metadata. Two very useful tools are xrlint which check our datarray against common expected standards and advise corrections (such as missing attributes which should be filled in). It’s basically a linter for xarray - xarray + linter.\n\nlinter = xrl.new_linter(\"recommended\")\nlinter.validate(dnbr_dataset)\n\n\n","type":"content","url":"/pangeo-on-earthcode#linting","position":51},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Add metadata descriptions to our data","lvl3":"Linting","lvl2":"Saving Your Work"},"type":"lvl4","url":"/pangeo-on-earthcode#add-metadata-descriptions-to-our-data","position":52},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Add metadata descriptions to our data","lvl3":"Linting","lvl2":"Saving Your Work"},"content":"As we see, there is quite a few attributes missing from our data cube. As a best practices, it’s recommended to have it very well described, to ensure FAIR-ness and, specifically, intraoperability.\n\n# Assign dataset-level attributes\ndnbr_dataset.attrs.update({\n    'title': 'Delta NBR and Burned Area Mask Dataset',\n    'history': 'Created by reprojecting and aligning datasets for fire severity analysis',\n    'Conventions': 'CF-1.7'\n})\n\n\n# Assign variable-level attributes for delta_NBR\ndnbr_dataset.delta_NBR.attrs.update({\n    'institution': 'Lampata',\n    'source': 'Sentinel-2 imagery; processed with open-source dNBR code, element84...',\n    'references': 'https://registry.opendata.aws/sentinel-2-l2a-cogs/',\n    'comment': 'dNBR values represent change in vegetation severity post-fire',\n    'standard_name': 'difference_normalized_burn_ratio',\n    'long_name': 'Differenced Normalized Burn Ratio (dNBR)',\n    'units': 'm'\n})\n\n# Example for burned_ha_mask data variable\ndnbr_dataset.burned_ha_mask.attrs.update({\n    'standard_name': 'burned_area_mask',\n    'long_name': 'Burned Area Mask in Hectares',\n    'units': 'hectares',\n    'institution': 'Your Institution Name',\n    'source': 'Derived from wildfire impact analysis',\n    'references': 'https://example.com/ref',\n    'comment': 'Burned area mask showing presence of burned areas'\n})\n\n\nGreat! Let’s re-run the linter and see if we’re missing anything.","type":"content","url":"/pangeo-on-earthcode#add-metadata-descriptions-to-our-data","position":53},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Data Cube Validation","lvl2":"Saving Your Work"},"type":"lvl3","url":"/pangeo-on-earthcode#data-cube-validation","position":54},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Data Cube Validation","lvl2":"Saving Your Work"},"content":"Another useful tool is xcube’s assert cube method which validates our data cube (e.g. dimensions, chunks, grids between dataarray’s match and other checks - see more information at \n\nhttps://​xcube​.readthedocs​.io​/en​/latest​/cubespec​.html​#xcube​-dataset​-convention). Making sure we validate our data before saving or sharing will ensure that our data can be exploited by different, common EO libraries.\n\nassert_cube(dnbr_dataset)  # raises ValueError if it's not xcube-valid\n\nIt seems like our data is not actually valid, as we missed timestamping our data burnt area final dataset! There’s not really any information in the data to tell us when this wildfire event actually happened as well. Since deltaNBR highlights the burnt area after a fire, we will add the date of the postfire_ds to the dataset.\n\nSimple checks like this help us avoid making simple mistakes, and catching/correcting these errors early on is  much easier than at a later stage.\n\nimport numpy as np\n\ntimestamp = np.datetime64(items[-1].properties['created'])\n\n# add time\ndnbr_dataset = dnbr_dataset.expand_dims(time=[timestamp])\n\ndnbr_dataset\n\n","type":"content","url":"/pangeo-on-earthcode#data-cube-validation","position":55},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Why is this important?","lvl3":"Data Cube Validation","lvl2":"Saving Your Work"},"type":"lvl4","url":"/pangeo-on-earthcode#why-is-this-important","position":56},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl4":"Why is this important?","lvl3":"Data Cube Validation","lvl2":"Saving Your Work"},"content":"By ensuring your data follows common standards and FAIR principles, firstly and most importantly you enable make it usable by others and therefore increase its impact!\n\nYou also enable libraries that implement the common standards you follow to use your data. For example, for our dataset above, if we do not apply the corrections (for time specifically) any future applications using xcube won’t be able to open up our dataset. Applications or users trying to understand how we derived our data will not have information, and etc...\n\nThe tools and standards help us along on the way to FAIR-ness!\n\n","type":"content","url":"/pangeo-on-earthcode#why-is-this-important","position":57},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Saving and Chunking","lvl2":"Saving Your Work"},"type":"lvl3","url":"/pangeo-on-earthcode#saving-and-chunking","position":58},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl3":"Saving and Chunking","lvl2":"Saving Your Work"},"content":"To make our data easier to use for future users we will chunk it the recommended chunk sizes.\n\ndnbr_dataset = dnbr_dataset.chunk({\"time\": 1, \"y\": 1000, \"x\": 1000}).load()\nprint(type(dnbr_dataset.burned_ha_mask.data)) # check data format \n\nFinally we create our local zarr store with .to_zarr. This will save the dataset locally.\n\nNote: You can easily store it on cloud storge such as s3 with some slight edits to the code below.\n\nsave_at_folder = '../wildfires'\nif not os.path.exists(save_at_folder):\n    os.makedirs(save_at_folder)\n\n# Define the output path within your notebook folder\noutput_path = os.path.join(save_at_folder, \"dnbr_dataset.zarr\")\n\n# save\ndnbr_dataset.to_zarr(output_path, mode=\"w\")\n\n","type":"content","url":"/pangeo-on-earthcode#saving-and-chunking","position":59},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Conclusion"},"type":"lvl2","url":"/pangeo-on-earthcode#conclusion","position":60},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Conclusion"},"content":"Congratulations! You’ve gotten to the end of this notebook. We explored a simple workflow with the \n\nPangeo open-source software stack, demonstrated how to fetch and access EarthCODE published data and publically available Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires. Finally, we saved our work and learned about why FAIRness matters.\n\nAs next steps we recommend reading through the documentation pages of each of the libraries, and going through the example in your own time in more detail!\n\nexploring the EarthCODE catalog\n\n","type":"content","url":"/pangeo-on-earthcode#conclusion","position":61},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Links and Refernces"},"type":"lvl2","url":"/pangeo-on-earthcode#links-and-refernces","position":62},{"hierarchy":{"lvl1":"Example EarthCODE Workflow","lvl2":"Links and Refernces"},"content":"https://​earthcode​.esa​.int/\n\nhttps://​opensciencedata​.esa​.int/\n\nhttps://​www​.sciencedirect​.com​/science​/article​/pii​/S1470160X22004708​#f0035\n\nhttps://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire Analysis Compiled Scripts (Gadi)/dNBR_full.py\n\nAlonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. \n\nAlonso et al. (2024). The same dataset can also be downloaded from Zenodo: \n\nhttps://​zenodo​.org​/records​/13834057\n\nhttps://​opensciencedata​.esa​.int​/products​/seasfire​-cube​/collection)\n\nhttps://​un​-spider​.org​/advisory​-support​/recommended​-practices​/recommended​-practice​-burn​-severity​/in​-detail​/normalized​-burn​-ratio](\n\nhttps://​un​-spider​.org​/advisory​-support​/recommended​-practices​/recommended​-practice​-burn​-severity​/in​-detail​/normalized​-burn​-ratio\n\nhttps://​registry​.opendata​.aws​/sentinel​-2​-l2a​-cogs/\n\nelement84’s STAC API](\n\nhttps://​element84​.com​/earth​-search/)\n\nhttps://​github​.com​/bcdev​/xrlint​/tree​/main​/docs\n\nhttps://​xcube​.readthedocs​.io​/en​/latest/\n\nhttps://​docs​.dask​.org​/en​/stable/\n\nhttps://​docs​.xarray​.dev​/en​/stable\n\nhttps://​zarr​.readthedocs​.io​/en​/stable/\n\nhttps://​corteva​.github​.io​/rioxarray​/stable​/examples​/reproject​_match​.html","type":"content","url":"/pangeo-on-earthcode#links-and-refernces","position":63},{"hierarchy":{"lvl1":"EDC Pangeo Cloud Platform"},"type":"lvl1","url":"/about-edc-pangeo","position":0},{"hierarchy":{"lvl1":"EDC Pangeo Cloud Platform"},"content":"","type":"content","url":"/about-edc-pangeo","position":1},{"hierarchy":{"lvl1":"EDC Pangeo Cloud Platform","lvl2":"About the Environment: Introducing the EDC Pangeo Cloud Platform"},"type":"lvl2","url":"/about-edc-pangeo#about-the-environment-introducing-the-edc-pangeo-cloud-platform","position":2},{"hierarchy":{"lvl1":"EDC Pangeo Cloud Platform","lvl2":"About the Environment: Introducing the EDC Pangeo Cloud Platform"},"content":"You can now work with the Pangeo stack on the EDC EOxHub!\n\nWhat is Pangeo?\n\nPangeo is a community-driven open-source ecosystem designed for scalable, Pythonic big data analysis. It brings a suite of powerful libraries that make working with large Earth observation and climate datasets easier and more interactive.\n\nKey Tools in Pangeo that you will learn about today:\n\nXarray: Handles multi-dimensional labeled arrays (like NetCDF), with intuitive syntax that feels natural for Python users.\n\nSTAC: Simplifies discovery and access to large Earth observation datasets with a consistent, JSON-based catalog standard.\n\nDask: Enables parallel and distributed computing, scaling analysis from laptops to powerful cloud clusters.\n\nZarr: Provides efficient, cloud-native storage for chunked and compressed data, perfect for massive datasets.\n\nJupyter: Delivers an interactive, shareable environment for building and sharing workflows.\n\nThese tools work together embracing the Pythonic way of data science and enabling intuitive exploration and high-performance analysis in the cloud.\n\nEuro Data Cube (EDC)A cloud platform for Earth observation data access and analysis, featuring a rich data catalog (Sentinel, Landsat, Copernicus, etc.), cloud-native analytics, and collaborative sharing tools for environmental, disaster, and climate applications. See more details at: \n\neurodatacube.com\n\nEDC EOxHub Workspace OfferingThe EDC EOxHub Workspaces provide managed JupyterLab environments with curated images for EO projects. They offer customizable computing resources, persistent storage, and fast network connections. Users can run notebooks and applications, with fair use limits based on resource consumption. See more details at: \n\nEOxHub Workspace\n\nNetwork of Resources Sponsorship!\n\nNoR provides non-commercial and commercial users with a unique environment to discover European cloud services and their estimate costs for Earth Observation exploitation. ESA offers sponsorship to eligible entities to cover the costs of trying out the various services.\n\nYou can apply to sponsor your project’s compute and data access at: \n\nhttps://​nor​-discover​.org/","type":"content","url":"/about-edc-pangeo#about-the-environment-introducing-the-edc-pangeo-cloud-platform","position":3},{"hierarchy":{"lvl1":"EarthCODE and Pangeo"},"type":"lvl1","url":"/earthcode101","position":0},{"hierarchy":{"lvl1":"EarthCODE and Pangeo"},"content":"","type":"content","url":"/earthcode101","position":1},{"hierarchy":{"lvl1":"Accessing EarthCODE Data"},"type":"lvl1","url":"/stac-and-data-access","position":0},{"hierarchy":{"lvl1":"Accessing EarthCODE Data"},"content":"","type":"content","url":"/stac-and-data-access","position":1},{"hierarchy":{"lvl1":"Accessing EarthCODE Data","lvl2":"Accessing EarthCODE Data"},"type":"lvl2","url":"/stac-and-data-access#accessing-earthcode-data","position":2},{"hierarchy":{"lvl1":"Accessing EarthCODE Data","lvl2":"Accessing EarthCODE Data"},"content":"\n\n","type":"content","url":"/stac-and-data-access#accessing-earthcode-data","position":3},{"hierarchy":{"lvl1":"Accessing EarthCODE Data","lvl3":"Packages","lvl2":"Accessing EarthCODE Data"},"type":"lvl3","url":"/stac-and-data-access#packages","position":4},{"hierarchy":{"lvl1":"Accessing EarthCODE Data","lvl3":"Packages","lvl2":"Accessing EarthCODE Data"},"content":"As best practices dictate, we recommend that you install and import all the necessary libraries at the top of your Jupyter notebook.\n\nfrom pystac.extensions.storage import StorageExtension\nfrom datetime import datetime\n\nfrom pystac_client import Client as pystac_client\nfrom odc.stac import configure_rio, stac_load\n\nimport pystac\nimport xarray\n\nfrom datetime import timedelta\nimport pandas as pd\nimport shapely\n\n\nimport hvplot.xarray\n\n\nimport geopandas as gpd\nepsg = 4326 # our data's projection\n\ngdf = gpd.read_file(\"../aoi/feature.geojson\")\ngdf = gdf.set_crs(epsg=epsg)\ngdf.explore()\n\n","type":"content","url":"/stac-and-data-access#packages","position":5},{"hierarchy":{"lvl1":"What is STAC?"},"type":"lvl1","url":"/stac-and-data-access#what-is-stac","position":6},{"hierarchy":{"lvl1":"What is STAC?"},"content":"(summarised from \n\nhttps://​stacspec​.org​/en​/tutorials​/intro​-to​-stac/)\n\nThe SpatioTemporal Asset Catalog (STAC) specification was designed to establish a standard, unified language to talk about geospatial data, allowing it to be more easily searchable and queryable.\n\nSTAC has been designed to be simple, flexible, and extensible. STAC is a network of JSON files that reference other JSON files, with each JSON file adhering to a specific core specification depending on which STAC component it is describing. This core JSON format can also be customized to fit differing needs, making the STAC specification highly flexible and adaptable.\n\nTherefore, if the STAC specification seems ‘light’, that is because it is light-- by design. Through this flexibility, different domains and tools can easily utilize the STAC specification and make it their own.\n\nThe Extensions section of the spec is where the community collaborates on more detail about specific data types and new functionality.\n\nNOTE STAC helps you catalog and describe at a higher level.\n\n","type":"content","url":"/stac-and-data-access#what-is-stac","position":7},{"hierarchy":{"lvl1":"What is STAC?","lvl2":"What is STAC"},"type":"lvl2","url":"/stac-and-data-access#what-is-stac-1","position":8},{"hierarchy":{"lvl1":"What is STAC?","lvl2":"What is STAC"},"content":"The SpatioTemporal Asset Catalog (STAC) specification was designed to establish a standard, unified language to talk about geospatial data, allowing it to be more easily searchable and queryable.\n\nSTAC has been designed to be simple, flexible, and extensible. STAC is a network of JSON files that reference other JSON files, with each JSON file adhering to a specific core specification depending on which STAC component it is describing. This core JSON format can also be customized to fit differing needs, making the STAC specification highly flexible and adaptable.\n\nTherefore, if the STAC specification seems ‘light’, that is because it is light-- by design. Through this flexibility, different domains and tools can easily utilize the STAC specification and make it their own.\n\nThe Extensions section of the spec is where the community collaborates on more detail about specific data types and new functionality.","type":"content","url":"/stac-and-data-access#what-is-stac-1","position":9},{"hierarchy":{"lvl1":"What is STAC?","lvl3":"What is a SpatioTemporal Asset","lvl2":"What is STAC"},"type":"lvl3","url":"/stac-and-data-access#what-is-a-spatiotemporal-asset","position":10},{"hierarchy":{"lvl1":"What is STAC?","lvl3":"What is a SpatioTemporal Asset","lvl2":"What is STAC"},"content":"A SpatioTemporal Asset is any file that represents information about the Earth captured in a certain place and at a particular time. Examples include spatiotemporal data derived from imagery (from satellites, airplanes, and drones), Synthetic Aperture Radar (SAR), point clouds (from LiDAR, structure from motion, etc.), data cubes, and full-motion video. The key is that the GeoJSON is not the actual ‘thing’, but instead references files and serves as an index to the STAC Assets.\n\n","type":"content","url":"/stac-and-data-access#what-is-a-spatiotemporal-asset","position":11},{"hierarchy":{"lvl1":"STAC Components"},"type":"lvl1","url":"/stac-and-data-access#stac-components","position":12},{"hierarchy":{"lvl1":"STAC Components"},"content":"There are three component specifications that together make up the core SpatioTemporal Asset Catalog specification. These components are:\n\nItem - A STAC Item is the foundational building block of STAC. It is a GeoJSON feature supplemented with additional metadata that enables clients to traverse through catalogs. Since an item is a GeoJSON, it can be easily read by any modern GIS or geospatial library. One item can describe one or more SpatioTemporal Asset(s). For example, a common practice of using STAC for imagery is that each band in a scene is its own STAC Asset and there is one STAC Item to represent all the bands in a single scene.\n\nCatalog - A Catalog is usually the starting point for navigating a STAC. A catalog.json file contains links to some combination of other STAC Catalogs, Collections, and/or Items. We can think of it like a directory tree on a computer.\n\nCollection - A STAC Collection builds upon the STAC Catalog specification to include additional metadata about a set of items that exist as part of the collection. It extends the parent catalog by adding additional fields to enable the description of information like the spatial and temporal extent of the data, the license, keywords, providers, etc. Therefore, it can easily be extended for additional collection-level metadata.\n\nEach component can be used alone, but they work best in concert with one another.\n\nA STAC Item represents one or more spatiotemporal assets as GeoJSON so that it can be easily searched. The STAC Catalog specification provides structural elements to group STAC Items and Collections. STAC Collections are catalogs that add more required metadata and describe a group of related items. Now, let’s dive into each one of these components a bit more in-depth.\n\nThe STAC API specification builds on top of that core and allows us to do dynamic queries over our collections.\n\n","type":"content","url":"/stac-and-data-access#stac-components","position":13},{"hierarchy":{"lvl1":"Accessing the EarthCODE Catalog"},"type":"lvl1","url":"/stac-and-data-access#accessing-the-earthcode-catalog","position":14},{"hierarchy":{"lvl1":"Accessing the EarthCODE Catalog"},"content":"This section introduces \n\nSTAC, the SpatioTemporal Asset Catalog. STAC provides a standardized way to structure metadata about spatialotemporal data. The STAC community are building APIs and tools on top of this structure to make working with spatiotemporal data easier.\n\nUsers of STAC will interact most often with Collections and Items (there’s also Catalogs, which group together collections). A Collection is just a collection of items, plus some additional metadata like the license and summaries of what’s available on each item. You can view available collections on the EarthCODE catalog with\n\n","type":"content","url":"/stac-and-data-access#accessing-the-earthcode-catalog","position":15},{"hierarchy":{"lvl1":"Accessing the EarthCODE Catalog","lvl2":"Open Science Catalog"},"type":"lvl2","url":"/stac-and-data-access#open-science-catalog","position":16},{"hierarchy":{"lvl1":"Accessing the EarthCODE Catalog","lvl2":"Open Science Catalog"},"content":"The \n\nOpen Science Data Catalog is a publicly accessible platform that enables anyone—whether or not they have a GitHub account—to discover and access Earth Observation research. It provides a transparent and structured way to explore the latest results from EO projects by organizing metadata in a consistent and harmonized format.\n\nBuilt on the open-source STAC Browser, the catalog allows users to browse and explore interlinked elements such as themes, variables, EO missions, projects, products, workflows, and experiments, all described using STAC-compliant JSON files.\n\nurl = \"https://raw.githubusercontent.com/ESA-EarthCODE/open-science-catalog-metadata/main/catalog.json\"\ncat = pystac.Catalog.from_file(url)\ncat\n\nNow let’s search and access the same data we were just looking at, SeasFire:\n\nhttps://​opensciencedata​.esa​.int​/products​/seasfire​-cube​/collection\n\nIn the examples we’ve seen so far, we’ve just been given a STAC item. How do you find the items you want in the first place? That’s where a STAC API comes in.\n\nA STAC API is some web service that accepts queries and returns STAC objects. The ability to handle queries is what differentiates a STAC API from a static STAC catalog, where items are just present on some file system.\n\nprint(cat.title, cat.description)\n\nosc_products = cat.get_child(\"products\")\nosc_products\n\nsearch_f = lambda q, coll: q in f\"{coll.id or ''} {coll.title or ''} {coll.description or ''}\".lower()\nfire_colls = [c for c in osc_products.get_all_collections() if search_f('fire',c)]\nfire_colls\n\nseas_fire_osc_product = osc_products.get_child('seasfire-cube')\nseas_fire_osc_product\n\nseas_fire_item = next(seas_fire_osc_product.get_child('seasfire-cube').get_items())\nseas_fire_item\n\nYou can use the same methods on a variety of catalogs that expose different query parameters. STAC items can either be an API or static assets. We just explored static assets, now we will quickly demo using STAC API.\n\ncatalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"], \n    bbox=[-6, 50, 2, 56],                      \n    datetime=\"2024-06-01/2024-06-30\",         \n    query={\"eo:cloud_cover\":\"<10\"}               \n)\nprint(f\"{search.matched()} items in June with <10 % cloud\")\n\n\nNote that the collection points to another collection, which contains the actual data. The EarthCODE STAC extension describes some metadata that enrich the STAC collection \n\nhttps://​github​.com​/stac​-extensions​/osc.\n\n","type":"content","url":"/stac-and-data-access#open-science-catalog","position":17},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE"},"type":"lvl1","url":"/stac-and-data-access#accessing-data-from-earthcode","position":18},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE"},"content":"Now we will load the actual data from the STAC item we’ve found...\n\nThe SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses ‘Earth System Deep Learning for Seasonal Fire Forecasting’ and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions.\n\nseas_fire_item\n# https://s3.waw4-1.cloudferro.com/EarthCODE/Catalogs/seasfire/seasfire-cube_v0.4/seasfire-cube-v.0.4/seasfire-cube-v.0.4.json\n\n","type":"content","url":"/stac-and-data-access#accessing-data-from-earthcode","position":19},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE","lvl2":"Items and Assets"},"type":"lvl2","url":"/stac-and-data-access#items-and-assets","position":20},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE","lvl2":"Items and Assets"},"content":"STAC is a metadata standard. It doesn’t really deal with data files directly. Instead, it links to the data files under the “assets” property.\n\nAs described in https://​guide​.cloudnativegeo​.org/ in this case the STAC catalog contains a collection for each Zarr store and there are collection-level assets that point to the location of the Zarr store. There are no items at all in this setup.\n\nIn this scenario any STAC metadata exists purely for discovery and cannot be used for filtering or subsetting (see Future Work for more on that). To search the STAC catalog to find collections of interest you will use the Collection Search API Extension. Depending on the level of metadata that has been provided in the STAC catalog you can search by the name of the collection and possibly by the variables – exposed via the Data Cube Extension.","type":"content","url":"/stac-and-data-access#items-and-assets","position":21},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE","lvl3":"Read straight to xarray","lvl2":"Items and Assets"},"type":"lvl3","url":"/stac-and-data-access#read-straight-to-xarray","position":22},{"hierarchy":{"lvl1":"Accessing Data from EarthCODE","lvl3":"Read straight to xarray","lvl2":"Items and Assets"},"content":"For a collection of interest, the best approach for accessing the data is to construct the lazily-loaded data cube in xarray (or an xarray.DataTree if the Zarr store has more than one group) and filter from there.\n\nTo do this you can use the zarr backend directly or you can use the stac backend to streamline even more. The stac backend is mostly useful if the STAC collection uses the xarray extension.\n\nConstructing the lazy data cube is likely to be very fast if there is a consolidated metadata file OR the data is in Zarr-3 and the Zarr metadata fetch is highly parallelized (read more).\n\n","type":"content","url":"/stac-and-data-access#read-straight-to-xarray","position":23},{"hierarchy":{"lvl1":"Loading and Fetching Data Cube - Xarray"},"type":"lvl1","url":"/stac-and-data-access#loading-and-fetching-data-cube-xarray","position":24},{"hierarchy":{"lvl1":"Loading and Fetching Data Cube - Xarray"},"content":"\n\nhttp_url = seas_fire_item.assets[\"data\"].href.replace(\n    \"s3://\",\n    f\"{seas_fire_item.properties['storage:schemes'][seas_fire_item.assets['data'].extra_fields['storage:refs'][0]]['platform'].rstrip('/')}/\",\n)\nhttp_url\n\n\nds = xarray.open_dataset(\n\thttp_url,\n\tengine='zarr',\n    chunks={},\n\tconsolidated=True\n\t# storage_options = {'token': 'anon'}\n)\nds\n\ngwis = ds.gwis_ba\ngwis\n\n\n\nmin_lon, min_lat, max_lon, max_lat = gdf.total_bounds\n\n# find the nearest points on our grid\nlat_start = gwis.latitude.sel(latitude=max_lat, method=\"nearest\").item()\nlat_stop  = gwis.latitude.sel(latitude=min_lat, method=\"nearest\").item()\nlon_start = gwis.longitude.sel(longitude=min_lon, method=\"nearest\").item()\nlon_stop  = gwis.longitude.sel(longitude=max_lon, method=\"nearest\").item()\n\nlat_slice = slice(lat_start, lat_stop)\nlon_slice = slice(lon_start, lon_stop)\n\nbbox=[lon_stop , lat_stop , lon_start, lat_start]\nbbox\n\ntime_oi='2018-08-01'\n\n\ngwis_aoi = gwis.sel(latitude=lat_slice,longitude=lon_slice).sel(time=time_oi,method=\"nearest\")\ngwis_aoi.plot()\n\n\n","type":"content","url":"/stac-and-data-access#loading-and-fetching-data-cube-xarray","position":25},{"hierarchy":{"lvl1":"Loading and Fetching Data Cube - Xarray","lvl2":"Another Example..."},"type":"lvl2","url":"/stac-and-data-access#another-example","position":26},{"hierarchy":{"lvl1":"Loading and Fetching Data Cube - Xarray","lvl2":"Another Example..."},"content":"We will now do something a bit more complicated and fetch data using pystac to search through the \n\nearth​-search​.aws​.element84​.com​/v1 STAC API endpoint. As mentioned in the \n\nSTAC and Data Access guide, the STAC API provides us with endpoints to query STAC collections.\n\nThere is an ecosystem of libraries that implement STAC standards (pystac, odc-stac, stackstac, etc..) that allow us to analyse, load and use data described in STAC. We will briefly explore in the following cells below.\n\nAs a first step, we will open a catalog with the pystac library: \n\nhttps://​earth​-search​.aws​.element84​.com​/v1 This catalog contains STAC collections of various datasets (such as sentinel-1 and sentinel-2 data) in cloud optimised formats (COGs in this case) - see \n\nhttps://​earth​-search​.aws​.element84​.com. for more details. With these formats we are able to use the assets using cloud-native access patterns (see \n\nhttps://​guide​.cloudnativegeo​.org​/cookbooks/ for more details).\n\nNote that a STAC endpoint like the one below, only returns STAC items and not the actual data. The STAC items returned have Assets defined which link to the actual data (and provide metadata).\n\nimport pandas as pd\n\nfire_date_t = pd.to_datetime(time_oi) # get the date of the forest fire and a the dates before and after it\nweek_before = (fire_date_t - timedelta(days=7))\nweek_after = (fire_date_t + timedelta(days=7))\n\nprint(week_before.date(), \"---\" , week_after.date())\n\ncatalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\nchunk={} # <-- use dask\nres=10 # 10m resolution\n\nIn the next step we will do two things:\n\n1. Search for multiple cloudless sentinel-2 satellite images within a month of our pre-fire (week_before) date. The STAC API allows us to do a couple of things in a simple API call:\n\nWe can define an arrea of interest (bbox) and search for items that cover this region\n\nWe can subset for the time of interest as well (datetime)\n\nWe can define custom querries over implemented STAC extensions. For example, the Sentinel STAC collection we are querrying implements the eo STAC extension, and defines cloud_cover - this allows us to search for quality assets with minimal cloudy pixels\n\nThis will return the relevant STAC items - which contain assets that point to our data of interest. We now need to load this data into a dataarray to start analysing it.\n\n2. Note that in the above step, we only have items that point to some data - this data can be tiff, zarr, netcdf, COG, or other SpatioTemporal asset data. In our case, the element84 endpoint points to the data collection of  https://​registry​.opendata​.aws​/sentinel​-2​-l2a​-cogs/ - the format of our data is cloud-optimized GeoTiff. A cloud-friendly format such as this enables us cloud-native access patterns, such as easily fetching only our area of interest (as opposed to several tiff files and manually subsetting them after downloading).\n\nLibraries such as odc-stac integrate with STAC standards and allow us to load data as well as leveraging the cloud-optimised formats. For example, in the cell below we define how we want to transform/load our data by:\n\nPassing the STAC item (or multiple items) we want to load\n\ndefining a particular chunk size (the passed {} asks for the data to be automatically chunked as it is originally);\n\nWe can request only the spectral bands of interest, in this case nir and swir22, to reduce the amount of data that we fetch.\n\nWe can define a resolution to retrieve the data at, note that this will also resample automatically. For example the nir band data has a 10m resolution, but the swir22 - 20m resolution.\n\nThere are multiple other options as well, such as defining in which projection we want our data in. More information can be found at: \n\nhttps://​odc​-stac​.readthedocs​.io​/en​/latest​/​_api​/odc​.stac​.load​.html\n\n# STAC search for relevant items\nweek_before_start = (week_before - timedelta(days=30))\ntime_range_start = str(week_before_start.date()) + \"/\" + str(week_before.date())\n\nquery1 = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=time_range_start, limit=100,\n    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\nitems = list(query1.items())\nprint(f\"Found: {len(items):d} datasets\")\n\n# plot all the STAC assets\npoly_pre = gpd.GeoSeries([shapely.Polygon(item.geometry['coordinates'][0]) for item in items], name='geometry', crs='epsg:4236')\npoly_pre.explore()\n\nprefire_ds = stac_load(\n    items,\n    bands=(\"nir\", \"swir22\"),\n    chunks=chunk,  # <-- use Dask\n    resolution=res,\n    crs=\"EPSG:32629\",\n        groupby=\"datetime\",\n    bbox=bbox,\n)\nprefire_ds = prefire_ds.mean(dim=\"time\")\nprefire_ds","type":"content","url":"/stac-and-data-access#another-example","position":27},{"hierarchy":{"lvl1":"Pangeo"},"type":"lvl1","url":"/index-4","position":0},{"hierarchy":{"lvl1":"Pangeo"},"content":"","type":"content","url":"/index-4","position":1},{"hierarchy":{"lvl1":"Pangeo","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"type":"lvl2","url":"/index-4#welcome-to-the-esa-earthcode-101-workshop","position":2},{"hierarchy":{"lvl1":"Pangeo","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"content":"Doing Open Science shouldn’t be hard, and EarthCODE makes it easy!\n\nThis tutorial introduces participants to the EDC Pangeo Cloud Platform and scalable geospatial analysis using Python tools. Participants will learn how to access data programmatically from the EarthCODE Open Science Catalog via the STAC API, explore Zarr-formatted datasets, and use Xarray with Dask for efficient data processing. The session includes a hands-on example and guidance on saving and publishing results back to the EarthCODE Catalog.","type":"content","url":"/index-4#welcome-to-the-esa-earthcode-101-workshop","position":3},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"type":"lvl2","url":"/introduction#welcome-to-the-esa-earthcode-101-workshop","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"content":"Doing Open Science shouldn’t be hard, and EarthCODE makes it easy!\n\nThis hands-on tutorial is designed to introduce participants to EarthCODE’s and the EDC Pangeo Platform’s capabilities, guiding them from searching, finding, and accessing EO datasets and workflows to publishing reproducible experiments that can be shared with the wider scientific community.\n\nThis workshop will equip you with the tools and knowledge to leverage EarthCODE for your own projects and contribute to the future of open science. During this workshop, participants will, in a hands-on fashion will learn about the following:\n\nIntroduction to EarthCODE and the future of FAIR and Open Science in Earth Observation\n\nGain understanding in Finding, Accessing, Interoperability, and Reusability of data and workflows on EarthCODE\n\nCreating reproducible experiments using EarthCODE’s platforms - with a hands-on example with Euro Data Cube and Pangeo\n\nPublishing data and experiments to EarthCODE At the end of the workshop, we will take time for discussion and feedback on how to make EarthCODE better for the community.","type":"content","url":"/introduction#welcome-to-the-esa-earthcode-101-workshop","position":3},{"hierarchy":{"lvl1":"Introduction","lvl3":"Running on EDC EOxHub Workspace","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"type":"lvl3","url":"/introduction#running-on-edc-eoxhub-workspace","position":4},{"hierarchy":{"lvl1":"Introduction","lvl3":"Running on EDC EOxHub Workspace","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"content":"If you are using the EDC environment and have accessed this example through the Open Science Catalog, this example should have the correct kernel selected, and packages installed. You can directly use this example as is, with Dask Gateway. You can access this example on EDC Pangeo Platform.","type":"content","url":"/introduction#running-on-edc-eoxhub-workspace","position":5},{"hierarchy":{"lvl1":"Introduction","lvl3":"Running on your own computer","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"type":"lvl3","url":"/introduction#running-on-your-own-computer","position":6},{"hierarchy":{"lvl1":"Introduction","lvl3":"Running on your own computer","lvl2":"Welcome to the ESA EarthCODE 101 Workshop!"},"content":"Most parts of this tutorial were designed to run with limited computer resources, so it is possible to run on your laptop.\nIt is a bit more complicated as you will have to install the software environment yourself. Also you will not be able to test real cloud distributed processing with Dask gateway.\n\nSteps to run this tutorial on your own computer are listed below and demonstrated through Linux commands only:\n\ngit clone the repository.git clone http://github.com/ESA-EarthCODE/tutorials/\n\nInstall the required software environment with Conda. If you do not have Conda, install it by following these instructions (see \n\nhere). Then create the environment, this can take a few minutes.conda env create -n pangeo -f ESA-EarthCODE/tutorials/pangeo/environment.yml\n\nLaunch a Jupyterlab notebook server from this environment.conda activate pangeo\njupyter lab\n\nOpen a web browser and connect to the Jupyterlab provided URL (you should see it in the jupyter lab command outputs), something like: \n\nhttp://​localhost:8888​/lab​?token​=​42fac6733c6854578b981bca3abf5152.\n\nNavigate to pangeo_on_EarthCODE using the file browser on the left side of the Jupyterlab screen.","type":"content","url":"/introduction#running-on-your-own-computer","position":7},{"hierarchy":{"lvl1":"Zarr 101"},"type":"lvl1","url":"/cloud-native-formats-101","position":0},{"hierarchy":{"lvl1":"Zarr 101"},"content":"from pystac.extensions.storage import StorageExtension\nfrom datetime import datetime\nfrom pystac_client import Client as pystac_client\nfrom odc.stac import configure_rio, stac_load\n\nimport xarray\n\n","type":"content","url":"/cloud-native-formats-101","position":1},{"hierarchy":{"lvl1":"Zarr 101","lvl3":"Context"},"type":"lvl3","url":"/cloud-native-formats-101#context","position":2},{"hierarchy":{"lvl1":"Zarr 101","lvl3":"Context"},"content":"When dealing with large data files or collections, it’s often impossible to load all the data you want to analyze into a single computer’s RAM at once. This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data chunks, which means pieces of an entire dataset. By reading a dataset in chunks we can process our data piece by piece on a single computer and even on a distributed computing cluster using Dask (Cloud or HPC for instance).\n\nHow we will process these ‘chunks’ in a parallel environment will be discussed in \n\ndask_introduction. The concept of chunk will be explained here.\n\nWhen we process our data piece by piece, it’s easier to have our input or ouput data also saved in chunks. \n\nZarr is the reference library in the Pangeo ecosystem to save our Xarray multidimentional datasets in chunks.\n\nZarr is not the only file format which uses chunk. \n\nkerchunk library for example builds a virtual chunked dataset based on NetCDF files, and optimizes the access and analysis of large datasets.","type":"content","url":"/cloud-native-formats-101#context","position":3},{"hierarchy":{"lvl1":"Zarr 101","lvl4":"Data","lvl3":"Context"},"type":"lvl4","url":"/cloud-native-formats-101#data","position":4},{"hierarchy":{"lvl1":"Zarr 101","lvl4":"Data","lvl3":"Context"},"content":"In this workshop, we will be using the \n\nSeasFire Data Cube published to the EarthCODE Open Science Catalog","type":"content","url":"/cloud-native-formats-101#data","position":5},{"hierarchy":{"lvl1":"Zarr 101","lvl5":"Related publications","lvl4":"Data","lvl3":"Context"},"type":"lvl5","url":"/cloud-native-formats-101#related-publications","position":6},{"hierarchy":{"lvl1":"Zarr 101","lvl5":"Related publications","lvl4":"Data","lvl3":"Context"},"content":"Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. \n\nAlonso et al. (2024). The same dataset can also be downloaded from Zenodo: \n\nhttps://​zenodo​.org​/records​/13834057\n\nhttp_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n\nds = xarray.open_dataset(\n\thttp_url,\n\tengine='zarr',\n    chunks={},\n\tconsolidated=True\n\t# storage_options = {'token': 'anon'}\n)\nds\n\ntotal_size_GB = sum(var.nbytes for var in ds.data_vars.values()) / 1e9  # B to GB\nprint(f\"Total size: {total_size_GB:.2f} GB\")\n\n","type":"content","url":"/cloud-native-formats-101#related-publications","position":7},{"hierarchy":{"lvl1":"Zarr 101","lvl3":"What is a chunk"},"type":"lvl3","url":"/cloud-native-formats-101#what-is-a-chunk","position":8},{"hierarchy":{"lvl1":"Zarr 101","lvl3":"What is a chunk"},"content":"If you look carefully at our dataset, each Data Variable is a ndarray (or dask.array if you have a dask client instantiated) with a chunk size of (966, 180, 360). So basically accessing one data variable would load arrays of dimensions (966, 180, 360) into the computer’s RAM. You can see this information and more details by clicking the icon as indicated in the image below.\n\nWhen you need to analyze large files, a computer’s memory may not be sufficient anymore.\n\nThis is where understanding and using chunking correctly comes into play.\n\nChunking is splitting a dataset into small pieces.\n\nOriginal dataset is in one piece,\n\nand we split it into several smaller pieces.\n\nWe split it into pieces so that we can process our data block by block or chunk by chunk.\n\nIn our case, the data is already chunked, in a cloud-native format ready for analysis and usage - in zarr format\n\n","type":"content","url":"/cloud-native-formats-101#what-is-a-chunk","position":9},{"hierarchy":{"lvl1":"Zarr 101","lvl2":"Zarr"},"type":"lvl2","url":"/cloud-native-formats-101#zarr","position":10},{"hierarchy":{"lvl1":"Zarr 101","lvl2":"Zarr"},"content":"Zarr’s main characteristics are the following:\n\nEvery chunk of a Zarr dataset is stored as a single file\n\nEach Data array in a Zarr dataset has a two unique files containing metadata:\n\n.zattrs for dataset or dataarray general metadatas\n\n.zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n\nZarr can be considered as an Analysis Ready, cloud optimized data (ARCO) file format!","type":"content","url":"/cloud-native-formats-101#zarr","position":11},{"hierarchy":{"lvl1":"Dask 101"},"type":"lvl1","url":"/dask101","position":0},{"hierarchy":{"lvl1":"Dask 101"},"content":"We will be using Dask with Xarray to parallelize our data analysis. The analysis is very similar to what we have done in previous examples but this time we will use data on a global coverage that we read from the SeasFire Cube.\n\nWe will learn how to use the EDC Pangeo Dask Gateway to analyse data at scale.","type":"content","url":"/dask101","position":1},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Data"},"type":"lvl4","url":"/dask101#data","position":2},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Data"},"content":"In this workshop, we will be using the \n\nSeasFire Data Cube published to the EarthCODE Open Science Catalog","type":"content","url":"/dask101#data","position":3},{"hierarchy":{"lvl1":"Dask 101","lvl5":"Related publications","lvl4":"Data"},"type":"lvl5","url":"/dask101#related-publications","position":4},{"hierarchy":{"lvl1":"Dask 101","lvl5":"Related publications","lvl4":"Data"},"content":"Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. \n\nAlonso et al. (2024). The same dataset can also be downloaded from Zenodo: \n\nhttps://​zenodo​.org​/records​/13834057\n\nimport dask.distributed\nimport xarray\n\n","type":"content","url":"/dask101#related-publications","position":5},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Parallelize with Dask"},"type":"lvl3","url":"/dask101#parallelize-with-dask","position":6},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Parallelize with Dask"},"content":"We know from previous chapter \n\ncloud native formats 101 that chunking is key for analyzing large datasets. In this episode, we will learn to parallelize our data analysis using \n\nDask on our chunked dataset.\n\n","type":"content","url":"/dask101#parallelize-with-dask","position":7},{"hierarchy":{"lvl1":"Dask 101","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"type":"lvl4","url":"/dask101#what-is-dask","position":8},{"hierarchy":{"lvl1":"Dask 101","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"content":"Dask scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n\nDask is a flexible library for parallel computing in Python.\n\nIt is widely used for handling large and complex Earth Science datasets and speed up science.\n\nDask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n\nIt scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n\nThe strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n\nIt is interesting to note that at first, \n\nDask has been created to handle data that is larger than memory, on a single computer. It then was extended with Distributed to compute data in parallel over clusters of computers.\n\n","type":"content","url":"/dask101#what-is-dask","position":9},{"hierarchy":{"lvl1":"Dask 101","lvl5":"How does Dask scale and accelerate your data analysis?","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"type":"lvl5","url":"/dask101#how-does-dask-scale-and-accelerate-your-data-analysis","position":10},{"hierarchy":{"lvl1":"Dask 101","lvl5":"How does Dask scale and accelerate your data analysis?","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"content":"Dask proposes different abstractions to distribute your computation. In this Dask Introduction section, we will focus on \n\nDask Array which is widely used in pangeo ecosystem as a back end of Xarray.\n\nAs shown in the \n\nprevious section Dask Array is based on chunks.\nChunks of a Dask Array are well-known Numpy arrays. By transforming our big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n\n","type":"content","url":"/dask101#how-does-dask-scale-and-accelerate-your-data-analysis","position":11},{"hierarchy":{"lvl1":"Dask 101","lvl5":"How does Xarray with Dask distribute data analysis?","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"type":"lvl5","url":"/dask101#how-does-xarray-with-dask-distribute-data-analysis","position":12},{"hierarchy":{"lvl1":"Dask 101","lvl5":"How does Xarray with Dask distribute data analysis?","lvl4":"What is Dask ?","lvl3":"Parallelize with Dask"},"content":"When we use chunks with Xarray, the real computation is only done when needed or asked for, usually when invoking compute() or load() functions. Dask generates a task graph describing the computations to be done. When using \n\nDask Distributed a Scheduler distributes these tasks across several Workers.\n\n","type":"content","url":"/dask101#how-does-xarray-with-dask-distribute-data-analysis","position":13},{"hierarchy":{"lvl1":"Dask 101","lvl4":"What is a Dask Distributed cluster ?","lvl3":"Parallelize with Dask"},"type":"lvl4","url":"/dask101#what-is-a-dask-distributed-cluster","position":14},{"hierarchy":{"lvl1":"Dask 101","lvl4":"What is a Dask Distributed cluster ?","lvl3":"Parallelize with Dask"},"content":"A Dask Distributed cluster is made of two main components:\n\na Scheduler, responsible for handling computations graph and distributing tasks to Workers.\n\nOne or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker’s local disk).\n\nA user usually needs Client and Cluster objects as shown below to use Dask Distributed.\n\n","type":"content","url":"/dask101#what-is-a-dask-distributed-cluster","position":15},{"hierarchy":{"lvl1":"Dask 101","lvl5":"Where can we deploy a Dask distributed cluster?","lvl4":"What is a Dask Distributed cluster ?","lvl3":"Parallelize with Dask"},"type":"lvl5","url":"/dask101#where-can-we-deploy-a-dask-distributed-cluster","position":16},{"hierarchy":{"lvl1":"Dask 101","lvl5":"Where can we deploy a Dask distributed cluster?","lvl4":"What is a Dask Distributed cluster ?","lvl3":"Parallelize with Dask"},"content":"Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.)  Dask distributed Cluster object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n\nEDC has one such deployment\n\nTip\n\nA Dask Cluster can be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n\n","type":"content","url":"/dask101#where-can-we-deploy-a-dask-distributed-cluster","position":17},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Dask distributed Client","lvl3":"Parallelize with Dask"},"type":"lvl4","url":"/dask101#dask-distributed-client","position":18},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Dask distributed Client","lvl3":"Parallelize with Dask"},"content":"The Dask distributed Client is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a Client object. Once a Client has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n\nNo matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n\nUsing Client() function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the ‘local’ machine. Here, during the workshop, we are running this notebook in the EDC Pangeo cloud deployment, so the ‘local’ machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you’ve been given (not on your laptop).\n\nfrom dask.distributed import Client\n\nclient = Client()   # create a local dask cluster on the local machine.\nclient\n\nInspecting the Cluster Info section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker).\n\n# close client to clean resources\n# Note, you can run this tutorial locally if you uncomment this line \nclient.close()\n\n","type":"content","url":"/dask101#dask-distributed-client","position":19},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Scaling your Computation using Dask Gateway."},"type":"lvl3","url":"/dask101#scaling-your-computation-using-dask-gateway","position":20},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Scaling your Computation using Dask Gateway."},"content":"For this workshop, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n\nDask Gateway is a component that helps you manage and create Dask Clusters across your environment.\nAs Dask Gateway is configured by default on this infrastructure, you just need to execute the following cells.\n\nNote that, if you’re executing this locally, without prior setup for dask gateway, the following code will crash. If you would like to run this locally, skip these lines and continue with the local dask client above\n\nfrom dask_gateway import Gateway\ngateway = Gateway()\n\n","type":"content","url":"/dask101#scaling-your-computation-using-dask-gateway","position":21},{"hierarchy":{"lvl1":"Dask 101","lvl3":"EDC Dask Gateway Options"},"type":"lvl3","url":"/dask101#edc-dask-gateway-options","position":22},{"hierarchy":{"lvl1":"Dask 101","lvl3":"EDC Dask Gateway Options"},"content":"EDC Pangeo’s Dask Gateway provides several cluster configurations for your workloads. You can choose the appropriate size based on your computational needs:\n\nsmall\n\nWorker cores: 0.5\n\nWorker memory: 1 GB\n\nmedium\n\nWorker cores: 2\n\nWorker memory: 2 GB\n\nlarger\n\nWorker cores: 4\n\nWorker memory: 4 GB\n\nYou can set these options when spawning your Dask clusters to ensure optimal resource allocation.\n\ncluster_options = gateway.cluster_options()\ncluster_options\n\ncluster = gateway.new_cluster(cluster_options=cluster_options)\n# cluster.scale(2)\ncluster\n\n","type":"content","url":"/dask101#edc-dask-gateway-options","position":23},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Get a client from the Dask Gateway Cluster","lvl3":"EDC Dask Gateway Options"},"type":"lvl4","url":"/dask101#get-a-client-from-the-dask-gateway-cluster","position":24},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Get a client from the Dask Gateway Cluster","lvl3":"EDC Dask Gateway Options"},"content":"As stated above, creating a Dask Client is mandatory in order to perform following Daks computations on your Dask Cluster.\n\nclient = cluster.get_client()\nclient\n\n","type":"content","url":"/dask101#get-a-client-from-the-dask-gateway-cluster","position":25},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Dask Dashboard","lvl3":"EDC Dask Gateway Options"},"type":"lvl4","url":"/dask101#dask-dashboard","position":26},{"hierarchy":{"lvl1":"Dask 101","lvl4":"Dask Dashboard","lvl3":"EDC Dask Gateway Options"},"content":"Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separated tab of your browser.\n\nWe will learn here how to use it through \n\ndask jupyterlab extension.\n\nTo use Dask Dashboard through jupyterlab extension on Pangeo EDC infrastructure,\nyou will just need too look at the html link you have for your jupyterlab, and Dask dashboard port number, as highlighted in the figure below.\n\n\n\n\nThen click the orange icon indicated in the above figure, and type ‘your’ dashboard link.\n\nYou can click several buttons indicated with blue arrows in above figures, then drag and drop to place them as your convenience.\n\nIt’s really helpfull to understand your computation and how it is distributed.\n\n","type":"content","url":"/dask101#dask-dashboard","position":27},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Dask Distributed computations on our dataset"},"type":"lvl3","url":"/dask101#dask-distributed-computations-on-our-dataset","position":28},{"hierarchy":{"lvl1":"Dask 101","lvl3":"Dask Distributed computations on our dataset"},"content":"Let’s open the SeasFire dataset we previously looked at, select a single location over time, visualize the task graph generated by Dask, and observe the Dask Dashboard.\n\nhttp_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n\nds = xarray.open_dataset(\n\thttp_url,\n\tengine='zarr',\n    chunks={},\n\tconsolidated=True\n)\nds\n\nmask= ds['lsm'][:,:]\ngwis_all= ds.gwis_ba.resample(time=\"1YE\").sum()\ngwis_all= gwis_all.where(mask>0.5)\n\ngwis_2020= gwis_all.sel(time='2020-08-01', method='nearest')\ngwis_2020.data\n\ngwis_2020.data.visualize(optimize_graph=True)\n\nDid you notice something on the Dask Dashboard when running the two previous cells?\n\nWe didn’t ‘compute’ anything. We just built a Dask task graph with it’s size indicated as count above, but did not ask Dask to return a result.\n\nNote that underneath, dask optimizes the execution graph (opaquely), so as to minimize overheads and overall execution resources (hence why we’re passing optimize_graph=True)\n\n","type":"content","url":"/dask101#dask-distributed-computations-on-our-dataset","position":29},{"hierarchy":{"lvl1":"Dask 101","lvl2":"Computing"},"type":"lvl2","url":"/dask101#computing","position":30},{"hierarchy":{"lvl1":"Dask 101","lvl2":"Computing"},"content":"Calling compute on our Xarray object will trigger the execution on the Dask Cluster. Alternatively any action that would demand the computation of our data (e.g. plotting) would trigger the execution of our workflow.\n\nYou should be able to see how Dask is working on Dask Dashboard.\n\ngwis_2020.plot()\n\n","type":"content","url":"/dask101#computing","position":31},{"hierarchy":{"lvl1":"Dask 101","lvl2":"Closing Clusters"},"type":"lvl2","url":"/dask101#closing-clusters","position":32},{"hierarchy":{"lvl1":"Dask 101","lvl2":"Closing Clusters"},"content":"Close Clusters to Clean Resources for the next exercise (generally a good practice!)\n\n# close client to clean resources\nclient.close()","type":"content","url":"/dask101#closing-clusters","position":33},{"hierarchy":{"lvl1":"Pangeo 101"},"type":"lvl1","url":"/pangeo101","position":0},{"hierarchy":{"lvl1":"Pangeo 101"},"content":"","type":"content","url":"/pangeo101","position":1},{"hierarchy":{"lvl1":"Xarray 101"},"type":"lvl1","url":"/xarray101","position":0},{"hierarchy":{"lvl1":"Xarray 101"},"content":"from pystac.extensions.storage import StorageExtension\nfrom datetime import datetime\nfrom pystac_client import Client as pystac_client\nfrom odc.stac import configure_rio, stac_load\nimport matplotlib.pyplot as plt\n\nimport xarray\n\n","type":"content","url":"/xarray101","position":1},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"What is xarray?"},"type":"lvl3","url":"/xarray101#what-is-xarray","position":2},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"What is xarray?"},"content":"Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like multi-dimensional arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.","type":"content","url":"/xarray101#what-is-xarray","position":3},{"hierarchy":{"lvl1":"Xarray 101","lvl4":"How is xarray structured?","lvl3":"What is xarray?"},"type":"lvl4","url":"/xarray101#how-is-xarray-structured","position":4},{"hierarchy":{"lvl1":"Xarray 101","lvl4":"How is xarray structured?","lvl3":"What is xarray?"},"content":"Xarray has two core data structures, which build upon and extend the core strengths of NumPy and Pandas libraries. Both data structures are fundamentally N-dimensional:\n\nDataArray is the implementation of a labeled, N-dimensional array. It is an N-D generalization of a Pandas.Series. The name DataArray itself is borrowed from \n\nFernando Perez’s datarray project, which prototyped a similar data structure.\n\nDataset is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray as the pandas.DataFrame.\n\nData can be read from online sources, as in the example below where we loaded metadata\n\nhttp_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n\nds = xarray.open_dataset(\n\thttp_url,\n\tengine='zarr',\n    chunks={},\n\tconsolidated=True\n\t# storage_options = {'token': 'anon'}\n)\nds\n\n","type":"content","url":"/xarray101#how-is-xarray-structured","position":5},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Accessing Coordinates and Data Variables"},"type":"lvl3","url":"/xarray101#accessing-coordinates-and-data-variables","position":6},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Accessing Coordinates and Data Variables"},"content":"DataArray, within Datasets, can be accessed through:\n\nthe dot notation like Dataset.NameofVariable\n\nor using square brackets, like Dataset[‘NameofVariable’] (NameofVariable needs to be a string so use quotes or double quotes)\n\nds.latitude\n\nds.lai\n\nds[‘lai’] is a one-dimensional xarray.DataArray with dates of type datetime64[ns]\n\nds['lai']\n\nds['lai'].attrs\n\n","type":"content","url":"/xarray101#accessing-coordinates-and-data-variables","position":7},{"hierarchy":{"lvl1":"Xarray 101","lvl4":"Xarray and Memory usage","lvl3":"Accessing Coordinates and Data Variables"},"type":"lvl4","url":"/xarray101#xarray-and-memory-usage","position":8},{"hierarchy":{"lvl1":"Xarray 101","lvl4":"Xarray and Memory usage","lvl3":"Accessing Coordinates and Data Variables"},"content":"Once a Data Array|Set is opened, xarray loads into memory only the coordinates and all the metadata needed to describe it.\nThe underlying data, the component written into the datastore, are loaded into memory as a NumPy array, only once directly accessed; once in there, it will be kept to avoid re-readings.\nThis brings the fact that it is good practice to have a look to the size of the data before accessing it. A classical mistake is to try loading arrays bigger than the memory with the obvious result of killing a notebook Kernel or Python process.\nIf the dataset does not fit in the available memory, then the only option will be to load it through the chunking; later on, in the tutorial ‘chunking_introduction’, we will introduce this concept.\n\nAs the size of the data is not too big here, we can continue without any problem. But let’s first have a look to the actual size and then how it impacts the memory once loaded into it.\n\nimport numpy as np\n\nprint(f'{np.round(ds.lai.nbytes / 1024**3, 2)} GB') # all the data are automatically loaded into memory as NumpyArray once they are accessed.\n\nds.lai.data\n\nAs other datasets have dimensions named according to the more common triad lat,lon,time a renomination is needed.\n\n","type":"content","url":"/xarray101#xarray-and-memory-usage","position":9},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Selection methods"},"type":"lvl3","url":"/xarray101#selection-methods","position":10},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Selection methods"},"content":"As underneath DataArrays are Numpy Array objects (that implement the standard Python x[obj] (x: array, obj: int,slice) syntax). Their data can be accessed through the same approach of numpy indexing.\n\nds.lai[0,100,100].load()\n\nAs it is not easy to remember the order of dimensions, Xarray really helps by making it possible to select the position using names:\n\n.isel -> selection based on positional index\n\n.sel  -> selection based on coordinate values\n\nds.lai.isel(time=0, latitude=100, longitude=100)\n\nThe more common way to select a point is through the labeled coordinate using the .sel method.\n\nTime is easy to be used as there is a 1 to 1 correspondence with values in the index, float values are not that easy to be used and a small discrepancy can make a big difference in terms of results.\n\nCoordinates are always affected by precision issues; the best option to quickly get a point over the coordinates is to set the sampling method (method=‘’) that will search for the closest point according to the specified one.\n\nOptions for the method are:\n\npad / ffill: propagate last valid index value forward\n\nbackfill / bfill: propagate next valid index value backward\n\nnearest: use nearest valid index value\n\nAnother important parameter that can be set is the tolerance that specifies the distance between the requested and the target (so that abs(index[indexer] - target) <= tolerance) from \n\ndocumentation.\n\nds.lai.sel(time=datetime(2020, 1, 8), method='nearest')\n\nds.lai.sel(latitude=46.3, longitude=8.8, method='nearest').isel(time=0)\n\nWarning\n\nTo select a single real value without specifying a method, you would need to specify the exact encoded value; not the one you see when printed.\n\nds.lai.isel(longitude=100).longitude.values.item()\n\n","type":"content","url":"/xarray101#selection-methods","position":11},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Plotting"},"type":"lvl3","url":"/xarray101#plotting","position":12},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Plotting"},"content":"Plotting data can easily be obtained through matplotlib.pyplot back-end \n\nmatplotlib documentation.\n\nAs the exercise is focused on an Area Of Interest, this can be obtained through a bounding box defined with slices.\n\nPlot fires\n\nlai_aoi = ds.lai.sel(latitude=slice(65.5,25.5), longitude=slice(-20.5,20.5))\nlai_aoi.sel(time=datetime(2020,6,23), method='nearest').plot()\n\n\nTip\n\nHave you noticed that latitudes are selected from the largest to the smallest values e.g. 46.5, 44.5 while longitudes are selected from the smallest to the largest value e.g. 8.5,11.5?\nThe reason is that you need to use the same order as the corresponding DataArray.\n\n","type":"content","url":"/xarray101#plotting","position":13},{"hierarchy":{"lvl1":"Xarray 101","lvl2":"Basic Maths and Stats"},"type":"lvl2","url":"/xarray101#basic-maths-and-stats","position":14},{"hierarchy":{"lvl1":"Xarray 101","lvl2":"Basic Maths and Stats"},"content":"\n\n# E.g. example scaling \n\nlai_aoi * 0.0001 + 1500\n\n\n","type":"content","url":"/xarray101#basic-maths-and-stats","position":15},{"hierarchy":{"lvl1":"Xarray 101","lvl2":"Statistics and Aggregation"},"type":"lvl2","url":"/xarray101#statistics-and-aggregation","position":16},{"hierarchy":{"lvl1":"Xarray 101","lvl2":"Statistics and Aggregation"},"content":"Calculate simple statistics:\n\nlai_aoi.min()\nlai_aoi.max()\nlai_aoi.mean()\n\nAggregate by month if the dataset spans multiple months:\n\nlai_monthly = lai_aoi.groupby(lai_aoi.time.dt.month).mean()\nlai_monthly\n\n","type":"content","url":"/xarray101#statistics-and-aggregation","position":17},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Masking","lvl2":"Statistics and Aggregation"},"type":"lvl3","url":"/xarray101#masking","position":18},{"hierarchy":{"lvl1":"Xarray 101","lvl3":"Masking","lvl2":"Statistics and Aggregation"},"content":"Not all values are valid and masking all those which are not in the valid range is necessary. Masking can be achieved through the method DataSet|Array.where(cond, other) or xr.where(cond, x, y).\n\nThe difference consists in the possibility to specify the value in case the condition is positive or not; DataSet|Array.where(cond, other) only offer the possibility to define the false condition value (by default is set to np.NaN))\n\n\nlai_masked = lai_aoi.where(lai_aoi >= 1.5) \nlai_masked\n\nlai_masked.isel(time=0).plot(cmap='viridis')\nplt.title(\"Masked Leaf Area Index\")\nplt.show()\n\n\nmask = xarray.where((lai_aoi > 1.5), 1, 0)\nmask.isel(time=0).plot()\nplt.title(\"Masked Leaf Area Index\")\nplt.show()\n\n\n By inspecting any of the variables on the representation above, you'll see that each data array represents __about 85GiB of data__, so much more than the availabe memory on this notebook server, and even on the Dask Cluster we created above. But thanks to chunking, we can still analyze it! ","type":"content","url":"/xarray101#masking","position":19},{"hierarchy":{"lvl1":"Publishing to EarthCODE"},"type":"lvl1","url":"/publishing","position":0},{"hierarchy":{"lvl1":"Publishing to EarthCODE"},"content":"To see all data and Open Science Catalog publishing options, go to the homepage - \n\nhttps://​esa​-earthcode​.github​.io​/tutorials/ .","type":"content","url":"/publishing","position":1}]}