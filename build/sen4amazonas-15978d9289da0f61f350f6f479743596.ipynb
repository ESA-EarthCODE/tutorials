{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Sen4Amazonas: Time-Series Forest Loss Zarr Cube\n",
    "\n",
    "**Goal**: Produce a time-indexed Zarr data cube of forest-loss masks for the Amazonas region to enable scalable analysis and visualization. This example is based on the [Sentinel for amazonas project](https://sen4ama.gisat.cz/) project.\n",
    "\n",
    "**Classes**:\n",
    "- `1` — forest loss\n",
    "- `0` — no data / background (treated as missing)\n",
    "\n",
    "**Study area and grid**:\n",
    "- CRS: `EPSG:4326`\n",
    "- Bounds: `-79.1997438, -17.27354892, -44.91217178, 9.0467434`\n",
    "- Resolution: `0.00017966` degrees (see `gdalwarp` step)\n",
    "\n",
    "**Workflow overview**:\n",
    "1. Convert each GeoTIFF mosaic to per-date Zarr with `gdalwarp`.\n",
    "2. Build a sorted list of Zarr stores and timestamps.\n",
    "3. Initialize an empty target cube (`cube.zarr`) with the full time axis.\n",
    "4. Append each slice into the cube using `region` writes.\n",
    "5. Tune chunking for performance and memory.\n",
    "\n",
    "**Requirements**:\n",
    "- GDAL built with Zarr driver (`gdalwarp -of Zarr`)\n",
    "- Python: `xarray`, `rioxarray`, `zarr`, `dask[distributed]`, `numpy`, `pandas`, optionally `hvplot.xarray` for quick QA\n",
    "\n",
    "**Data layout**:\n",
    "- Input: `mosaics/*.tif` (per-date masks)\n",
    "- Intermediate: `mosaics/*.zarr` (per-date stores)\n",
    "- Output: `cube.zarr` (time-series cube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ab8d5",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "When handling many temporal slices in Earth Observation, storing them as a single Zarr data cube is far superior to managing hundreds of COGs linked via a STAC collection because Zarr treats time as a first-class dimension within a unified array structure rather than as disconnected files. This enables efficient temporal queries and parallel access using chunked, compressed blocks that map directly to analysis patterns like pixel-wise time series or rolling statistics, all without the per-file latency, catalog management, and HTTP overhead that cripple COG-based stacks at scale.\n",
    "\n",
    "To learn more read the [EarthCODE data best practices pages](https://esa-earthcode.github.io/documentation/Community%20and%20Best%20Practices/Data%20and%20Workflow%20Best%20Practices/Data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gdal-step",
   "metadata": {},
   "source": [
    "## 1) Convert GeoTIFFs to Zarr with GDAL\n",
    "\n",
    "Use GDAL to warp each input GeoTIFF to `EPSG:4326`, clip to the Amazonas extent, set the target resolution, and write per-date Zarr stores. Adjust `IN_DIR`, `OUT_DIR`, bounds (`-te`), resolution (`-tr`), and compression to match your data and hardware.\n",
    "\n",
    "Tip: Confirm Zarr support with `gdalinfo --formats | grep -i zarr`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5bf97",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "IN_DIR=\"mosaics\"\n",
    "OUT_DIR=\"mosaics\"\n",
    "\n",
    "for tif in \"${IN_DIR}\"/*.tif; do\n",
    "  base=\"$(basename \"${tif}\" .tif)\"\n",
    "  out=\"${OUT_DIR}/${base}.zarr\"\n",
    "\n",
    "  echo \"Warping ${tif} -> ${out}\"\n",
    "  gdalwarp -overwrite \\\n",
    "    -t_srs EPSG:4326 \\\n",
    "    -te -79.1997438 -17.27354892 -44.91217178 9.0467434 \\\n",
    "    -tr 0.00017966 0.00017966 \\\n",
    "    -r near \\\n",
    "    -dstnodata 0 \\\n",
    "    \"${tif}\" \\\n",
    "    \"${out}\" \\\n",
    "    -of Zarr \\\n",
    "    -multi -wo NUM_THREADS=ALL_CPUS \\\n",
    "    -co COMPRESS=ZSTD -co ZSTD_LEVEL=3 -co BLOCKSIZE=512,512\n",
    "\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py-setup",
   "metadata": {},
   "source": [
    "## 2) Python setup and file discovery\n",
    "\n",
    "Import libraries, set the input folder (`mosaics/`), and collect per-date Zarr stores produced in step 1. Timestamps are parsed from filenames like `prefix_YYYYMMDD.zarr`. If your naming differs, adjust the parsing logic accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "folder = \"mosaics\"\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".zarr\")]\n",
    "# print(files)\n",
    "\n",
    "timestamps = [\n",
    "    pd.to_datetime(os.path.splitext(Path(f).name)[0].split(\"_\")[1]) for f in files\n",
    "]\n",
    "# timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sort-by-timestamp",
   "metadata": {},
   "source": [
    "### Sort files by timestamp\n",
    "Ensure files and timestamps are aligned chronologically for consistent time indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ii = np.argsort(timestamps)\n",
    "timestamps = np.array(timestamps)[ii]\n",
    "files = np.array(files)[ii]\n",
    "# files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dask-client",
   "metadata": {},
   "source": [
    "### Optional: Start a Dask client\n",
    "Spins up a local Dask scheduler/workers to parallelize IO and computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83078443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.client import Client\n",
    "cl = Client()\n",
    "cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-cube",
   "metadata": {},
   "source": [
    "## 3) Initialize target cube (template)\n",
    "\n",
    "Define chunk sizes and build a template dataset containing only the `time` coordinate. This pre-allocates the `cube.zarr` store with the desired chunking.\n",
    "\n",
    "Note: Adjust chunk sizes (`x`, `y`, `time`) to balance memory, IO, and parallelism for your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c867670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunk_size = {\"time\": 1,\"x\": 512*10, \"y\": 512*10}\n",
    "temp_data = da.expand_dims({'time': timestamps.tolist()})\n",
    "temp_das = xr.Dataset({'forest_loss': temp_data})\n",
    "temp_das = temp_das.chunk(chunk_size)\n",
    "temp_das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "write-template",
   "metadata": {},
   "source": [
    "### Write template to `cube.zarr`\n",
    "Create the target store with the full time axis and chosen chunks without writing data yet (`compute=False`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15374482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tempalte data that has al the timestamps\n",
    "temp_das.forest_loss.attrs.pop(\"_FillValue\")\n",
    "\n",
    "enc = {\n",
    "    \"forest_loss\": {\n",
    "        \"_FillValue\": np.nan,\n",
    "        \"dtype\": np.float32,\n",
    "    }\n",
    "}\n",
    "\n",
    "temp_das.to_zarr('cube.zarr', mode='w', compute=False, encoding=enc, write_empty_chunks=False, \n",
    "                 safe_chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-guidance",
   "metadata": {},
   "source": [
    "### Chunking strategy\n",
    "\n",
    "- Choose chunk sizes that fit in memory and align with access patterns (typical: larger `x`/`y`, small `time`).\n",
    "- You can override chunks when opening inputs, e.g.: `xr.open_dataset(f, engine=\"zarr\", chunks={\"X\": 512*10, \"Y\": 512*10})`.\n",
    "- Keep chunk shapes consistent across inputs and the target cube for best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "append-slices",
   "metadata": {},
   "source": [
    "## 4) Append slices to the cube\n",
    "\n",
    "For each per-date Zarr store:\n",
    "- Open with `xarray` and rename dims `X/Y` to `x/y`.\n",
    "- Convert `0` to missing (`NaN`) so background doesn’t accumulate and skip writing empty chunks\n",
    "- Attach CRS (`EPSG:4326`) and set spatial dims via `rioxarray`.\n",
    "- Expand with a singleton `time` coordinate and write into `cube.zarr` at the matching `time` index using `region`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (date, f) in enumerate(zip(timestamps, files)):\n",
    "    da = xr.open_dataset(f,\n",
    "             engine=\"zarr\", chunks={\"X\": 512*10, \"Y\": 512*10}) \n",
    "    da = da[next(iter(da.data_vars))]\n",
    "    da = da.rename({\"X\":\"x\", \"Y\":\"y\"})\n",
    "    da.name = \"forest_loss\"\n",
    "    da = da.where(da != 0)\n",
    "    da = da.rio.write_crs(\"EPSG:4326\")\n",
    "    da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")   # <-- solves MissingSpatialDimensionError\n",
    "    da = da.expand_dims({'time': [date]})\n",
    "    da = da.chunk(chunk_size)\n",
    "    da.forest_loss.attrs.pop(\"_FillValue\")\n",
    "\n",
    "    enc = {\n",
    "        \"forest_loss\": {\n",
    "            \"_FillValue\": np.nan,\n",
    "            \"dtype\": np.float32,\n",
    "        }\n",
    "    }\n",
    "    da.drop_vars(['x', 'y', 'spatial_ref']).to_zarr('cube.zarr', encoding=enc, write_empty_chunks=False, safe_chunks=True, region={'time': slice(i, i+1)})\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93446c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
